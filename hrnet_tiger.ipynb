{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebdeb7f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name vocstartsoft to get Role path.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'calvinandpogs-ee148'\n",
    "prefix = 'atrw/pose/out/'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cc376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calvinandpogs-ee148 arn:aws:iam::652516965730:role/service-role/AmazonSageMaker-ExecutionRole-20210513T011299\n",
      "/home/ec2-user/SageMaker/atrw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(bucket, role)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31a4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='CVWC2019-pose/main.py',\n",
    "                    source_dir='./',\n",
    "                    framework_version='1.8.0',\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.g4dn.xlarge',\n",
    "                    hyperparameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c67879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://calvinandpogs-ee148/atrw/detection/\n"
     ]
    }
   ],
   "source": [
    "print(f's3://{bucket}/atrw/pose/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13104a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-15 08:13:55 Starting - Starting the training job...\n",
      "2021-05-15 08:13:57 Starting - Launching requested ML instancesProfilerReport-1621066434: InProgress\n",
      "......\n",
      "2021-05-15 08:15:24 Starting - Preparing the instances for training......\n",
      "2021-05-15 08:16:24 Downloading - Downloading input data.........\n",
      "2021-05-15 08:17:45 Training - Downloading the training image.............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-05-15 08:19:58,555 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-05-15 08:19:58,577 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-05-15 08:20:04,858 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-15 08:20:05,274 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"annot\": \"/opt/ml/input/data/annot\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"annot\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-05-15-08-13-54-314\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-652516965730/pytorch-training-2021-05-15-08-13-54-314/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"YOLO-mini-tiger/main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"YOLO-mini-tiger/main.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=YOLO-mini-tiger/main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"annot\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"annot\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=YOLO-mini-tiger/main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-652516965730/pytorch-training-2021-05-15-08-13-54-314/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"annot\":\"/opt/ml/input/data/annot\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"annot\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-05-15-08-13-54-314\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-652516965730/pytorch-training-2021-05-15-08-13-54-314/source/sourcedir.tar.gz\",\"module_name\":\"YOLO-mini-tiger/main\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"YOLO-mini-tiger/main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ANNOT=/opt/ml/input/data/annot\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 YOLO-mini-tiger/main.py\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-05-15 08:20:25 Training - Training image download completed. Training in progress.\u001b[34m/opt/ml/code/YOLO-mini-tiger/darknet/data/tiger\u001b[0m\n",
      "\u001b[34m CUDA-version: 11010 (11010), GPU count: 1  \n",
      " OpenCV isn't used - data augmentation will be slow \n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 2    224 x 224 x   3 ->  112 x 112 x  16 0.011 BF\n",
      "   1 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   2 conv     16/  16  3 x 3/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.004 BF\n",
      "   3 avg                           112 x 112 x  16 ->     16\n",
      "   4 conv      4       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x   4 0.000 BF\n",
      "   5 conv     16       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  16 0.000 BF\n",
      "   6 scale Layer: 2\n",
      "   7 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   8 conv     48       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  48 0.019 BF\n",
      "   9 conv     48/  48  3 x 3/ 2    112 x 112 x  48 ->   56 x  56 x  48 0.003 BF\n",
      "  10 avg                            56 x  56 x  48 ->     48\n",
      "  11 conv     16       1 x 1/ 1      1 x   1 x  48 ->    1 x   1 x  16 0.000 BF\n",
      "  12 conv     48       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  48 0.000 BF\n",
      "  13 scale Layer: 9\n",
      "  14 conv     24       1 x 1/ 1     56 x  56 x  48 ->   56 x  56 x  24 0.007 BF\n",
      "  15 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  16 conv     72/  72  3 x 3/ 1     56 x  56 x  72 ->   56 x  56 x  72 0.004 BF\n",
      "  17 avg                            56 x  56 x  72 ->     72\n",
      "  18 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  19 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  20 scale Layer: 16\n",
      "  21 conv     24       1 x 1/ 1     56 x  56 x  72 ->   56 x  56 x  24 0.011 BF\n",
      "  22 dropout    p = 0.200        75264  ->   75264\n",
      "  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs:  56 x  56 x  24 0.000 BF\n",
      "  24 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  25 conv     72/  72  5 x 5/ 2     56 x  56 x  72 ->   28 x  28 x  72 0.003 BF\n",
      "  26 avg                            28 x  28 x  72 ->     72\n",
      "  27 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  28 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  29 scale Layer: 25\n",
      "  30 conv     20       1 x 1/ 1     28 x  28 x  72 ->   28 x  28 x  20 0.002 BF\n",
      "  31 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  32 conv     96/  96  5 x 5/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.004 BF\n",
      "  33 avg                            28 x  28 x  96 ->     96\n",
      "  34 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  35 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  36 scale Layer: 32\n",
      "  37 conv     20       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  20 0.003 BF\n",
      "  38 dropout    p = 0.200        15680  ->   15680\n",
      "  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  28 x  28 x  20 0.000 BF\n",
      "  40 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  41 conv     96/  96  3 x 3/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.001 BF\n",
      "  42 avg                            28 x  28 x  96 ->     96\n",
      "  43 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  44 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  45 scale Layer: 41\n",
      "  46 conv     40       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  40 0.006 BF\n",
      "  47 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  48 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  49 avg                            28 x  28 x 192 ->    192\n",
      "  50 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  51 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  52 scale Layer: 48\n",
      "  53 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  54 dropout    p = 0.200        31360  ->   31360\n",
      "  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  56 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  57 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  58 avg                            28 x  28 x 192 ->    192\n",
      "  59 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  60 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  61 scale Layer: 57\n",
      "  62 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  63 dropout    p = 0.200        31360  ->   31360\n",
      "  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  65 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  66 conv    192/ 192  5 x 5/ 2     28 x  28 x 192 ->   14 x  14 x 192 0.002 BF\n",
      "  67 avg                            14 x  14 x 192 ->    192\n",
      "  68 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  69 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  70 scale Layer: 66\n",
      "  71 conv     56       1 x 1/ 1     14 x  14 x 192 ->   14 x  14 x  56 0.004 BF\n",
      "  72 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  73 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  74 avg                            14 x  14 x 288 ->    288\n",
      "  75 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  76 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  77 scale Layer: 73\n",
      "  78 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  79 dropout    p = 0.200        10976  ->   10976\n",
      "  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  81 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  82 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  83 avg                            14 x  14 x 288 ->    288\n",
      "  84 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  85 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  86 scale Layer: 82\n",
      "  87 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  88 dropout    p = 0.200        10976  ->   10976\n",
      "  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  90 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  91 conv    288/ 288  5 x 5/ 2     14 x  14 x 288 ->    7 x   7 x 288 0.001 BF\n",
      "  92 avg                             7 x   7 x 288 ->    288\n",
      "  93 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  94 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  95 scale Layer: 91\n",
      "  96 conv     96       1 x 1/ 1      7 x   7 x 288 ->    7 x   7 x  96 0.003 BF\n",
      "  97 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      "  98 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      "  99 avg                             7 x   7 x 480 ->    480\n",
      " 100 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 101 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 102 scale Layer: 98\n",
      " 103 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 104 dropout    p = 0.200        4704  ->   4704\n",
      " 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 106 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 107 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 108 avg                             7 x   7 x 480 ->    480\n",
      " 109 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 110 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 111 scale Layer: 107\n",
      " 112 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 113 dropout    p = 0.200        4704  ->   4704\n",
      " 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 115 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 116 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 117 avg                             7 x   7 x 480 ->    480\n",
      " 118 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 119 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 120 scale Layer: 116\n",
      " 121 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 122 dropout    p = 0.200        4704  ->   4704\n",
      " 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 124 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 125 conv    480/ 480  3 x 3/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.000 BF\n",
      " 126 avg                             7 x   7 x 480 ->    480\n",
      " 127 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 128 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 129 scale Layer: 125\n",
      " 130 conv    160       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x 160 0.008 BF\n",
      " 131 conv    640       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 640 0.010 BF\n",
      " 132 conv     18       1 x 1/ 1      7 x   7 x 640 ->    7 x   7 x  18 0.001 BF\n",
      " 133 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 134 route  130 #011#011                           ->    7 x   7 x 160 \n",
      " 135 conv    256       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 256 0.004 BF\n",
      " 136 upsample                 2x     7 x   7 x 256 ->   14 x  14 x 256\n",
      " 137 route  136 89 #011                           ->   14 x  14 x 312 \n",
      " 138 conv     64       1 x 1/ 1     14 x  14 x 312 ->   14 x  14 x  64 0.008 BF\n",
      " 139 conv    128       3 x 3/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.029 BF\n",
      " 140 conv     18       1 x 1/ 1     14 x  14 x 128 ->   14 x  14 x  18 0.001 BF\n",
      " 141 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 142 route  138 #011#011                           ->   14 x  14 x  64 \n",
      " 143 conv    128       1 x 1/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.003 BF\n",
      " 144 upsample                 2x    14 x  14 x 128 ->   28 x  28 x 128\n",
      " 145 route  144 64 #011                           ->   28 x  28 x 168 \n",
      " 146 conv     64       1 x 1/ 1     28 x  28 x 168 ->   28 x  28 x  64 0.017 BF\n",
      " 147 conv    128       3 x 3/ 1     28 x  28 x  64 ->   28 x  28 x 128 0.116 BF\n",
      " 148 conv     18       1 x 1/ 1     28 x  28 x 128 ->   28 x  28 x  18 0.004 BF\n",
      " 149 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\u001b[0m\n",
      "\u001b[34mTotal BFLOPS 0.466 \u001b[0m\n",
      "\u001b[34mavg_outputs = 54324 \n",
      " Allocate additional workspace_size = 1.81 MB \n",
      " Create 64 permanent cpu-threads \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.442094), count: 5, class_loss = 32.344833, iou_loss = 2.181683, total_loss = 34.526516 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.439016), count: 6, class_loss = 178.921463, iou_loss = 8.266693, total_loss = 187.188156 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.455090), count: 7, class_loss = 642.539917, iou_loss = 79.020081, total_loss = 721.559998 \u001b[0m\n",
      "\u001b[34m total_bbox = 18, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.675731), count: 4, class_loss = 32.827988, iou_loss = 3.510448, total_loss = 36.338436 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.550751), count: 8, class_loss = 175.893738, iou_loss = 15.007797, total_loss = 190.901535 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.327512), count: 8, class_loss = 637.485840, iou_loss = 93.283386, total_loss = 730.769226 \u001b[0m\n",
      "\u001b[34m total_bbox = 38, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.534563), count: 7, class_loss = 32.389751, iou_loss = 4.507626, total_loss = 36.897377 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.444321), count: 10, class_loss = 178.277679, iou_loss = 19.904297, total_loss = 198.181976 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.483948), count: 1, class_loss = 640.527893, iou_loss = 24.992249, total_loss = 665.520142 \u001b[0m\n",
      "\u001b[34m total_bbox = 56, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.513947), count: 3, class_loss = 32.347412, iou_loss = 0.776917, total_loss = 33.124329 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.347927), count: 2, class_loss = 177.272919, iou_loss = 2.621231, total_loss = 179.894150 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.371800), count: 8, class_loss = 640.724487, iou_loss = 61.188110, total_loss = 701.912598 \u001b[0m\n",
      "\u001b[34m total_bbox = 69, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.544038), count: 3, class_loss = 32.870426, iou_loss = 1.500122, total_loss = 34.370548 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.518616), count: 7, class_loss = 174.839417, iou_loss = 10.943207, total_loss = 185.782623 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.493644), count: 5, class_loss = 639.799133, iou_loss = 74.550171, total_loss = 714.349304 \n",
      " total_bbox = 84, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.772182), count: 1, class_loss = 32.994743, iou_loss = 0.883995, total_loss = 33.878738 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.499757), count: 9, class_loss = 174.014954, iou_loss = 16.337738, total_loss = 190.352692 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.469546), count: 2, class_loss = 637.410095, iou_loss = 25.940857, total_loss = 663.350952 \n",
      " total_bbox = 96, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.681553), count: 2, class_loss = 32.642944, iou_loss = 0.662228, total_loss = 33.305172 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.455747), count: 4, class_loss = 176.102661, iou_loss = 6.257217, total_loss = 182.359879 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.238538), count: 8, class_loss = 639.301025, iou_loss = 54.211243, total_loss = 693.512268 \u001b[0m\n",
      "\u001b[34m total_bbox = 110, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.618796), count: 3, class_loss = 32.925697, iou_loss = 2.228954, total_loss = 35.154652 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.470870), count: 4, class_loss = 173.827164, iou_loss = 8.738388, total_loss = 182.565552 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.444428), count: 9, class_loss = 639.249329, iou_loss = 108.809937, total_loss = 748.059265 \u001b[0m\n",
      "\u001b[34m total_bbox = 126, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34myolo-mini-tiger\u001b[0m\n",
      "\u001b[34mnet.optimized_memory = 0 \u001b[0m\n",
      "\u001b[34mmini_batch = 8, batch = 64, time_steps = 1, train = 1 \u001b[0m\n",
      "\u001b[34mCreate CUDA-stream - 0 \u001b[0m\n",
      "\u001b[34mLearning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\n",
      " Detection layer: 133 - type = 28 \n",
      " Detection layer: 141 - type = 28 \n",
      " Detection layer: 149 - type = 28 \u001b[0m\n",
      "\u001b[34mLoaded: 2.878740 seconds\n",
      "\n",
      " 1: 282.849487, 282.849487 avg loss, 0.000000 rate, 12.089941 seconds, 64 images, -1.000000 hours left\u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.530587), count: 2, class_loss = 32.598682, iou_loss = 1.621651, total_loss = 34.220333 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.593565), count: 4, class_loss = 176.911087, iou_loss = 8.408096, total_loss = 185.319183 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.301853), count: 10, class_loss = 640.144165, iou_loss = 59.229187, total_loss = 699.373352 \u001b[0m\n",
      "\u001b[34m total_bbox = 142, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.585784), count: 3, class_loss = 33.029598, iou_loss = 1.874046, total_loss = 34.903645 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.488553), count: 7, class_loss = 174.737534, iou_loss = 12.636551, total_loss = 187.374084 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.306233), count: 19, class_loss = 636.369812, iou_loss = 102.286987, total_loss = 738.656799 \u001b[0m\n",
      "\u001b[34m total_bbox = 171, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.549352), count: 7, class_loss = 32.861858, iou_loss = 4.879990, total_loss = 37.741848 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.473773), count: 2, class_loss = 175.365143, iou_loss = 2.449097, total_loss = 177.814240 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.105899), count: 3, class_loss = 637.055359, iou_loss = 3.158386, total_loss = 640.213745 \n",
      " total_bbox = 183, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.646997), count: 4, class_loss = 33.786572, iou_loss = 2.812569, total_loss = 36.599140 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.522840), count: 8, class_loss = 170.761673, iou_loss = 15.169220, total_loss = 185.930893 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.105324), count: 5, class_loss = 631.350220, iou_loss = 10.036926, total_loss = 641.387146 \n",
      " total_bbox = 200, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.607641), count: 5, class_loss = 34.148911, iou_loss = 3.169178, total_loss = 37.318089 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.571605), count: 3, class_loss = 168.923645, iou_loss = 8.230453, total_loss = 177.154099 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.176335), count: 6, class_loss = 632.483521, iou_loss = 11.789124, total_loss = 644.272644 \u001b[0m\n",
      "\u001b[34m total_bbox = 214, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.385376), count: 4, class_loss = 32.370251, iou_loss = 1.257210, total_loss = 33.627460 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.534519), count: 9, class_loss = 179.219910, iou_loss = 15.929382, total_loss = 195.149292 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.491327), count: 3, class_loss = 641.564758, iou_loss = 20.858643, total_loss = 662.423401 \u001b[0m\n",
      "\u001b[34m total_bbox = 230, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.512156), count: 3, class_loss = 32.424831, iou_loss = 1.235725, total_loss = 33.660557 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.511132), count: 4, class_loss = 178.498199, iou_loss = 5.718521, total_loss = 184.216721 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.399212), count: 5, class_loss = 642.780212, iou_loss = 48.113770, total_loss = 690.893982 \u001b[0m\n",
      "\u001b[34m total_bbox = 241, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.741695), count: 4, class_loss = 32.900612, iou_loss = 3.096584, total_loss = 35.997196 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.401376), count: 4, class_loss = 175.633331, iou_loss = 5.375031, total_loss = 181.008362 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.342953), count: 5, class_loss = 638.156006, iou_loss = 34.077576, total_loss = 672.233582 \u001b[0m\n",
      "\u001b[34m total_bbox = 254, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mLoaded: 0.000050 seconds\n",
      "\n",
      " 2: 281.874420, 282.751984 avg loss, 0.000000 rate, 11.191496 seconds, 128 images, 0.008316 hours left\u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.500546), count: 7, class_loss = 32.490929, iou_loss = 3.753208, total_loss = 36.244137 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.409586), count: 4, class_loss = 177.374817, iou_loss = 5.720245, total_loss = 183.095062 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.365912), count: 4, class_loss = 640.986023, iou_loss = 34.624268, total_loss = 675.610291 \n",
      " total_bbox = 269, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.649984), count: 5, class_loss = 33.236488, iou_loss = 2.915600, total_loss = 36.152088 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.495056), count: 5, class_loss = 172.193817, iou_loss = 8.319229, total_loss = 180.513046 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.297564), count: 6, class_loss = 634.373169, iou_loss = 46.097778, total_loss = 680.470947 \u001b[0m\n",
      "\u001b[34m total_bbox = 285, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.592923), count: 3, class_loss = 32.944962, iou_loss = 1.493881, total_loss = 34.438843 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.578175), count: 4, class_loss = 175.471786, iou_loss = 9.605270, total_loss = 185.077057 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.319325), count: 9, class_loss = 639.029480, iou_loss = 43.810547, total_loss = 682.840027 \u001b[0m\n",
      "\u001b[34m total_bbox = 301, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.447412), count: 5, class_loss = 32.959549, iou_loss = 1.208565, total_loss = 34.168114 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.489694), count: 8, class_loss = 176.332428, iou_loss = 11.199326, total_loss = 187.531754 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.221342), count: 1, class_loss = 637.377136, iou_loss = 0.846497, total_loss = 638.223633 \u001b[0m\n",
      "\u001b[34m total_bbox = 314, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.469207), count: 6, class_loss = 32.705830, iou_loss = 2.862408, total_loss = 35.568237 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.232347), count: 4, class_loss = 178.714279, iou_loss = 3.006912, total_loss = 181.721191 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.408772), count: 3, class_loss = 641.096008, iou_loss = 48.235962, total_loss = 689.331970 \u001b[0m\n",
      "\u001b[34m total_bbox = 327, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.562239), count: 5, class_loss = 33.065380, iou_loss = 2.687389, total_loss = 35.752769 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.521700), count: 5, class_loss = 176.483612, iou_loss = 7.525040, total_loss = 184.008652 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.315462), count: 13, class_loss = 635.976685, iou_loss = 103.410156, total_loss = 739.386841 \u001b[0m\n",
      "\u001b[34m total_bbox = 350, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.616751), count: 4, class_loss = 32.868980, iou_loss = 2.948486, total_loss = 35.817467 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.415241), count: 5, class_loss = 175.835831, iou_loss = 6.018723, total_loss = 181.854553 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.247229), count: 1, class_loss = 637.696289, iou_loss = 2.775330, total_loss = 640.471619 \u001b[0m\n",
      "\u001b[34m total_bbox = 360, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.557537), count: 5, class_loss = 33.411198, iou_loss = 2.817165, total_loss = 36.228363 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.649495), count: 2, class_loss = 174.644653, iou_loss = 3.137253, total_loss = 177.781906 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.241694), count: 2, class_loss = 638.141296, iou_loss = 4.058716, total_loss = 642.200012 \u001b[0m\n",
      "\u001b[34m total_bbox = 369, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mLoaded: 0.000064 seconds\n",
      "\n",
      " 3: 282.347534, 282.711548 avg loss, 0.000000 rate, 11.375200 seconds, 192 images, 0.008264 hours left\u001b[0m\n",
      "\u001b[34mSaving weights to backup/yolo-mini-tiger_final.weights\u001b[0m\n",
      "\u001b[34mIf you want to train from the beginning, then use flag in the end of training command: -clear \u001b[0m\n",
      "\u001b[34m CUDA-version: 11010 (11010), GPU count: 1  \n",
      " OpenCV isn't used - data augmentation will be slow \n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 2    224 x 224 x   3 ->  112 x 112 x  16 0.011 BF\n",
      "   1 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   2 conv     16/  16  3 x 3/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.004 BF\n",
      "   3 avg                           112 x 112 x  16 ->     16\n",
      "   4 conv      4       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x   4 0.000 BF\n",
      "   5 conv     16       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  16 0.000 BF\n",
      "   6 scale Layer: 2\n",
      "   7 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   8 conv     48       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  48 0.019 BF\n",
      "   9 conv     48/  48  3 x 3/ 2    112 x 112 x  48 ->   56 x  56 x  48 0.003 BF\n",
      "  10 avg                            56 x  56 x  48 ->     48\n",
      "  11 conv     16       1 x 1/ 1      1 x   1 x  48 ->    1 x   1 x  16 0.000 BF\n",
      "  12 conv     48       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  48 0.000 BF\n",
      "  13 scale Layer: 9\n",
      "  14 conv     24       1 x 1/ 1     56 x  56 x  48 ->   56 x  56 x  24 0.007 BF\n",
      "  15 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  16 conv     72/  72  3 x 3/ 1     56 x  56 x  72 ->   56 x  56 x  72 0.004 BF\n",
      "  17 avg                            56 x  56 x  72 ->     72\n",
      "  18 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  19 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  20 scale Layer: 16\n",
      "  21 conv     24       1 x 1/ 1     56 x  56 x  72 ->   56 x  56 x  24 0.011 BF\n",
      "  22 dropout    p = 0.200        75264  ->   75264\n",
      "  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs:  56 x  56 x  24 0.000 BF\n",
      "  24 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  25 conv     72/  72  5 x 5/ 2     56 x  56 x  72 ->   28 x  28 x  72 0.003 BF\n",
      "  26 avg                            28 x  28 x  72 ->     72\n",
      "  27 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  28 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  29 scale Layer: 25\n",
      "  30 conv     20       1 x 1/ 1     28 x  28 x  72 ->   28 x  28 x  20 0.002 BF\n",
      "  31 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  32 conv     96/  96  5 x 5/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.004 BF\n",
      "  33 avg                            28 x  28 x  96 ->     96\n",
      "  34 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  35 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  36 scale Layer: 32\n",
      "  37 conv     20       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  20 0.003 BF\n",
      "  38 dropout    p = 0.200        15680  ->   15680\n",
      "  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  28 x  28 x  20 0.000 BF\n",
      "  40 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  41 conv     96/  96  3 x 3/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.001 BF\n",
      "  42 avg                            28 x  28 x  96 ->     96\n",
      "  43 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  44 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  45 scale Layer: 41\n",
      "  46 conv     40       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  40 0.006 BF\n",
      "  47 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  48 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  49 avg                            28 x  28 x 192 ->    192\n",
      "  50 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  51 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  52 scale Layer: 48\n",
      "  53 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  54 dropout    p = 0.200        31360  ->   31360\n",
      "  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  56 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  57 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  58 avg                            28 x  28 x 192 ->    192\n",
      "  59 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  60 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  61 scale Layer: 57\n",
      "  62 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  63 dropout    p = 0.200        31360  ->   31360\n",
      "  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  65 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  66 conv    192/ 192  5 x 5/ 2     28 x  28 x 192 ->   14 x  14 x 192 0.002 BF\n",
      "  67 avg                            14 x  14 x 192 ->    192\n",
      "  68 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  69 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  70 scale Layer: 66\n",
      "  71 conv     56       1 x 1/ 1     14 x  14 x 192 ->   14 x  14 x  56 0.004 BF\n",
      "  72 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  73 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  74 avg                            14 x  14 x 288 ->    288\n",
      "  75 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  76 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  77 scale Layer: 73\n",
      "  78 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  79 dropout    p = 0.200        10976  ->   10976\n",
      "  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  81 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  82 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  83 avg                            14 x  14 x 288 ->    288\n",
      "  84 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  85 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  86 scale Layer: 82\n",
      "  87 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  88 dropout    p = 0.200        10976  ->   10976\n",
      "  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  90 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  91 conv    288/ 288  5 x 5/ 2     14 x  14 x 288 ->    7 x   7 x 288 0.001 BF\n",
      "  92 avg                             7 x   7 x 288 ->    288\n",
      "  93 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  94 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  95 scale Layer: 91\n",
      "  96 conv     96       1 x 1/ 1      7 x   7 x 288 ->    7 x   7 x  96 0.003 BF\n",
      "  97 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      "  98 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      "  99 avg                             7 x   7 x 480 ->    480\n",
      " 100 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 101 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 102 scale Layer: 98\n",
      " 103 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 104 dropout    p = 0.200        4704  ->   4704\n",
      " 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 106 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 107 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 108 avg                             7 x   7 x 480 ->    480\n",
      " 109 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 110 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 111 scale Layer: 107\n",
      " 112 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 113 dropout    p = 0.200        4704  ->   4704\n",
      " 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 115 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 116 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 117 avg                             7 x   7 x 480 ->    480\n",
      " 118 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 119 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 120 scale Layer: 116\n",
      " 121 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 122 dropout    p = 0.200        4704  ->   4704\n",
      " 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 124 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 125 conv    480/ 480  3 x 3/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.000 BF\n",
      " 126 avg                             7 x   7 x 480 ->    480\n",
      " 127 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 128 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 129 scale Layer: 125\n",
      " 130 conv    160       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x 160 0.008 BF\n",
      " 131 conv    640       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 640 0.010 BF\n",
      " 132 conv     18       1 x 1/ 1      7 x   7 x 640 ->    7 x   7 x  18 0.001 BF\n",
      " 133 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 134 route  130 #011#011                           ->    7 x   7 x 160 \n",
      " 135 conv    256       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 256 0.004 BF\n",
      " 136 upsample                 2x     7 x   7 x 256 ->   14 x  14 x 256\n",
      " 137 route  136 89 #011                           ->   14 x  14 x 312 \n",
      " 138 conv     64       1 x 1/ 1     14 x  14 x 312 ->   14 x  14 x  64 0.008 BF\n",
      " 139 conv    128       3 x 3/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.029 BF\n",
      " 140 conv     18       1 x 1/ 1     14 x  14 x 128 ->   14 x  14 x  18 0.001 BF\n",
      " 141 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 142 route  138 #011#011                           ->   14 x  14 x  64 \n",
      " 143 conv    128       1 x 1/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.003 BF\n",
      " 144 upsample                 2x    14 x  14 x 128 ->   28 x  28 x 128\n",
      " 145 route  144 64 #011                           ->   28 x  28 x 168 \n",
      " 146 conv     64       1 x 1/ 1     28 x  28 x 168 ->   28 x  28 x  64 0.017 BF\n",
      " 147 conv    128       3 x 3/ 1     28 x  28 x  64 ->   28 x  28 x 128 0.116 BF\n",
      " 148 conv     18       1 x 1/ 1     28 x  28 x 128 ->   28 x  28 x  18 0.004 BF\n",
      " 149 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\u001b[0m\n",
      "\u001b[34mTotal BFLOPS 0.466 \u001b[0m\n",
      "\u001b[34mavg_outputs = 54324 \n",
      " Allocate additional workspace_size = 1.81 MB \u001b[0m\n",
      "\u001b[34mLoading weights from backup/yolo-mini-tiger_final.weights...net.optimized_memory = 0 \u001b[0m\n",
      "\u001b[34mmini_batch = 1, batch = 8, time_steps = 1, train = 0 \u001b[0m\n",
      "\u001b[34mCreate CUDA-stream - 0 \u001b[0m\n",
      "\u001b[34mDone! Loaded 150 layers from weights-file \n",
      "\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34m12\u001b[0m\n",
      "\u001b[34m16\u001b[0m\n",
      "\u001b[34m20\u001b[0m\n",
      "\u001b[34m24\u001b[0m\n",
      "\u001b[34m28\u001b[0m\n",
      "\u001b[34m32\u001b[0m\n",
      "\u001b[34m36\u001b[0m\n",
      "\u001b[34m40\u001b[0m\n",
      "\u001b[34m44\u001b[0m\n",
      "\u001b[34m48\u001b[0m\n",
      "\u001b[34m52\u001b[0m\n",
      "\u001b[34m56\u001b[0m\n",
      "\u001b[34m60\u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34m68\u001b[0m\n",
      "\u001b[34m72\u001b[0m\n",
      "\u001b[34m76\u001b[0m\n",
      "\u001b[34m80\u001b[0m\n",
      "\u001b[34m84\u001b[0m\n",
      "\u001b[34m88\u001b[0m\n",
      "\u001b[34m92\u001b[0m\n",
      "\u001b[34m96\u001b[0m\n",
      "\u001b[34m100\u001b[0m\n",
      "\u001b[34m104\u001b[0m\n",
      "\u001b[34m108\u001b[0m\n",
      "\u001b[34m112\u001b[0m\n",
      "\u001b[34m116\u001b[0m\n",
      "\u001b[34m120\u001b[0m\n",
      "\u001b[34m124\u001b[0m\n",
      "\u001b[34m128\u001b[0m\n",
      "\u001b[34m132\u001b[0m\n",
      "\u001b[34m136\u001b[0m\n",
      "\u001b[34m140\u001b[0m\n",
      "\u001b[34m144\u001b[0m\n",
      "\u001b[34m148\u001b[0m\n",
      "\u001b[34m152\u001b[0m\n",
      "\u001b[34m156\u001b[0m\n",
      "\u001b[34m160\u001b[0m\n",
      "\u001b[34m164\u001b[0m\n",
      "\u001b[34m168\u001b[0m\n",
      "\u001b[34m172\u001b[0m\n",
      "\u001b[34m176\u001b[0m\n",
      "\u001b[34m180\u001b[0m\n",
      "\u001b[34m184\u001b[0m\n",
      "\u001b[34m188\u001b[0m\n",
      "\u001b[34m192\u001b[0m\n",
      "\u001b[34m196\u001b[0m\n",
      "\u001b[34m200\u001b[0m\n",
      "\u001b[34m204\u001b[0m\n",
      "\u001b[34m208\u001b[0m\n",
      "\u001b[34m212\u001b[0m\n",
      "\u001b[34m216\u001b[0m\n",
      "\u001b[34m220\u001b[0m\n",
      "\u001b[34m224\u001b[0m\n",
      "\u001b[34m228\u001b[0m\n",
      "\u001b[34m232\u001b[0m\n",
      "\u001b[34m236\u001b[0m\n",
      "\u001b[34m240\u001b[0m\n",
      "\u001b[34m244\u001b[0m\n",
      "\u001b[34m248\u001b[0m\n",
      "\u001b[34m252\u001b[0m\n",
      "\u001b[34m256\u001b[0m\n",
      "\u001b[34m260\u001b[0m\n",
      "\u001b[34m264\u001b[0m\n",
      "\u001b[34m268\u001b[0m\n",
      "\u001b[34m272\u001b[0m\n",
      "\u001b[34m276\u001b[0m\n",
      "\u001b[34m280\n",
      " seen 64, trained: 0 K-images (0 Kilo-batches_64) \n",
      "\n",
      " calculation mAP (mean average precision)...\n",
      " Detection layer: 133 - type = 28 \n",
      " Detection layer: 141 - type = 28 \n",
      " Detection layer: 149 - type = 28 \n",
      "\n",
      " detections_count = 111889, unique_truth_count = 544  \n",
      " rank = 0 of ranks = 111889 \n",
      " rank = 100 of ranks = 111889 \n",
      " rank = 200 of ranks = 111889 \n",
      " rank = 300 of ranks = 111889 \n",
      " rank = 400 of ranks = 111889 \n",
      " rank = 500 of ranks = 111889 \n",
      " rank = 600 of ranks = 111889 \n",
      " rank = 700 of ranks = 111889 \n",
      " rank = 800 of ranks = 111889 \n",
      " rank = 900 of ranks = 111889 \n",
      " rank = 1000 of ranks = 111889 \n",
      " rank = 1100 of ranks = 111889 \n",
      " rank = 1200 of ranks = 111889 \n",
      " rank = 1300 of ranks = 111889 \n",
      " rank = 1400 of ranks = 111889 \n",
      " rank = 1500 of ranks = 111889 \n",
      " rank = 1600 of ranks = 111889 \n",
      " rank = 1700 of ranks = 111889 \n",
      " rank = 1800 of ranks = 111889 \n",
      " rank = 1900 of ranks = 111889 \n",
      " rank = 2000 of ranks = 111889 \n",
      " rank = 2100 of ranks = 111889 \n",
      " rank = 2200 of ranks = 111889 \n",
      " rank = 2300 of ranks = 111889 \n",
      " rank = 2400 of ranks = 111889 \n",
      " rank = 2500 of ranks = 111889 \n",
      " rank = 2600 of ranks = 111889 \n",
      " rank = 2700 of ranks = 111889 \n",
      " rank = 2800 of ranks = 111889 \n",
      " rank = 2900 of ranks = 111889 \n",
      " rank = 3000 of ranks = 111889 \n",
      " rank = 3100 of ranks = 111889 \n",
      " rank = 3200 of ranks = 111889 \n",
      " rank = 3300 of ranks = 111889 \n",
      " rank = 3400 of ranks = 111889 \n",
      " rank = 3500 of ranks = 111889 \n",
      " rank = 3600 of ranks = 111889 \n",
      " rank = 3700 of ranks = 111889 \n",
      " rank = 3800 of ranks = 111889 \n",
      " rank = 3900 of ranks = 111889 \n",
      " rank = 4000 of ranks = 111889 \n",
      " rank = 4100 of ranks = 111889 \n",
      " rank = 4200 of ranks = 111889 \n",
      " rank = 4300 of ranks = 111889 \n",
      " rank = 4400 of ranks = 111889 \n",
      " rank = 4500 of ranks = 111889 \n",
      " rank = 4600 of ranks = 111889 \n",
      " rank = 4700 of ranks = 111889 \n",
      " rank = 4800 of ranks = 111889 \n",
      " rank = 4900 of ranks = 111889 \n",
      " rank = 5000 of ranks = 111889 \n",
      " rank = 5100 of ranks = 111889 \n",
      " rank = 5200 of ranks = 111889 \n",
      " rank = 5300 of ranks = 111889 \n",
      " rank = 5400 of ranks = 111889 \n",
      " rank = 5500 of ranks = 111889 \n",
      " rank = 5600 of ranks = 111889 \n",
      " rank = 5700 of ranks = 111889 \n",
      " rank = 5800 of ranks = 111889 \n",
      " rank = 5900 of ranks = 111889 \n",
      " rank = 6000 of ranks = 111889 \n",
      " rank = 6100 of ranks = 111889 \n",
      " rank = 6200 of ranks = 111889 \n",
      " rank = 6300 of ranks = 111889 \n",
      " rank = 6400 of ranks = 111889 \n",
      " rank = 6500 of ranks = 111889 \n",
      " rank = 6600 of ranks = 111889 \n",
      " rank = 6700 of ranks = 111889 \n",
      " rank = 6800 of ranks = 111889 \n",
      " rank = 6900 of ranks = 111889 \n",
      " rank = 7000 of ranks = 111889 \n",
      " rank = 7100 of ranks = 111889 \n",
      " rank = 7200 of ranks = 111889 \n",
      " rank = 7300 of ranks = 111889 \n",
      " rank = 7400 of ranks = 111889 \n",
      " rank = 7500 of ranks = 111889 \n",
      " rank = 7600 of ranks = 111889 \n",
      " rank = 7700 of ranks = 111889 \n",
      " rank = 7800 of ranks = 111889 \n",
      " rank = 7900 of ranks = 111889 \n",
      " rank = 8000 of ranks = 111889 \n",
      " rank = 8100 of ranks = 111889 \n",
      " rank = 8200 of ranks = 111889 \n",
      " rank = 8300 of ranks = 111889 \n",
      " rank = 8400 of ranks = 111889 \n",
      " rank = 8500 of ranks = 111889 \n",
      " rank = 8600 of ranks = 111889 \n",
      " rank = 8700 of ranks = 111889 \n",
      " rank = 8800 of ranks = 111889 \n",
      " rank = 8900 of ranks = 111889 \n",
      " rank = 9000 of ranks = 111889 \n",
      " rank = 9100 of ranks = 111889 \n",
      " rank = 9200 of ranks = 111889 \n",
      " rank = 9300 of ranks = 111889 \n",
      " rank = 9400 of ranks = 111889 \n",
      " rank = 9500 of ranks = 111889 \n",
      " rank = 9600 of ranks = 111889 \n",
      " rank = 9700 of ranks = 111889 \n",
      " rank = 9800 of ranks = 111889 \n",
      " rank = 9900 of ranks = 111889 \n",
      " rank = 10000 of ranks = 111889 \n",
      " rank = 10100 of ranks = 111889 \n",
      " rank = 10200 of ranks = 111889 \n",
      " rank = 10300 of ranks = 111889 \n",
      " rank = 10400 of ranks = 111889 \n",
      " rank = 10500 of ranks = 111889 \n",
      " rank = 10600 of ranks = 111889 \n",
      " rank = 10700 of ranks = 111889 \n",
      " rank = 10800 of ranks = 111889 \n",
      " rank = 10900 of ranks = 111889 \n",
      " rank = 11000 of ranks = 111889 \n",
      " rank = 11100 of ranks = 111889 \n",
      " rank = 11200 of ranks = 111889 \n",
      " rank = 11300 of ranks = 111889 \n",
      " rank = 11400 of ranks = 111889 \n",
      " rank = 11500 of ranks = 111889 \n",
      " rank = 11600 of ranks = 111889 \n",
      " rank = 11700 of ranks = 111889 \n",
      " rank = 11800 of ranks = 111889 \n",
      " rank = 11900 of ranks = 111889 \n",
      " rank = 12000 of ranks = 111889 \n",
      " rank = 12100 of ranks = 111889 \n",
      " rank = 12200 of ranks = 111889 \n",
      " rank = 12300 of ranks = 111889 \n",
      " rank = 12400 of ranks = 111889 \n",
      " rank = 12500 of ranks = 111889 \n",
      " rank = 12600 of ranks = 111889 \n",
      " rank = 12700 of ranks = 111889 \n",
      " rank = 12800 of ranks = 111889 \n",
      " rank = 12900 of ranks = 111889 \n",
      " rank = 13000 of ranks = 111889 \n",
      " rank = 13100 of ranks = 111889 \n",
      " rank = 13200 of ranks = 111889 \n",
      " rank = 13300 of ranks = 111889 \n",
      " rank = 13400 of ranks = 111889 \n",
      " rank = 13500 of ranks = 111889 \n",
      " rank = 13600 of ranks = 111889 \n",
      " rank = 13700 of ranks = 111889 \n",
      " rank = 13800 of ranks = 111889 \n",
      " rank = 13900 of ranks = 111889 \n",
      " rank = 14000 of ranks = 111889 \n",
      " rank = 14100 of ranks = 111889 \n",
      " rank = 14200 of ranks = 111889 \n",
      " rank = 14300 of ranks = 111889 \n",
      " rank = 14400 of ranks = 111889 \n",
      " rank = 14500 of ranks = 111889 \n",
      " rank = 14600 of ranks = 111889 \n",
      " rank = 14700 of ranks = 111889 \n",
      " rank = 14800 of ranks = 111889 \n",
      " rank = 14900 of ranks = 111889 \n",
      " rank = 15000 of ranks = 111889 \n",
      " rank = 15100 of ranks = 111889 \n",
      " rank = 15200 of ranks = 111889 \n",
      " rank = 15300 of ranks = 111889 \n",
      " rank = 15400 of ranks = 111889 \n",
      " rank = 15500 of ranks = 111889 \n",
      " rank = 15600 of ranks = 111889 \n",
      " rank = 15700 of ranks = 111889 \n",
      " rank = 15800 of ranks = 111889 \n",
      " rank = 15900 of ranks = 111889 \n",
      " rank = 16000 of ranks = 111889 \n",
      " rank = 16100 of ranks = 111889 \n",
      " rank = 16200 of ranks = 111889 \n",
      " rank = 16300 of ranks = 111889 \n",
      " rank = 16400 of ranks = 111889 \n",
      " rank = 16500 of ranks = 111889 \n",
      " rank = 16600 of ranks = 111889 \n",
      " rank = 16700 of ranks = 111889 \n",
      " rank = 16800 of ranks = 111889 \n",
      " rank = 16900 of ranks = 111889 \n",
      " rank = 17000 of ranks = 111889 \n",
      " rank = 17100 of ranks = 111889 \n",
      " rank = 17200 of ranks = 111889 \n",
      " rank = 17300 of ranks = 111889 \n",
      " rank = 17400 of ranks = 111889 \n",
      " rank = 17500 of ranks = 111889 \n",
      " rank = 17600 of ranks = 111889 \n",
      " rank = 17700 of ranks = 111889 \n",
      " rank = 17800 of ranks = 111889 \n",
      " rank = 17900 of ranks = 111889 \n",
      " rank = 18000 of ranks = 111889 \n",
      " rank = 18100 of ranks = 111889 \n",
      " rank = 18200 of ranks = 111889 \n",
      " rank = 18300 of ranks = 111889 \n",
      " rank = 18400 of ranks = 111889 \n",
      " rank = 18500 of ranks = 111889 \n",
      " rank = 18600 of ranks = 111889 \n",
      " rank = 18700 of ranks = 111889 \n",
      " rank = 18800 of ranks = 111889 \n",
      " rank = 18900 of ranks = 111889 \n",
      " rank = 19000 of ranks = 111889 \n",
      " rank = 19100 of ranks = 111889 \n",
      " rank = 19200 of ranks = 111889 \n",
      " rank = 19300 of ranks = 111889 \n",
      " rank = 19400 of ranks = 111889 \n",
      " rank = 19500 of ranks = 111889 \n",
      " rank = 19600 of ranks = 111889 \n",
      " rank = 19700 of ranks = 111889 \n",
      " rank = 19800 of ranks = 111889 \n",
      " rank = 19900 of ranks = 111889 \n",
      " rank = 20000 of ranks = 111889 \n",
      " rank = 20100 of ranks = 111889 \n",
      " rank = 20200 of ranks = 111889 \n",
      " rank = 20300 of ranks = 111889 \n",
      " rank = 20400 of ranks = 111889 \n",
      " rank = 20500 of ranks = 111889 \n",
      " rank = 20600 of ranks = 111889 \n",
      " rank = 20700 of ranks = 111889 \n",
      " rank = 20800 of ranks = 111889 \n",
      " rank = 20900 of ranks = 111889 \n",
      " rank = 21000 of ranks = 111889 \n",
      " rank = 21100 of ranks = 111889 \n",
      " rank = 21200 of ranks = 111889 \n",
      " rank = 21300 of ranks = 111889 \n",
      " rank = 21400 of ranks = 111889 \n",
      " rank = 21500 of ranks = 111889 \n",
      " rank = 21600 of ranks = 111889 \n",
      " rank = 21700 of ranks = 111889 \n",
      " rank = 21800 of ranks = 111889 \n",
      " rank = 21900 of ranks = 111889 \n",
      " rank = 22000 of ranks = 111889 \n",
      " rank = 22100 of ranks = 111889 \n",
      " rank = 22200 of ranks = 111889 \n",
      " rank = 22300 of ranks = 111889 \n",
      " rank = 22400 of ranks = 111889 \n",
      " rank = 22500 of ranks = 111889 \n",
      " rank = 22600 of ranks = 111889 \n",
      " rank = 22700 of ranks = 111889 \n",
      " rank = 22800 of ranks = 111889 \n",
      " rank = 22900 of ranks = 111889 \n",
      " rank = 23000 of ranks = 111889 \n",
      " rank = 23100 of ranks = 111889 \n",
      " rank = 23200 of ranks = 111889 \n",
      " rank = 23300 of ranks = 111889 \n",
      " rank = 23400 of ranks = 111889 \n",
      " rank = 23500 of ranks = 111889 \n",
      " rank = 23600 of ranks = 111889 \n",
      " rank = 23700 of ranks = 111889 \n",
      " rank = 23800 of ranks = 111889 \n",
      " rank = 23900 of ranks = 111889 \n",
      " rank = 24000 of ranks = 111889 \n",
      " rank = 24100 of ranks = 111889 \n",
      " rank = 24200 of ranks = 111889 \n",
      " rank = 24300 of ranks = 111889 \n",
      " rank = 24400 of ranks = 111889 \n",
      " rank = 24500 of ranks = 111889 \n",
      " rank = 24600 of ranks = 111889 \n",
      " rank = 24700 of ranks = 111889 \n",
      " rank = 24800 of ranks = 111889 \n",
      " rank = 24900 of ranks = 111889 \n",
      " rank = 25000 of ranks = 111889 \n",
      " rank = 25100 of ranks = 111889 \n",
      " rank = 25200 of ranks = 111889 \n",
      " rank = 25300 of ranks = 111889 \n",
      " rank = 25400 of ranks = 111889 \n",
      " rank = 25500 of ranks = 111889 \n",
      " rank = 25600 of ranks = 111889 \n",
      " rank = 25700 of ranks = 111889 \n",
      " rank = 25800 of ranks = 111889 \n",
      " rank = 25900 of ranks = 111889 \n",
      " rank = 26000 of ranks = 111889 \n",
      " rank = 26100 of ranks = 111889 \n",
      " rank = 26200 of ranks = 111889 \n",
      " rank = 26300 of ranks = 111889 \n",
      " rank = 26400 of ranks = 111889 \n",
      " rank = 26500 of ranks = 111889 \n",
      " rank = 26600 of ranks = 111889 \n",
      " rank = 26700 of ranks = 111889 \n",
      " rank = 26800 of ranks = 111889 \n",
      " rank = 26900 of ranks = 111889 \n",
      " rank = 27000 of ranks = 111889 \n",
      " rank = 27100 of ranks = 111889 \n",
      " rank = 27200 of ranks = 111889 \n",
      " rank = 27300 of ranks = 111889 \n",
      " rank = 27400 of ranks = 111889 \n",
      " rank = 27500 of ranks = 111889 \n",
      " rank = 27600 of ranks = 111889 \n",
      " rank = 27700 of ranks = 111889 \n",
      " rank = 27800 of ranks = 111889 \n",
      " rank = 27900 of ranks = 111889 \n",
      " rank = 28000 of ranks = 111889 \n",
      " rank = 28100 of ranks = 111889 \n",
      " rank = 28200 of ranks = 111889 \n",
      " rank = 28300 of ranks = 111889 \n",
      " rank = 28400 of ranks = 111889 \n",
      " rank = 28500 of ranks = 111889 \n",
      " rank = 28600 of ranks = 111889 \n",
      " rank = 28700 of ranks = 111889 \n",
      " rank = 28800 of ranks = 111889 \n",
      " rank = 28900 of ranks = 111889 \n",
      " rank = 29000 of ranks = 111889 \n",
      " rank = 29100 of ranks = 111889 \n",
      " rank = 29200 of ranks = 111889 \n",
      " rank = 29300 of ranks = 111889 \n",
      " rank = 29400 of ranks = 111889 \n",
      " rank = 29500 of ranks = 111889 \n",
      " rank = 29600 of ranks = 111889 \n",
      " rank = 29700 of ranks = 111889 \n",
      " rank = 29800 of ranks = 111889 \n",
      " rank = 29900 of ranks = 111889 \n",
      " rank = 30000 of ranks = 111889 \n",
      " rank = 30100 of ranks = 111889 \n",
      " rank = 30200 of ranks = 111889 \n",
      " rank = 30300 of ranks = 111889 \n",
      " rank = 30400 of ranks = 111889 \n",
      " rank = 30500 of ranks = 111889 \n",
      " rank = 30600 of ranks = 111889 \n",
      " rank = 30700 of ranks = 111889 \n",
      " rank = 30800 of ranks = 111889 \n",
      " rank = 30900 of ranks = 111889 \n",
      " rank = 31000 of ranks = 111889 \n",
      " rank = 31100 of ranks = 111889 \n",
      " rank = 31200 of ranks = 111889 \n",
      " rank = 31300 of ranks = 111889 \n",
      " rank = 31400 of ranks = 111889 \n",
      " rank = 31500 of ranks = 111889 \n",
      " rank = 31600 of ranks = 111889 \n",
      " rank = 31700 of ranks = 111889 \n",
      " rank = 31800 of ranks = 111889 \n",
      " rank = 31900 of ranks = 111889 \n",
      " rank = 32000 of ranks = 111889 \n",
      " rank = 32100 of ranks = 111889 \n",
      " rank = 32200 of ranks = 111889 \n",
      " rank = 32300 of ranks = 111889 \n",
      " rank = 32400 of ranks = 111889 \n",
      " rank = 32500 of ranks = 111889 \n",
      " rank = 32600 of ranks = 111889 \n",
      " rank = 32700 of ranks = 111889 \n",
      " rank = 32800 of ranks = 111889 \n",
      " rank = 32900 of ranks = 111889 \n",
      " rank = 33000 of ranks = 111889 \n",
      " rank = 33100 of ranks = 111889 \n",
      " rank = 33200 of ranks = 111889 \n",
      " rank = 33300 of ranks = 111889 \n",
      " rank = 33400 of ranks = 111889 \n",
      " rank = 33500 of ranks = 111889 \n",
      " rank = 33600 of ranks = 111889 \n",
      " rank = 33700 of ranks = 111889 \n",
      " rank = 33800 of ranks = 111889 \n",
      " rank = 33900 of ranks = 111889 \n",
      " rank = 34000 of ranks = 111889 \n",
      " rank = 34100 of ranks = 111889 \n",
      " rank = 34200 of ranks = 111889 \n",
      " rank = 34300 of ranks = 111889 \n",
      " rank = 34400 of ranks = 111889 \n",
      " rank = 34500 of ranks = 111889 \n",
      " rank = 34600 of ranks = 111889 \n",
      " rank = 34700 of ranks = 111889 \n",
      " rank = 34800 of ranks = 111889 \n",
      " rank = 34900 of ranks = 111889 \n",
      " rank = 35000 of ranks = 111889 \n",
      " rank = 35100 of ranks = 111889 \n",
      " rank = 35200 of ranks = 111889 \n",
      " rank = 35300 of ranks = 111889 \n",
      " rank = 35400 of ranks = 111889 \n",
      " rank = 35500 of ranks = 111889 \n",
      " rank = 35600 of ranks = 111889 \n",
      " rank = 35700 of ranks = 111889 \n",
      " rank = 35800 of ranks = 111889 \n",
      " rank = 35900 of ranks = 111889 \n",
      " rank = 36000 of ranks = 111889 \n",
      " rank = 36100 of ranks = 111889 \n",
      " rank = 36200 of ranks = 111889 \n",
      " rank = 36300 of ranks = 111889 \n",
      " rank = 36400 of ranks = 111889 \n",
      " rank = 36500 of ranks = 111889 \n",
      " rank = 36600 of ranks = 111889 \n",
      " rank = 36700 of ranks = 111889 \n",
      " rank = 36800 of ranks = 111889 \n",
      " rank = 36900 of ranks = 111889 \n",
      " rank = 37000 of ranks = 111889 \n",
      " rank = 37100 of ranks = 111889 \n",
      " rank = 37200 of ranks = 111889 \n",
      " rank = 37300 of ranks = 111889 \n",
      " rank = 37400 of ranks = 111889 \n",
      " rank = 37500 of ranks = 111889 \n",
      " rank = 37600 of ranks = 111889 \n",
      " rank = 37700 of ranks = 111889 \n",
      " rank = 37800 of ranks = 111889 \n",
      " rank = 37900 of ranks = 111889 \n",
      " rank = 38000 of ranks = 111889 \n",
      " rank = 38100 of ranks = 111889 \n",
      " rank = 38200 of ranks = 111889 \n",
      " rank = 38300 of ranks = 111889 \n",
      " rank = 38400 of ranks = 111889 \n",
      " rank = 38500 of ranks = 111889 \n",
      " rank = 38600 of ranks = 111889 \n",
      " rank = 38700 of ranks = 111889 \n",
      " rank = 38800 of ranks = 111889 \n",
      " rank = 38900 of ranks = 111889 \n",
      " rank = 39000 of ranks = 111889 \n",
      " rank = 39100 of ranks = 111889 \n",
      " rank = 39200 of ranks = 111889 \n",
      " rank = 39300 of ranks = 111889 \n",
      " rank = 39400 of ranks = 111889 \n",
      " rank = 39500 of ranks = 111889 \n",
      " rank = 39600 of ranks = 111889 \n",
      " rank = 39700 of ranks = 111889 \n",
      " rank = 39800 of ranks = 111889 \n",
      " rank = 39900 of ranks = 111889 \n",
      " rank = 40000 of ranks = 111889 \n",
      " rank = 40100 of ranks = 111889 \n",
      " rank = 40200 of ranks = 111889 \n",
      " rank = 40300 of ranks = 111889 \n",
      " rank = 40400 of ranks = 111889 \n",
      " rank = 40500 of ranks = 111889 \n",
      " rank = 40600 of ranks = 111889 \n",
      " rank = 40700 of ranks = 111889 \n",
      " rank = 40800 of ranks = 111889 \n",
      " rank = 40900 of ranks = 111889 \n",
      " rank = 41000 of ranks = 111889 \n",
      " rank = 41100 of ranks = 111889 \n",
      " rank = 41200 of ranks = 111889 \n",
      " rank = 41300 of ranks = 111889 \n",
      " rank = 41400 of ranks = 111889 \n",
      " rank = 41500 of ranks = 111889 \n",
      " rank = 41600 of ranks = 111889 \n",
      " rank = 41700 of ranks = 111889 \n",
      " rank = 41800 of ranks = 111889 \n",
      " rank = 41900 of ranks = 111889 \n",
      " rank = 42000 of ranks = 111889 \n",
      " rank = 42100 of ranks = 111889 \n",
      " rank = 42200 of ranks = 111889 \n",
      " rank = 42300 of ranks = 111889 \n",
      " rank = 42400 of ranks = 111889 \n",
      " rank = 42500 of ranks = 111889 \n",
      " rank = 42600 of ranks = 111889 \n",
      " rank = 42700 of ranks = 111889 \n",
      " rank = 42800 of ranks = 111889 \n",
      " rank = 42900 of ranks = 111889 \n",
      " rank = 43000 of ranks = 111889 \n",
      " rank = 43100 of ranks = 111889 \n",
      " rank = 43200 of ranks = 111889 \n",
      " rank = 43300 of ranks = 111889 \n",
      " rank = 43400 of ranks = 111889 \n",
      " rank = 43500 of ranks = 111889 \n",
      " rank = 43600 of ranks = 111889 \n",
      " rank = 43700 of ranks = 111889 \n",
      " rank = 43800 of ranks = 111889 \n",
      " rank = 43900 of ranks = 111889 \n",
      " rank = 44000 of ranks = 111889 \n",
      " rank = 44100 of ranks = 111889 \n",
      " rank = 44200 of ranks = 111889 \n",
      " rank = 44300 of ranks = 111889 \n",
      " rank = 44400 of ranks = 111889 \n",
      " rank = 44500 of ranks = 111889 \n",
      " rank = 44600 of ranks = 111889 \n",
      " rank = 44700 of ranks = 111889 \n",
      " rank = 44800 of ranks = 111889 \n",
      " rank = 44900 of ranks = 111889 \n",
      " rank = 45000 of ranks = 111889 \n",
      " rank = 45100 of ranks = 111889 \n",
      " rank = 45200 of ranks = 111889 \n",
      " rank = 45300 of ranks = 111889 \n",
      " rank = 45400 of ranks = 111889 \n",
      " rank = 45500 of ranks = 111889 \n",
      " rank = 45600 of ranks = 111889 \n",
      " rank = 45700 of ranks = 111889 \n",
      " rank = 45800 of ranks = 111889 \n",
      " rank = 45900 of ranks = 111889 \n",
      " rank = 46000 of ranks = 111889 \n",
      " rank = 46100 of ranks = 111889 \n",
      " rank = 46200 of ranks = 111889 \n",
      " rank = 46300 of ranks = 111889 \n",
      " rank = 46400 of ranks = 111889 \n",
      " rank = 46500 of ranks = 111889 \n",
      " rank = 46600 of ranks = 111889 \n",
      " rank = 46700 of ranks = 111889 \n",
      " rank = 46800 of ranks = 111889 \n",
      " rank = 46900 of ranks = 111889 \n",
      " rank = 47000 of ranks = 111889 \n",
      " rank = 47100 of ranks = 111889 \n",
      " rank = 47200 of ranks = 111889 \n",
      " rank = 47300 of ranks = 111889 \n",
      " rank = 47400 of ranks = 111889 \n",
      " rank = 47500 of ranks = 111889 \n",
      " rank = 47600 of ranks = 111889 \n",
      " rank = 47700 of ranks = 111889 \n",
      " rank = 47800 of ranks = 111889 \n",
      " rank = 47900 of ranks = 111889 \n",
      " rank = 48000 of ranks = 111889 \n",
      " rank = 48100 of ranks = 111889 \n",
      " rank = 48200 of ranks = 111889 \n",
      " rank = 48300 of ranks = 111889 \n",
      " rank = 48400 of ranks = 111889 \n",
      " rank = 48500 of ranks = 111889 \n",
      " rank = 48600 of ranks = 111889 \n",
      " rank = 48700 of ranks = 111889 \n",
      " rank = 48800 of ranks = 111889 \n",
      " rank = 48900 of ranks = 111889 \n",
      " rank = 49000 of ranks = 111889 \n",
      " rank = 49100 of ranks = 111889 \n",
      " rank = 49200 of ranks = 111889 \n",
      " rank = 49300 of ranks = 111889 \n",
      " rank = 49400 of ranks = 111889 \n",
      " rank = 49500 of ranks = 111889 \n",
      " rank = 49600 of ranks = 111889 \n",
      " rank = 49700 of ranks = 111889 \n",
      " rank = 49800 of ranks = 111889 \n",
      " rank = 49900 of ranks = 111889 \n",
      " rank = 50000 of ranks = 111889 \n",
      " rank = 50100 of ranks = 111889 \n",
      " rank = 50200 of ranks = 111889 \n",
      " rank = 50300 of ranks = 111889 \n",
      " rank = 50400 of ranks = 111889 \n",
      " rank = 50500 of ranks = 111889 \n",
      " rank = 50600 of ranks = 111889 \n",
      " rank = 50700 of ranks = 111889 \n",
      " rank = 50800 of ranks = 111889 \n",
      " rank = 50900 of ranks = 111889 \n",
      " rank = 51000 of ranks = 111889 \n",
      " rank = 51100 of ranks = 111889 \n",
      " rank = 51200 of ranks = 111889 \n",
      " rank = 51300 of ranks = 111889 \n",
      " rank = 51400 of ranks = 111889 \n",
      " rank = 51500 of ranks = 111889 \n",
      " rank = 51600 of ranks = 111889 \n",
      " rank = 51700 of ranks = 111889 \n",
      " rank = 51800 of ranks = 111889 \n",
      " rank = 51900 of ranks = 111889 \n",
      " rank = 52000 of ranks = 111889 \n",
      " rank = 52100 of ranks = 111889 \n",
      " rank = 52200 of ranks = 111889 \n",
      " rank = 52300 of ranks = 111889 \n",
      " rank = 52400 of ranks = 111889 \n",
      " rank = 52500 of ranks = 111889 \n",
      " rank = 52600 of ranks = 111889 \n",
      " rank = 52700 of ranks = 111889 \n",
      " rank = 52800 of ranks = 111889 \n",
      " rank = 52900 of ranks = 111889 \n",
      " rank = 53000 of ranks = 111889 \n",
      " rank = 53100 of ranks = 111889 \n",
      " rank = 53200 of ranks = 111889 \n",
      " rank = 53300 of ranks = 111889 \n",
      " rank = 53400 of ranks = 111889 \n",
      " rank = 53500 of ranks = 111889 \n",
      " rank = 53600 of ranks = 111889 \n",
      " rank = 53700 of ranks = 111889 \n",
      " rank = 53800 of ranks = 111889 \n",
      " rank = 53900 of ranks = 111889 \n",
      " rank = 54000 of ranks = 111889 \n",
      " rank = 54100 of ranks = 111889 \n",
      " rank = 54200 of ranks = 111889 \n",
      " rank = 54300 of ranks = 111889 \n",
      " rank = 54400 of ranks = 111889 \n",
      " rank = 54500 of ranks = 111889 \n",
      " rank = 54600 of ranks = 111889 \n",
      " rank = 54700 of ranks = 111889 \n",
      " rank = 54800 of ranks = 111889 \n",
      " rank = 54900 of ranks = 111889 \n",
      " rank = 55000 of ranks = 111889 \n",
      " rank = 55100 of ranks = 111889 \n",
      " rank = 55200 of ranks = 111889 \n",
      " rank = 55300 of ranks = 111889 \n",
      " rank = 55400 of ranks = 111889 \n",
      " rank = 55500 of ranks = 111889 \n",
      " rank = 55600 of ranks = 111889 \n",
      " rank = 55700 of ranks = 111889 \n",
      " rank = 55800 of ranks = 111889 \n",
      " rank = 55900 of ranks = 111889 \n",
      " rank = 56000 of ranks = 111889 \n",
      " rank = 56100 of ranks = 111889 \n",
      " rank = 56200 of ranks = 111889 \n",
      " rank = 56300 of ranks = 111889 \n",
      " rank = 56400 of ranks = 111889 \n",
      " rank = 56500 of ranks = 111889 \n",
      " rank = 56600 of ranks = 111889 \n",
      " rank = 56700 of ranks = 111889 \n",
      " rank = 56800 of ranks = 111889 \n",
      " rank = 56900 of ranks = 111889 \n",
      " rank = 57000 of ranks = 111889 \n",
      " rank = 57100 of ranks = 111889 \n",
      " rank = 57200 of ranks = 111889 \n",
      " rank = 57300 of ranks = 111889 \n",
      " rank = 57400 of ranks = 111889 \n",
      " rank = 57500 of ranks = 111889 \n",
      " rank = 57600 of ranks = 111889 \n",
      " rank = 57700 of ranks = 111889 \n",
      " rank = 57800 of ranks = 111889 \n",
      " rank = 57900 of ranks = 111889 \n",
      " rank = 58000 of ranks = 111889 \n",
      " rank = 58100 of ranks = 111889 \n",
      " rank = 58200 of ranks = 111889 \n",
      " rank = 58300 of ranks = 111889 \n",
      " rank = 58400 of ranks = 111889 \n",
      " rank = 58500 of ranks = 111889 \n",
      " rank = 58600 of ranks = 111889 \n",
      " rank = 58700 of ranks = 111889 \n",
      " rank = 58800 of ranks = 111889 \n",
      " rank = 58900 of ranks = 111889 \n",
      " rank = 59000 of ranks = 111889 \n",
      " rank = 59100 of ranks = 111889 \n",
      " rank = 59200 of ranks = 111889 \n",
      " rank = 59300 of ranks = 111889 \n",
      " rank = 59400 of ranks = 111889 \n",
      " rank = 59500 of ranks = 111889 \n",
      " rank = 59600 of ranks = 111889 \n",
      " rank = 59700 of ranks = 111889 \n",
      " rank = 59800 of ranks = 111889 \n",
      " rank = 59900 of ranks = 111889 \n",
      " rank = 60000 of ranks = 111889 \n",
      " rank = 60100 of ranks = 111889 \n",
      " rank = 60200 of ranks = 111889 \n",
      " rank = 60300 of ranks = 111889 \n",
      " rank = 60400 of ranks = 111889 \n",
      " rank = 60500 of ranks = 111889 \n",
      " rank = 60600 of ranks = 111889 \n",
      " rank = 60700 of ranks = 111889 \n",
      " rank = 60800 of ranks = 111889 \n",
      " rank = 60900 of ranks = 111889 \n",
      " rank = 61000 of ranks = 111889 \n",
      " rank = 61100 of ranks = 111889 \n",
      " rank = 61200 of ranks = 111889 \n",
      " rank = 61300 of ranks = 111889 \n",
      " rank = 61400 of ranks = 111889 \n",
      " rank = 61500 of ranks = 111889 \n",
      " rank = 61600 of ranks = 111889 \n",
      " rank = 61700 of ranks = 111889 \n",
      " rank = 61800 of ranks = 111889 \n",
      " rank = 61900 of ranks = 111889 \n",
      " rank = 62000 of ranks = 111889 \n",
      " rank = 62100 of ranks = 111889 \n",
      " rank = 62200 of ranks = 111889 \n",
      " rank = 62300 of ranks = 111889 \n",
      " rank = 62400 of ranks = 111889 \n",
      " rank = 62500 of ranks = 111889 \n",
      " rank = 62600 of ranks = 111889 \n",
      " rank = 62700 of ranks = 111889 \n",
      " rank = 62800 of ranks = 111889 \n",
      " rank = 62900 of ranks = 111889 \n",
      " rank = 63000 of ranks = 111889 \n",
      " rank = 63100 of ranks = 111889 \n",
      " rank = 63200 of ranks = 111889 \n",
      " rank = 63300 of ranks = 111889 \n",
      " rank = 63400 of ranks = 111889 \n",
      " rank = 63500 of ranks = 111889 \n",
      " rank = 63600 of ranks = 111889 \n",
      " rank = 63700 of ranks = 111889 \n",
      " rank = 63800 of ranks = 111889 \n",
      " rank = 63900 of ranks = 111889 \n",
      " rank = 64000 of ranks = 111889 \n",
      " rank = 64100 of ranks = 111889 \n",
      " rank = 64200 of ranks = 111889 \n",
      " rank = 64300 of ranks = 111889 \n",
      " rank = 64400 of ranks = 111889 \n",
      " rank = 64500 of ranks = 111889 \n",
      " rank = 64600 of ranks = 111889 \n",
      " rank = 64700 of ranks = 111889 \n",
      " rank = 64800 of ranks = 111889 \n",
      " rank = 64900 of ranks = 111889 \n",
      " rank = 65000 of ranks = 111889 \n",
      " rank = 65100 of ranks = 111889 \n",
      " rank = 65200 of ranks = 111889 \n",
      " rank = 65300 of ranks = 111889 \n",
      " rank = 65400 of ranks = 111889 \n",
      " rank = 65500 of ranks = 111889 \n",
      " rank = 65600 of ranks = 111889 \n",
      " rank = 65700 of ranks = 111889 \n",
      " rank = 65800 of ranks = 111889 \n",
      " rank = 65900 of ranks = 111889 \n",
      " rank = 66000 of ranks = 111889 \n",
      " rank = 66100 of ranks = 111889 \n",
      " rank = 66200 of ranks = 111889 \n",
      " rank = 66300 of ranks = 111889 \n",
      " rank = 66400 of ranks = 111889 \n",
      " rank = 66500 of ranks = 111889 \n",
      " rank = 66600 of ranks = 111889 \n",
      " rank = 66700 of ranks = 111889 \n",
      " rank = 66800 of ranks = 111889 \n",
      " rank = 66900 of ranks = 111889 \n",
      " rank = 67000 of ranks = 111889 \n",
      " rank = 67100 of ranks = 111889 \n",
      " rank = 67200 of ranks = 111889 \n",
      " rank = 67300 of ranks = 111889 \n",
      " rank = 67400 of ranks = 111889 \n",
      " rank = 67500 of ranks = 111889 \n",
      " rank = 67600 of ranks = 111889 \n",
      " rank = 67700 of ranks = 111889 \n",
      " rank = 67800 of ranks = 111889 \n",
      " rank = 67900 of ranks = 111889 \n",
      " rank = 68000 of ranks = 111889 \n",
      " rank = 68100 of ranks = 111889 \n",
      " rank = 68200 of ranks = 111889 \n",
      " rank = 68300 of ranks = 111889 \n",
      " rank = 68400 of ranks = 111889 \n",
      " rank = 68500 of ranks = 111889 \n",
      " rank = 68600 of ranks = 111889 \n",
      " rank = 68700 of ranks = 111889 \n",
      " rank = 68800 of ranks = 111889 \n",
      " rank = 68900 of ranks = 111889 \n",
      " rank = 69000 of ranks = 111889 \n",
      " rank = 69100 of ranks = 111889 \n",
      " rank = 69200 of ranks = 111889 \n",
      " rank = 69300 of ranks = 111889 \n",
      " rank = 69400 of ranks = 111889 \n",
      " rank = 69500 of ranks = 111889 \n",
      " rank = 69600 of ranks = 111889 \n",
      " rank = 69700 of ranks = 111889 \n",
      " rank = 69800 of ranks = 111889 \n",
      " rank = 69900 of ranks = 111889 \n",
      " rank = 70000 of ranks = 111889 \n",
      " rank = 70100 of ranks = 111889 \n",
      " rank = 70200 of ranks = 111889 \n",
      " rank = 70300 of ranks = 111889 \n",
      " rank = 70400 of ranks = 111889 \n",
      " rank = 70500 of ranks = 111889 \n",
      " rank = 70600 of ranks = 111889 \n",
      " rank = 70700 of ranks = 111889 \n",
      " rank = 70800 of ranks = 111889 \n",
      " rank = 70900 of ranks = 111889 \n",
      " rank = 71000 of ranks = 111889 \n",
      " rank = 71100 of ranks = 111889 \n",
      " rank = 71200 of ranks = 111889 \n",
      " rank = 71300 of ranks = 111889 \n",
      " rank = 71400 of ranks = 111889 \n",
      " rank = 71500 of ranks = 111889 \n",
      " rank = 71600 of ranks = 111889 \n",
      " rank = 71700 of ranks = 111889 \n",
      " rank = 71800 of ranks = 111889 \n",
      " rank = 71900 of ranks = 111889 \n",
      " rank = 72000 of ranks = 111889 \n",
      " rank = 72100 of ranks = 111889 \n",
      " rank = 72200 of ranks = 111889 \n",
      " rank = 72300 of ranks = 111889 \n",
      " rank = 72400 of ranks = 111889 \n",
      " rank = 72500 of ranks = 111889 \n",
      " rank = 72600 of ranks = 111889 \n",
      " rank = 72700 of ranks = 111889 \n",
      " rank = 72800 of ranks = 111889 \n",
      " rank = 72900 of ranks = 111889 \n",
      " rank = 73000 of ranks = 111889 \n",
      " rank = 73100 of ranks = 111889 \n",
      " rank = 73200 of ranks = 111889 \n",
      " rank = 73300 of ranks = 111889 \n",
      " rank = 73400 of ranks = 111889 \n",
      " rank = 73500 of ranks = 111889 \n",
      " rank = 73600 of ranks = 111889 \n",
      " rank = 73700 of ranks = 111889 \n",
      " rank = 73800 of ranks = 111889 \n",
      " rank = 73900 of ranks = 111889 \n",
      " rank = 74000 of ranks = 111889 \n",
      " rank = 74100 of ranks = 111889 \n",
      " rank = 74200 of ranks = 111889 \n",
      " rank = 74300 of ranks = 111889 \n",
      " rank = 74400 of ranks = 111889 \n",
      " rank = 74500 of ranks = 111889 \n",
      " rank = 74600 of ranks = 111889 \n",
      " rank = 74700 of ranks = 111889 \n",
      " rank = 74800 of ranks = 111889 \n",
      " rank = 74900 of ranks = 111889 \n",
      " rank = 75000 of ranks = 111889 \n",
      " rank = 75100 of ranks = 111889 \n",
      " rank = 75200 of ranks = 111889 \n",
      " rank = 75300 of ranks = 111889 \n",
      " rank = 75400 of ranks = 111889 \n",
      " rank = 75500 of ranks = 111889 \n",
      " rank = 75600 of ranks = 111889 \n",
      " rank = 75700 of ranks = 111889 \n",
      " rank = 75800 of ranks = 111889 \n",
      " rank = 75900 of ranks = 111889 \n",
      " rank = 76000 of ranks = 111889 \n",
      " rank = 76100 of ranks = 111889 \n",
      " rank = 76200 of ranks = 111889 \n",
      " rank = 76300 of ranks = 111889 \n",
      " rank = 76400 of ranks = 111889 \n",
      " rank = 76500 of ranks = 111889 \n",
      " rank = 76600 of ranks = 111889 \n",
      " rank = 76700 of ranks = 111889 \n",
      " rank = 76800 of ranks = 111889 \n",
      " rank = 76900 of ranks = 111889 \n",
      " rank = 77000 of ranks = 111889 \n",
      " rank = 77100 of ranks = 111889 \n",
      " rank = 77200 of ranks = 111889 \n",
      " rank = 77300 of ranks = 111889 \n",
      " rank = 77400 of ranks = 111889 \n",
      " rank = 77500 of ranks = 111889 \n",
      " rank = 77600 of ranks = 111889 \n",
      " rank = 77700 of ranks = 111889 \n",
      " rank = 77800 of ranks = 111889 \n",
      " rank = 77900 of ranks = 111889 \n",
      " rank = 78000 of ranks = 111889 \n",
      " rank = 78100 of ranks = 111889 \n",
      " rank = 78200 of ranks = 111889 \n",
      " rank = 78300 of ranks = 111889 \n",
      " rank = 78400 of ranks = 111889 \n",
      " rank = 78500 of ranks = 111889 \n",
      " rank = 78600 of ranks = 111889 \n",
      " rank = 78700 of ranks = 111889 \n",
      " rank = 78800 of ranks = 111889 \n",
      " rank = 78900 of ranks = 111889 \n",
      " rank = 79000 of ranks = 111889 \n",
      " rank = 79100 of ranks = 111889 \n",
      " rank = 79200 of ranks = 111889 \n",
      " rank = 79300 of ranks = 111889 \n",
      " rank = 79400 of ranks = 111889 \n",
      " rank = 79500 of ranks = 111889 \n",
      " rank = 79600 of ranks = 111889 \n",
      " rank = 79700 of ranks = 111889 \n",
      " rank = 79800 of ranks = 111889 \n",
      " rank = 79900 of ranks = 111889 \n",
      " rank = 80000 of ranks = 111889 \n",
      " rank = 80100 of ranks = 111889 \n",
      " rank = 80200 of ranks = 111889 \n",
      " rank = 80300 of ranks = 111889 \n",
      " rank = 80400 of ranks = 111889 \n",
      " rank = 80500 of ranks = 111889 \n",
      " rank = 80600 of ranks = 111889 \n",
      " rank = 80700 of ranks = 111889 \n",
      " rank = 80800 of ranks = 111889 \n",
      " rank = 80900 of ranks = 111889 \n",
      " rank = 81000 of ranks = 111889 \n",
      " rank = 81100 of ranks = 111889 \n",
      " rank = 81200 of ranks = 111889 \n",
      " rank = 81300 of ranks = 111889 \n",
      " rank = 81400 of ranks = 111889 \n",
      " rank = 81500 of ranks = 111889 \n",
      " rank = 81600 of ranks = 111889 \n",
      " rank = 81700 of ranks = 111889 \n",
      " rank = 81800 of ranks = 111889 \n",
      " rank = 81900 of ranks = 111889 \n",
      " rank = 82000 of ranks = 111889 \n",
      " rank = 82100 of ranks = 111889 \n",
      " rank = 82200 of ranks = 111889 \n",
      " rank = 82300 of ranks = 111889 \n",
      " rank = 82400 of ranks = 111889 \n",
      " rank = 82500 of ranks = 111889 \n",
      " rank = 82600 of ranks = 111889 \n",
      " rank = 82700 of ranks = 111889 \n",
      " rank = 82800 of ranks = 111889 \n",
      " rank = 82900 of ranks = 111889 \n",
      " rank = 83000 of ranks = 111889 \n",
      " rank = 83100 of ranks = 111889 \n",
      " rank = 83200 of ranks = 111889 \n",
      " rank = 83300 of ranks = 111889 \n",
      " rank = 83400 of ranks = 111889 \n",
      " rank = 83500 of ranks = 111889 \n",
      " rank = 83600 of ranks = 111889 \n",
      " rank = 83700 of ranks = 111889 \n",
      " rank = 83800 of ranks = 111889 \n",
      " rank = 83900 of ranks = 111889 \n",
      " rank = 84000 of ranks = 111889 \n",
      " rank = 84100 of ranks = 111889 \n",
      " rank = 84200 of ranks = 111889 \n",
      " rank = 84300 of ranks = 111889 \n",
      " rank = 84400 of ranks = 111889 \n",
      " rank = 84500 of ranks = 111889 \n",
      " rank = 84600 of ranks = 111889 \n",
      " rank = 84700 of ranks = 111889 \n",
      " rank = 84800 of ranks = 111889 \n",
      " rank = 84900 of ranks = 111889 \n",
      " rank = 85000 of ranks = 111889 \n",
      " rank = 85100 of ranks = 111889 \n",
      " rank = 85200 of ranks = 111889 \n",
      " rank = 85300 of ranks = 111889 \n",
      " rank = 85400 of ranks = 111889 \n",
      " rank = 85500 of ranks = 111889 \n",
      " rank = 85600 of ranks = 111889 \n",
      " rank = 85700 of ranks = 111889 \n",
      " rank = 85800 of ranks = 111889 \n",
      " rank = 85900 of ranks = 111889 \n",
      " rank = 86000 of ranks = 111889 \n",
      " rank = 86100 of ranks = 111889 \n",
      " rank = 86200 of ranks = 111889 \n",
      " rank = 86300 of ranks = 111889 \n",
      " rank = 86400 of ranks = 111889 \n",
      " rank = 86500 of ranks = 111889 \n",
      " rank = 86600 of ranks = 111889 \n",
      " rank = 86700 of ranks = 111889 \n",
      " rank = 86800 of ranks = 111889 \n",
      " rank = 86900 of ranks = 111889 \n",
      " rank = 87000 of ranks = 111889 \n",
      " rank = 87100 of ranks = 111889 \n",
      " rank = 87200 of ranks = 111889 \n",
      " rank = 87300 of ranks = 111889 \n",
      " rank = 87400 of ranks = 111889 \n",
      " rank = 87500 of ranks = 111889 \n",
      " rank = 87600 of ranks = 111889 \n",
      " rank = 87700 of ranks = 111889 \n",
      " rank = 87800 of ranks = 111889 \n",
      " rank = 87900 of ranks = 111889 \n",
      " rank = 88000 of ranks = 111889 \n",
      " rank = 88100 of ranks = 111889 \n",
      " rank = 88200 of ranks = 111889 \n",
      " rank = 88300 of ranks = 111889 \n",
      " rank = 88400 of ranks = 111889 \n",
      " rank = 88500 of ranks = 111889 \n",
      " rank = 88600 of ranks = 111889 \n",
      " rank = 88700 of ranks = 111889 \n",
      " rank = 88800 of ranks = 111889 \n",
      " rank = 88900 of ranks = 111889 \n",
      " rank = 89000 of ranks = 111889 \n",
      " rank = 89100 of ranks = 111889 \n",
      " rank = 89200 of ranks = 111889 \n",
      " rank = 89300 of ranks = 111889 \n",
      " rank = 89400 of ranks = 111889 \n",
      " rank = 89500 of ranks = 111889 \n",
      " rank = 89600 of ranks = 111889 \n",
      " rank = 89700 of ranks = 111889 \n",
      " rank = 89800 of ranks = 111889 \n",
      " rank = 89900 of ranks = 111889 \n",
      " rank = 90000 of ranks = 111889 \n",
      " rank = 90100 of ranks = 111889 \n",
      " rank = 90200 of ranks = 111889 \n",
      " rank = 90300 of ranks = 111889 \n",
      " rank = 90400 of ranks = 111889 \n",
      " rank = 90500 of ranks = 111889 \n",
      " rank = 90600 of ranks = 111889 \n",
      " rank = 90700 of ranks = 111889 \n",
      " rank = 90800 of ranks = 111889 \n",
      " rank = 90900 of ranks = 111889 \n",
      " rank = 91000 of ranks = 111889 \n",
      " rank = 91100 of ranks = 111889 \n",
      " rank = 91200 of ranks = 111889 \n",
      " rank = 91300 of ranks = 111889 \n",
      " rank = 91400 of ranks = 111889 \n",
      " rank = 91500 of ranks = 111889 \n",
      " rank = 91600 of ranks = 111889 \n",
      " rank = 91700 of ranks = 111889 \n",
      " rank = 91800 of ranks = 111889 \n",
      " rank = 91900 of ranks = 111889 \n",
      " rank = 92000 of ranks = 111889 \n",
      " rank = 92100 of ranks = 111889 \n",
      " rank = 92200 of ranks = 111889 \n",
      " rank = 92300 of ranks = 111889 \n",
      " rank = 92400 of ranks = 111889 \n",
      " rank = 92500 of ranks = 111889 \n",
      " rank = 92600 of ranks = 111889 \n",
      " rank = 92700 of ranks = 111889 \n",
      " rank = 92800 of ranks = 111889 \n",
      " rank = 92900 of ranks = 111889 \n",
      " rank = 93000 of ranks = 111889 \n",
      " rank = 93100 of ranks = 111889 \n",
      " rank = 93200 of ranks = 111889 \n",
      " rank = 93300 of ranks = 111889 \n",
      " rank = 93400 of ranks = 111889 \n",
      " rank = 93500 of ranks = 111889 \n",
      " rank = 93600 of ranks = 111889 \n",
      " rank = 93700 of ranks = 111889 \n",
      " rank = 93800 of ranks = 111889 \n",
      " rank = 93900 of ranks = 111889 \n",
      " rank = 94000 of ranks = 111889 \n",
      " rank = 94100 of ranks = 111889 \n",
      " rank = 94200 of ranks = 111889 \n",
      " rank = 94300 of ranks = 111889 \n",
      " rank = 94400 of ranks = 111889 \n",
      " rank = 94500 of ranks = 111889 \n",
      " rank = 94600 of ranks = 111889 \n",
      " rank = 94700 of ranks = 111889 \n",
      " rank = 94800 of ranks = 111889 \n",
      " rank = 94900 of ranks = 111889 \n",
      " rank = 95000 of ranks = 111889 \n",
      " rank = 95100 of ranks = 111889 \n",
      " rank = 95200 of ranks = 111889 \n",
      " rank = 95300 of ranks = 111889 \n",
      " rank = 95400 of ranks = 111889 \n",
      " rank = 95500 of ranks = 111889 \n",
      " rank = 95600 of ranks = 111889 \n",
      " rank = 95700 of ranks = 111889 \n",
      " rank = 95800 of ranks = 111889 \n",
      " rank = 95900 of ranks = 111889 \n",
      " rank = 96000 of ranks = 111889 \n",
      " rank = 96100 of ranks = 111889 \n",
      " rank = 96200 of ranks = 111889 \n",
      " rank = 96300 of ranks = 111889 \n",
      " rank = 96400 of ranks = 111889 \n",
      " rank = 96500 of ranks = 111889 \n",
      " rank = 96600 of ranks = 111889 \n",
      " rank = 96700 of ranks = 111889 \n",
      " rank = 96800 of ranks = 111889 \n",
      " rank = 96900 of ranks = 111889 \n",
      " rank = 97000 of ranks = 111889 \n",
      " rank = 97100 of ranks = 111889 \n",
      " rank = 97200 of ranks = 111889 \n",
      " rank = 97300 of ranks = 111889 \n",
      " rank = 97400 of ranks = 111889 \n",
      " rank = 97500 of ranks = 111889 \n",
      " rank = 97600 of ranks = 111889 \n",
      " rank = 97700 of ranks = 111889 \n",
      " rank = 97800 of ranks = 111889 \n",
      " rank = 97900 of ranks = 111889 \n",
      " rank = 98000 of ranks = 111889 \n",
      " rank = 98100 of ranks = 111889 \n",
      " rank = 98200 of ranks = 111889 \n",
      " rank = 98300 of ranks = 111889 \n",
      " rank = 98400 of ranks = 111889 \n",
      " rank = 98500 of ranks = 111889 \n",
      " rank = 98600 of ranks = 111889 \n",
      " rank = 98700 of ranks = 111889 \n",
      " rank = 98800 of ranks = 111889 \n",
      " rank = 98900 of ranks = 111889 \n",
      " rank = 99000 of ranks = 111889 \n",
      " rank = 99100 of ranks = 111889 \n",
      " rank = 99200 of ranks = 111889 \n",
      " rank = 99300 of ranks = 111889 \n",
      " rank = 99400 of ranks = 111889 \n",
      " rank = 99500 of ranks = 111889 \n",
      " rank = 99600 of ranks = 111889 \n",
      " rank = 99700 of ranks = 111889 \n",
      " rank = 99800 of ranks = 111889 \n",
      " rank = 99900 of ranks = 111889 \n",
      " rank = 100000 of ranks = 111889 \n",
      " rank = 100100 of ranks = 111889 \n",
      " rank = 100200 of ranks = 111889 \n",
      " rank = 100300 of ranks = 111889 \n",
      " rank = 100400 of ranks = 111889 \n",
      " rank = 100500 of ranks = 111889 \n",
      " rank = 100600 of ranks = 111889 \n",
      " rank = 100700 of ranks = 111889 \n",
      " rank = 100800 of ranks = 111889 \n",
      " rank = 100900 of ranks = 111889 \n",
      " rank = 101000 of ranks = 111889 \n",
      " rank = 101100 of ranks = 111889 \n",
      " rank = 101200 of ranks = 111889 \n",
      " rank = 101300 of ranks = 111889 \n",
      " rank = 101400 of ranks = 111889 \n",
      " rank = 101500 of ranks = 111889 \n",
      " rank = 101600 of ranks = 111889 \n",
      " rank = 101700 of ranks = 111889 \n",
      " rank = 101800 of ranks = 111889 \n",
      " rank = 101900 of ranks = 111889 \n",
      " rank = 102000 of ranks = 111889 \n",
      " rank = 102100 of ranks = 111889 \n",
      " rank = 102200 of ranks = 111889 \n",
      " rank = 102300 of ranks = 111889 \n",
      " rank = 102400 of ranks = 111889 \n",
      " rank = 102500 of ranks = 111889 \n",
      " rank = 102600 of ranks = 111889 \n",
      " rank = 102700 of ranks = 111889 \n",
      " rank = 102800 of ranks = 111889 \n",
      " rank = 102900 of ranks = 111889 \n",
      " rank = 103000 of ranks = 111889 \n",
      " rank = 103100 of ranks = 111889 \n",
      " rank = 103200 of ranks = 111889 \n",
      " rank = 103300 of ranks = 111889 \n",
      " rank = 103400 of ranks = 111889 \n",
      " rank = 103500 of ranks = 111889 \n",
      " rank = 103600 of ranks = 111889 \n",
      " rank = 103700 of ranks = 111889 \n",
      " rank = 103800 of ranks = 111889 \n",
      " rank = 103900 of ranks = 111889 \n",
      " rank = 104000 of ranks = 111889 \n",
      " rank = 104100 of ranks = 111889 \n",
      " rank = 104200 of ranks = 111889 \n",
      " rank = 104300 of ranks = 111889 \n",
      " rank = 104400 of ranks = 111889 \n",
      " rank = 104500 of ranks = 111889 \n",
      " rank = 104600 of ranks = 111889 \n",
      " rank = 104700 of ranks = 111889 \n",
      " rank = 104800 of ranks = 111889 \n",
      " rank = 104900 of ranks = 111889 \n",
      " rank = 105000 of ranks = 111889 \n",
      " rank = 105100 of ranks = 111889 \n",
      " rank = 105200 of ranks = 111889 \n",
      " rank = 105300 of ranks = 111889 \n",
      " rank = 105400 of ranks = 111889 \n",
      " rank = 105500 of ranks = 111889 \n",
      " rank = 105600 of ranks = 111889 \n",
      " rank = 105700 of ranks = 111889 \n",
      " rank = 105800 of ranks = 111889 \n",
      " rank = 105900 of ranks = 111889 \n",
      " rank = 106000 of ranks = 111889 \n",
      " rank = 106100 of ranks = 111889 \n",
      " rank = 106200 of ranks = 111889 \n",
      " rank = 106300 of ranks = 111889 \n",
      " rank = 106400 of ranks = 111889 \n",
      " rank = 106500 of ranks = 111889 \n",
      " rank = 106600 of ranks = 111889 \n",
      " rank = 106700 of ranks = 111889 \n",
      " rank = 106800 of ranks = 111889 \n",
      " rank = 106900 of ranks = 111889 \n",
      " rank = 107000 of ranks = 111889 \n",
      " rank = 107100 of ranks = 111889 \n",
      " rank = 107200 of ranks = 111889 \n",
      " rank = 107300 of ranks = 111889 \n",
      " rank = 107400 of ranks = 111889 \n",
      " rank = 107500 of ranks = 111889 \n",
      " rank = 107600 of ranks = 111889 \n",
      " rank = 107700 of ranks = 111889 \n",
      " rank = 107800 of ranks = 111889 \n",
      " rank = 107900 of ranks = 111889 \n",
      " rank = 108000 of ranks = 111889 \n",
      " rank = 108100 of ranks = 111889 \n",
      " rank = 108200 of ranks = 111889 \n",
      " rank = 108300 of ranks = 111889 \n",
      " rank = 108400 of ranks = 111889 \n",
      " rank = 108500 of ranks = 111889 \n",
      " rank = 108600 of ranks = 111889 \n",
      " rank = 108700 of ranks = 111889 \n",
      " rank = 108800 of ranks = 111889 \n",
      " rank = 108900 of ranks = 111889 \n",
      " rank = 109000 of ranks = 111889 \n",
      " rank = 109100 of ranks = 111889 \n",
      " rank = 109200 of ranks = 111889 \n",
      " rank = 109300 of ranks = 111889 \n",
      " rank = 109400 of ranks = 111889 \n",
      " rank = 109500 of ranks = 111889 \n",
      " rank = 109600 of ranks = 111889 \n",
      " rank = 109700 of ranks = 111889 \n",
      " rank = 109800 of ranks = 111889 \n",
      " rank = 109900 of ranks = 111889 \n",
      " rank = 110000 of ranks = 111889 \n",
      " rank = 110100 of ranks = 111889 \n",
      " rank = 110200 of ranks = 111889 \n",
      " rank = 110300 of ranks = 111889 \n",
      " rank = 110400 of ranks = 111889 \n",
      " rank = 110500 of ranks = 111889 \n",
      " rank = 110600 of ranks = 111889 \n",
      " rank = 110700 of ranks = 111889 \n",
      " rank = 110800 of ranks = 111889 \n",
      " rank = 110Total Detection Time: 17 Seconds\u001b[0m\n",
      "\u001b[34m900 of ranks = 111889 \n",
      " rank = 111000 of ranks = 111889 \n",
      " rank = 111100 of ranks = 111889 \n",
      " rank = 111200 of ranks = 111889 \n",
      " rank = 111300 of ranks = 111889 \n",
      " rank = 111400 of ranks = 111889 \n",
      " rank = 111500 of ranks = 111889 \n",
      " rank = 111600 of ranks = 111889 \n",
      " rank = 111700 of ranks = 111889 \n",
      " rank = 111800 of ranks = 111889 \u001b[0m\n",
      "\u001b[34mclass_id = 0, name = tiger, ap = 0.00%   #011 (TP = 0, FP = 111889) \n",
      "\n",
      " for conf_thresh = 0.25, precision = 0.00, recall = 0.00, F1-score = -nan \n",
      " for conf_thresh = 0.25, TP = 0, FP = 111889, FN = 544, average IoU = 0.00 % \n",
      "\n",
      " IoU threshold = 50 %, used Area-Under-Curve for each unique Recall \n",
      " mean average precision (mAP@0.50) = 0.000000, or 0.00 % \n",
      "\u001b[0m\n",
      "\u001b[34mSet -points flag:\n",
      " `-points 101` for MS COCO \n",
      " `-points 11` for PascalVOC 2007 (uncomment `difficult` in voc.data) \n",
      " `-points 0` (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset\u001b[0m\n",
      "\u001b[34m CUDA-version: 11010 (11010), GPU count: 1  \n",
      " OpenCV isn't used - data augmentation will be slow \u001b[0m\n",
      "\u001b[34mresults: Using default 'results'\n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 2    224 x 224 x   3 ->  112 x 112 x  16 0.011 BF\n",
      "   1 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   2 conv     16/  16  3 x 3/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.004 BF\n",
      "   3 avg                           112 x 112 x  16 ->     16\n",
      "   4 conv      4       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x   4 0.000 BF\n",
      "   5 conv     16       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  16 0.000 BF\n",
      "   6 scale Layer: 2\n",
      "   7 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   8 conv     48       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  48 0.019 BF\n",
      "   9 conv     48/  48  3 x 3/ 2    112 x 112 x  48 ->   56 x  56 x  48 0.003 BF\n",
      "  10 avg                            56 x  56 x  48 ->     48\n",
      "  11 conv     16       1 x 1/ 1      1 x   1 x  48 ->    1 x   1 x  16 0.000 BF\n",
      "  12 conv     48       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  48 0.000 BF\n",
      "  13 scale Layer: 9\n",
      "  14 conv     24       1 x 1/ 1     56 x  56 x  48 ->   56 x  56 x  24 0.007 BF\n",
      "  15 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  16 conv     72/  72  3 x 3/ 1     56 x  56 x  72 ->   56 x  56 x  72 0.004 BF\n",
      "  17 avg                            56 x  56 x  72 ->     72\n",
      "  18 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  19 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  20 scale Layer: 16\n",
      "  21 conv     24       1 x 1/ 1     56 x  56 x  72 ->   56 x  56 x  24 0.011 BF\n",
      "  22 dropout    p = 0.200        75264  ->   75264\n",
      "  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs:  56 x  56 x  24 0.000 BF\n",
      "  24 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  25 conv     72/  72  5 x 5/ 2     56 x  56 x  72 ->   28 x  28 x  72 0.003 BF\n",
      "  26 avg                            28 x  28 x  72 ->     72\n",
      "  27 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  28 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  29 scale Layer: 25\n",
      "  30 conv     20       1 x 1/ 1     28 x  28 x  72 ->   28 x  28 x  20 0.002 BF\n",
      "  31 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  32 conv     96/  96  5 x 5/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.004 BF\n",
      "  33 avg                            28 x  28 x  96 ->     96\n",
      "  34 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  35 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  36 scale Layer: 32\n",
      "  37 conv     20       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  20 0.003 BF\n",
      "  38 dropout    p = 0.200        15680  ->   15680\n",
      "  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  28 x  28 x  20 0.000 BF\n",
      "  40 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  41 conv     96/  96  3 x 3/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.001 BF\n",
      "  42 avg                            28 x  28 x  96 ->     96\n",
      "  43 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  44 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  45 scale Layer: 41\n",
      "  46 conv     40       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  40 0.006 BF\n",
      "  47 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  48 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  49 avg                            28 x  28 x 192 ->    192\n",
      "  50 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  51 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  52 scale Layer: 48\n",
      "  53 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  54 dropout    p = 0.200        31360  ->   31360\n",
      "  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  56 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  57 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  58 avg                            28 x  28 x 192 ->    192\n",
      "  59 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  60 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  61 scale Layer: 57\n",
      "  62 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  63 dropout    p = 0.200        31360  ->   31360\n",
      "  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  65 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  66 conv    192/ 192  5 x 5/ 2     28 x  28 x 192 ->   14 x  14 x 192 0.002 BF\n",
      "  67 avg                            14 x  14 x 192 ->    192\n",
      "  68 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  69 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  70 scale Layer: 66\n",
      "  71 conv     56       1 x 1/ 1     14 x  14 x 192 ->   14 x  14 x  56 0.004 BF\n",
      "  72 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  73 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  74 avg                            14 x  14 x 288 ->    288\n",
      "  75 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  76 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  77 scale Layer: 73\n",
      "  78 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  79 dropout    p = 0.200        10976  ->   10976\n",
      "  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  81 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  82 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  83 avg                            14 x  14 x 288 ->    288\n",
      "  84 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  85 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  86 scale Layer: 82\n",
      "  87 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  88 dropout    p = 0.200        10976  ->   10976\n",
      "  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  90 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  91 conv    288/ 288  5 x 5/ 2     14 x  14 x 288 ->    7 x   7 x 288 0.001 BF\n",
      "  92 avg                             7 x   7 x 288 ->    288\n",
      "  93 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  94 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  95 scale Layer: 91\n",
      "  96 conv     96       1 x 1/ 1      7 x   7 x 288 ->    7 x   7 x  96 0.003 BF\n",
      "  97 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      "  98 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      "  99 avg                             7 x   7 x 480 ->    480\n",
      " 100 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 101 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 102 scale Layer: 98\n",
      " 103 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 104 dropout    p = 0.200        4704  ->   4704\n",
      " 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 106 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 107 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 108 avg                             7 x   7 x 480 ->    480\n",
      " 109 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 110 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 111 scale Layer: 107\n",
      " 112 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 113 dropout    p = 0.200        4704  ->   4704\n",
      " 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 115 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 116 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 117 avg                             7 x   7 x 480 ->    480\n",
      " 118 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 119 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 120 scale Layer: 116\n",
      " 121 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 122 dropout    p = 0.200        4704  ->   4704\n",
      " 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 124 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 125 conv    480/ 480  3 x 3/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.000 BF\n",
      " 126 avg                             7 x   7 x 480 ->    480\n",
      " 127 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 128 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 129 scale Layer: 125\n",
      " 130 conv    160       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x 160 0.008 BF\n",
      " 131 conv    640       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 640 0.010 BF\n",
      " 132 conv     18       1 x 1/ 1      7 x   7 x 640 ->    7 x   7 x  18 0.001 BF\n",
      " 133 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 134 route  130 #011#011                           ->    7 x   7 x 160 \n",
      " 135 conv    256       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 256 0.004 BF\n",
      " 136 upsample                 2x     7 x   7 x 256 ->   14 x  14 x 256\n",
      " 137 route  136 89 #011                           ->   14 x  14 x 312 \n",
      " 138 conv     64       1 x 1/ 1     14 x  14 x 312 ->   14 x  14 x  64 0.008 BF\n",
      " 139 conv    128       3 x 3/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.029 BF\n",
      " 140 conv     18       1 x 1/ 1     14 x  14 x 128 ->   14 x  14 x  18 0.001 BF\n",
      " 141 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 142 route  138 #011#011                           ->   14 x  14 x  64 \n",
      " 143 conv    128       1 x 1/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.003 BF\n",
      " 144 upsample                 2x    14 x  14 x 128 ->   28 x  28 x 128\n",
      " 145 route  144 64 #011                           ->   28 x  28 x 168 \n",
      " 146 conv     64       1 x 1/ 1     28 x  28 x 168 ->   28 x  28 x  64 0.017 BF\n",
      " 147 conv    128       3 x 3/ 1     28 x  28 x  64 ->   28 x  28 x 128 0.116 BF\n",
      " 148 conv     18       1 x 1/ 1     28 x  28 x 128 ->   28 x  28 x  18 0.004 BF\n",
      " 149 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\u001b[0m\n",
      "\u001b[34mTotal BFLOPS 0.466 \u001b[0m\n",
      "\u001b[34mavg_outputs = 54324 \n",
      " Allocate additional workspace_size = 1.81 MB \u001b[0m\n",
      "\u001b[34mLoading weights from backup/yolo-mini-tiger_final.weights...net.optimized_memory = 0 \u001b[0m\n",
      "\u001b[34mmini_batch = 1, batch = 8, time_steps = 1, train = 0 \u001b[0m\n",
      "\u001b[34mCreate CUDA-stream - 0 \u001b[0m\n",
      "\u001b[34mDone! Loaded 150 layers from weights-file \u001b[0m\n",
      "\u001b[34mLearning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\u001b[0m\n",
      "\u001b[34meval: Using default 'voc'\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34m12\u001b[0m\n",
      "\u001b[34m16\u001b[0m\n",
      "\u001b[34m20\u001b[0m\n",
      "\u001b[34m24\u001b[0m\n",
      "\u001b[34m28\u001b[0m\n",
      "\u001b[34m32\u001b[0m\n",
      "\u001b[34m36\u001b[0m\n",
      "\u001b[34m40\u001b[0m\n",
      "\u001b[34m44\u001b[0m\n",
      "\u001b[34m48\u001b[0m\n",
      "\u001b[34m52\u001b[0m\n",
      "\u001b[34m56\u001b[0m\n",
      "\u001b[34m60\u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34m68\u001b[0m\n",
      "\u001b[34m72\u001b[0m\n",
      "\u001b[34m76\u001b[0m\n",
      "\u001b[34m80\u001b[0m\n",
      "\u001b[34m84\u001b[0m\n",
      "\u001b[34m88\u001b[0m\n",
      "\u001b[34m92\u001b[0m\n",
      "\u001b[34m96\u001b[0m\n",
      "\u001b[34m100\u001b[0m\n",
      "\u001b[34m104\u001b[0m\n",
      "\u001b[34m108\u001b[0m\n",
      "\u001b[34m112\u001b[0m\n",
      "\u001b[34m116\u001b[0m\n",
      "\u001b[34m120\u001b[0m\n",
      "\u001b[34m124\u001b[0m\n",
      "\u001b[34m128\u001b[0m\n",
      "\u001b[34m132\u001b[0m\n",
      "\u001b[34m136\u001b[0m\n",
      "\u001b[34m140\u001b[0m\n",
      "\u001b[34m144\u001b[0m\n",
      "\u001b[34m148\u001b[0m\n",
      "\u001b[34m152\u001b[0m\n",
      "\u001b[34m156\u001b[0m\n",
      "\u001b[34m160\u001b[0m\n",
      "\u001b[34m164\u001b[0m\n",
      "\u001b[34m168\u001b[0m\n",
      "\u001b[34m172\u001b[0m\n",
      "\u001b[34m176\u001b[0m\n",
      "\u001b[34m180\u001b[0m\n",
      "\u001b[34m184\u001b[0m\n",
      "\u001b[34m188\u001b[0m\n",
      "\u001b[34m192\u001b[0m\n",
      "\u001b[34m196\u001b[0m\n",
      "\u001b[34m200\u001b[0m\n",
      "\n",
      "2021-05-15 08:21:57 Uploading - Uploading generated training model\n",
      "2021-05-15 08:21:57 Completed - Training job completed\n",
      "\u001b[34m204\u001b[0m\n",
      "\u001b[34m208\u001b[0m\n",
      "\u001b[34m212\u001b[0m\n",
      "\u001b[34m216\u001b[0m\n",
      "\u001b[34m220\u001b[0m\n",
      "\u001b[34m224\u001b[0m\n",
      "\u001b[34m228\u001b[0m\n",
      "\u001b[34m232\u001b[0m\n",
      "\u001b[34m236\u001b[0m\n",
      "\u001b[34m240\u001b[0m\n",
      "\u001b[34m244\u001b[0m\n",
      "\u001b[34m248\u001b[0m\n",
      "\u001b[34m252\u001b[0m\n",
      "\u001b[34m256\u001b[0m\n",
      "\u001b[34m260\u001b[0m\n",
      "\u001b[34m264\u001b[0m\n",
      "\u001b[34m268\u001b[0m\n",
      "\u001b[34m272\u001b[0m\n",
      "\u001b[34m276\u001b[0m\n",
      "\u001b[34m280\u001b[0m\n",
      "\u001b[34mTotal Detection Time: 17.000000 Seconds\n",
      "\n",
      " seen 64, trained: 0 K-images (0 Kilo-batches_64) \n",
      " Detection layer: 133 - type = 28 \n",
      " Detection layer: 141 - type = 28 \n",
      " Detection layer: 149 - type = 28 \u001b[0m\n",
      "\u001b[34mCompleted 256.0 KiB/4.6 MiB (3.7 MiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mCompleted 512.0 KiB/4.6 MiB (6.8 MiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mCompleted 768.0 KiB/4.6 MiB (9.7 MiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mCompleted 1.0 MiB/4.6 MiB (12.3 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 1.2 MiB/4.6 MiB (14.8 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 1.5 MiB/4.6 MiB (17.0 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 1.8 MiB/4.6 MiB (19.3 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.0 MiB/4.6 MiB (21.3 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.2 MiB/4.6 MiB (23.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.5 MiB/4.6 MiB (25.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.8 MiB/4.6 MiB (27.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.0 MiB/4.6 MiB (29.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.2 MiB/4.6 MiB (31.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.5 MiB/4.6 MiB (33.2 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.8 MiB/4.6 MiB (35.1 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.0 MiB/4.6 MiB (37.0 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.2 MiB/4.6 MiB (38.9 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.5 MiB/4.6 MiB (40.7 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.6 MiB/4.6 MiB (26.0 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mupload: backup/yolo-mini-tiger_final.weights to s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/yolo-mini-tiger.weights\u001b[0m\n",
      "\u001b[34mCompleted 24.8 KiB/24.8 KiB (238.2 KiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mupload: ./train_output.txt to s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/train_output.txt\u001b[0m\n",
      "\u001b[34mCompleted 47.9 KiB/47.9 KiB (399.0 KiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mupload: ./map_output.txt to s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/map_output.txt\u001b[0m\n",
      "\u001b[34mCompleted 11.3 KiB/11.3 KiB (99.3 KiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mupload: ./valid_output.txt to s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/valid_output.txt\u001b[0m\n",
      "\u001b[34m++ date +%m-%d-%y-%H-%M-%S\u001b[0m\n",
      "\u001b[34m+ S3_BASE=s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05\u001b[0m\n",
      "\u001b[34m+ DN_BASE=darknet/data/tiger/VOCdevkit/VOC2007\u001b[0m\n",
      "\u001b[34m+ git clone https://github.com/AlexeyAB/darknet\u001b[0m\n",
      "\u001b[34mCloning into 'darknet'...\u001b[0m\n",
      "\u001b[34m+ rm darknet/Makefile\u001b[0m\n",
      "\u001b[34m+ cp -r darknet_files/Makefile darknet_files/cfg darknet_files/data darknet/\u001b[0m\n",
      "\u001b[34m+ mkdir -p darknet/data/tiger/VOCdevkit/VOC2007\u001b[0m\n",
      "\u001b[34m+ ln -s /opt/ml/input/data/annot/Annotations darknet/data/tiger/VOCdevkit/VOC2007/Annotations\u001b[0m\n",
      "\u001b[34m+ ln -s /opt/ml/input/data/annot/ImageSets darknet/data/tiger/VOCdevkit/VOC2007/ImageSets\u001b[0m\n",
      "\u001b[34m+ ln -s /opt/ml/input/data/train darknet/data/tiger/VOCdevkit/VOC2007/JPEGImages\u001b[0m\n",
      "\u001b[34m+ cd darknet\u001b[0m\n",
      "\u001b[34m+ make -j4\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34m+ cd data/tiger\u001b[0m\n",
      "\u001b[34m+ python voc_label.py\u001b[0m\n",
      "\u001b[34m+ cd ../..\u001b[0m\n",
      "\u001b[34m+ ./darknet detector train cfg/tiger.data cfg/yolo-mini-tiger.cfg -dont_show\u001b[0m\n",
      "\u001b[34m+ tee train_output.txt\u001b[0m\n",
      "\u001b[34m+ ./darknet detector map cfg/tiger.data cfg/yolo-mini-tiger.cfg backup/yolo-mini-tiger_final.weights\u001b[0m\n",
      "\u001b[34m+ tee map_output.txt\u001b[0m\n",
      "\u001b[34m+ ./darknet detector valid cfg/tiger.data cfg/yolo-mini-tiger.cfg backup/yolo-mini-tiger_final.weights -out ''\u001b[0m\n",
      "\u001b[34m+ tee valid_output.txt\u001b[0m\n",
      "\u001b[34m+ aws s3 cp backup/yolo-mini-tiger_final.weights s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/yolo-mini-tiger.weights\u001b[0m\n",
      "\u001b[34m+ aws s3 cp train_output.txt s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/\u001b[0m\n",
      "\u001b[34m+ aws s3 cp map_output.txt s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/\u001b[0m\n",
      "\u001b[34m+ aws s3 cp valid_output.txt s3://calvinandpogs-ee148/atrw/detection/out/05-15-21-08-20-05/\n",
      "\u001b[0m\n",
      "\u001b[34m2021-05-15 08:21:51,493 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 335\n",
      "Billable seconds: 335\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'POSE': f's3://{bucket}/atrw/pose/'})\n",
    "#estimator.fit({'annot': f's3://{bucket}/atrw/pose/annotations/',\n",
    "#               'train': f's3://{bucket}/atrw/pose/images/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06214a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The function delete_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b1319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd03c1f9f65caee21a70539b01e93d04a36b5cac4b9fe4edd9f04915068aa35de43",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}