{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a51f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'calvinandpogs-ee148'\n",
    "prefix = 'atrw/detection/out/'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25e6823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calvinandpogs-ee148 arn:aws:iam::652516965730:role/service-role/AmazonSageMaker-ExecutionRole-20210513T011299\n",
      "/home/ec2-user/SageMaker/atrw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(bucket, role)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3d06e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='YOLO-mini-tiger/main.py',\n",
    "                    source_dir='./',\n",
    "                    framework_version='1.8.0',\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.g4dn.xlarge',\n",
    "                    hyperparameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca539597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://calvinandpogs-ee148/atrw/detection/\n"
     ]
    }
   ],
   "source": [
    "print(f's3://{bucket}/atrw/detection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5e91ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-15 04:54:00 Starting - Starting the training job...\n",
      "2021-05-15 04:54:23 Starting - Launching requested ML instancesProfilerReport-1621054439: InProgress\n",
      "......\n",
      "2021-05-15 04:55:23 Starting - Preparing the instances for training......\n",
      "2021-05-15 04:56:31 Downloading - Downloading input data......\n",
      "2021-05-15 04:57:29 Training - Downloading the training image...............\n",
      "2021-05-15 04:59:49 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-05-15 04:59:49,656 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-05-15 04:59:49,678 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-05-15 04:59:52,699 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-15 04:59:53,197 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"annot\": \"/opt/ml/input/data/annot\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"annot\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-05-15-04-53-59-718\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-652516965730/pytorch-training-2021-05-15-04-53-59-718/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"YOLO-mini-tiger/main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"YOLO-mini-tiger/main.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=YOLO-mini-tiger/main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"annot\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"annot\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=YOLO-mini-tiger/main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-652516965730/pytorch-training-2021-05-15-04-53-59-718/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"annot\":\"/opt/ml/input/data/annot\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"annot\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-05-15-04-53-59-718\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-652516965730/pytorch-training-2021-05-15-04-53-59-718/source/sourcedir.tar.gz\",\"module_name\":\"YOLO-mini-tiger/main\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"YOLO-mini-tiger/main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ANNOT=/opt/ml/input/data/annot\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 YOLO-mini-tiger/main.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/ml/code/YOLO-mini-tiger/darknet/data/tiger\u001b[0m\n",
      "\u001b[34m----------------------------LISTING----------------------------\u001b[0m\n",
      "\u001b[34mAnnotations\u001b[0m\n",
      "\u001b[34mImageSets\u001b[0m\n",
      "\u001b[34mJPEGImages\u001b[0m\n",
      "\u001b[34mlabels\u001b[0m\n",
      "\u001b[34m0000.xml\u001b[0m\n",
      "\u001b[34m0002.xml\u001b[0m\n",
      "\u001b[34m0003.xml\u001b[0m\n",
      "\u001b[34m0004.xml\u001b[0m\n",
      "\u001b[34m0005.xml\u001b[0m\n",
      "\u001b[34m0006.xml\u001b[0m\n",
      "\u001b[34m0008.xml\u001b[0m\n",
      "\u001b[34m0009.xml\u001b[0m\n",
      "\u001b[34m0010.xml\u001b[0m\n",
      "\u001b[34m0011.xml\u001b[0m\n",
      "\u001b[34m---------------------------DONE LISTING---------------------------\u001b[0m\n",
      "\u001b[34myolo-mini-tiger\u001b[0m\n",
      "\u001b[34mnet.optimized_memory = 0 \u001b[0m\n",
      "\u001b[34mmini_batch = 8, batch = 64, time_steps = 1, train = 1 \u001b[0m\n",
      "\u001b[34mCreate CUDA-stream - 0 \u001b[0m\n",
      "\u001b[34mLearning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\n",
      " Detection layer: 133 - type = 28 \n",
      " Detection layer: 141 - type = 28 \n",
      " Detection layer: 149 - type = 28 \u001b[0m\n",
      "\u001b[34mLoaded: 2.702721 seconds\n",
      "\n",
      " 1: 238.355209, 238.355209 avg loss, 0.000000 rate, 11.865760 seconds, 64 images, -1.000000 hours left\u001b[0m\n",
      "\u001b[34mLoaded: 0.000044 seconds\n",
      "\n",
      " 2: 236.918396, 238.211533 avg loss, 0.000000 rate, 10.611637 seconds, 128 images, 0.008094 hours left\u001b[0m\n",
      "\u001b[34mLoaded: 0.000060 seconds\n",
      "\n",
      " 3: 238.632233, 238.253601 avg loss, 0.000000 rate, 10.706528 seconds, 192 images, 0.008042 hours left\u001b[0m\n",
      "\u001b[34mIf you want to train from the beginning, then use flag in the end of training command: -clear \u001b[0m\n",
      "\u001b[34mnet.optimized_memory = 0 \u001b[0m\n",
      "\u001b[34mmini_batch = 1, batch = 8, time_steps = 1, train = 0 \u001b[0m\n",
      "\u001b[34mCreate CUDA-stream - 0 \u001b[0m\n",
      "\u001b[34m seen 64, trained: 0 K-images (0 Kilo-batches_64) \n",
      "\n",
      " calculation mAP (mean average precision)...\n",
      " Detection layer: 133 - type = 28 \n",
      " Detection layer: 141 - type = 28 \n",
      " Detection layer: 149 - type = 28 \n",
      "\n",
      " detections_count = 99353, unique_truth_count = 544  \n",
      " rank = 0 of ranks = 99353 \n",
      " rank = 100 of ranks = 99353 \n",
      " rank = 200 of ranks = 99353 \n",
      " rank = 300 of ranks = 99353 \n",
      " rank = 400 of ranks = 99353 \n",
      " rank = 500 of ranks = 99353 \n",
      " rank = 600 of ranks = 99353 \n",
      " rank = 700 of ranks = 99353 \n",
      " rank = 800 of ranks = 99353 \n",
      " rank = 900 of ranks = 99353 \n",
      " rank = 1000 of ranks = 99353 \n",
      " rank = 1100 of ranks = 99353 \n",
      " rank = 1200 of ranks = 99353 \n",
      " rank = 1300 of ranks = 99353 \n",
      " rank = 1400 of ranks = 99353 \n",
      " rank = 1500 of ranks = 99353 \n",
      " rank = 1600 of ranks = 99353 \n",
      " rank = 1700 of ranks = 99353 \n",
      " rank = 1800 of ranks = 99353 \n",
      " rank = 1900 of ranks = 99353 \n",
      " rank = 2000 of ranks = 99353 \n",
      " rank = 2100 of ranks = 99353 \n",
      " rank = 2200 of ranks = 99353 \n",
      " rank = 2300 of ranks = 99353 \n",
      " rank = 2400 of ranks = 99353 \n",
      " rank = 2500 of ranks = 99353 \n",
      " rank = 2600 of ranks = 99353 \n",
      " rank = 2700 of ranks = 99353 \n",
      " rank = 2800 of ranks = 99353 \n",
      " rank = 2900 of ranks = 99353 \n",
      " rank = 3000 of ranks = 99353 \n",
      " rank = 3100 of ranks = 99353 \n",
      " rank = 3200 of ranks = 99353 \n",
      " rank = 3300 of ranks = 99353 \n",
      " rank = 3400 of ranks = 99353 \n",
      " rank = 3500 of ranks = 99353 \n",
      " rank = 3600 of ranks = 99353 \n",
      " rank = 3700 of ranks = 99353 \n",
      " rank = 3800 of ranks = 99353 \n",
      " rank = 3900 of ranks = 99353 \n",
      " rank = 4000 of ranks = 99353 \n",
      " rank = 4100 of ranks = 99353 \n",
      " rank = 4200 of ranks = 99353 \n",
      " rank = 4300 of ranks = 99353 \n",
      " rank = 4400 of ranks = 99353 \n",
      " rank = 4500 of ranks = 99353 \n",
      " rank = 4600 of ranks = 99353 \n",
      " rank = 4700 of ranks = 99353 \n",
      " rank = 4800 of ranks = 99353 \n",
      " rank = 4900 of ranks = 99353 \n",
      " rank = 5000 of ranks = 99353 \n",
      " rank = 5100 of ranks = 99353 \n",
      " rank = 5200 of ranks = 99353 \n",
      " rank = 5300 of ranks = 99353 \n",
      " rank = 5400 of ranks = 99353 \n",
      " rank = 5500 of ranks = 99353 \n",
      " rank = 5600 of ranks = 99353 \n",
      " rank = 5700 of ranks = 99353 \n",
      " rank = 5800 of ranks = 99353 \n",
      " rank = 5900 of ranks = 99353 \n",
      " rank = 6000 of ranks = 99353 \n",
      " rank = 6100 of ranks = 99353 \n",
      " rank = 6200 of ranks = 99353 \n",
      " rank = 6300 of ranks = 99353 \n",
      " rank = 6400 of ranks = 99353 \n",
      " rank = 6500 of ranks = 99353 \n",
      " rank = 6600 of ranks = 99353 \n",
      " rank = 6700 of ranks = 99353 \n",
      " rank = 6800 of ranks = 99353 \n",
      " rank = 6900 of ranks = 99353 \n",
      " rank = 7000 of ranks = 99353 \n",
      " rank = 7100 of ranks = 99353 \n",
      " rank = 7200 of ranks = 99353 \n",
      " rank = 7300 of ranks = 99353 \n",
      " rank = 7400 of ranks = 99353 \n",
      " rank = 7500 of ranks = 99353 \n",
      " rank = 7600 of ranks = 99353 \n",
      " rank = 7700 of ranks = 99353 \n",
      " rank = 7800 of ranks = 99353 \n",
      " rank = 7900 of ranks = 99353 \n",
      " rank = 8000 of ranks = 99353 \n",
      " rank = 8100 of ranks = 99353 \n",
      " rank = 8200 of ranks = 99353 \n",
      " rank = 8300 of ranks = 99353 \n",
      " rank = 8400 of ranks = 99353 \n",
      " rank = 8500 of ranks = 99353 \n",
      " rank = 8600 of ranks = 99353 \n",
      " rank = 8700 of ranks = 99353 \n",
      " rank = 8800 of ranks = 99353 \n",
      " rank = 8900 of ranks = 99353 \n",
      " rank = 9000 of ranks = 99353 \n",
      " rank = 9100 of ranks = 99353 \n",
      " rank = 9200 of ranks = 99353 \n",
      " rank = 9300 of ranks = 99353 \n",
      " rank = 9400 of ranks = 99353 \n",
      " rank = 9500 of ranks = 99353 \n",
      " rank = 9600 of ranks = 99353 \n",
      " rank = 9700 of ranks = 99353 \n",
      " rank = 9800 of ranks = 99353 \n",
      " rank = 9900 of ranks = 99353 \n",
      " rank = 10000 of ranks = 99353 \n",
      " rank = 10100 of ranks = 99353 \n",
      " rank = 10200 of ranks = 99353 \n",
      " rank = 10300 of ranks = 99353 \n",
      " rank = 10400 of ranks = 99353 \n",
      " rank = 10500 of ranks = 99353 \n",
      " rank = 10600 of ranks = 99353 \n",
      " rank = 10700 of ranks = 99353 \n",
      " rank = 10800 of ranks = 99353 \n",
      " rank = 10900 of ranks = 99353 \n",
      " rank = 11000 of ranks = 99353 \n",
      " rank = 11100 of ranks = 99353 \n",
      " rank = 11200 of ranks = 99353 \n",
      " rank = 11300 of ranks = 99353 \n",
      " rank = 11400 of ranks = 99353 \n",
      " rank = 11500 of ranks = 99353 \n",
      " rank = 11600 of ranks = 99353 \n",
      " rank = 11700 of ranks = 99353 \n",
      " rank = 11800 of ranks = 99353 \n",
      " rank = 11900 of ranks = 99353 \n",
      " rank = 12000 of ranks = 99353 \n",
      " rank = 12100 of ranks = 99353 \n",
      " rank = 12200 of ranks = 99353 \n",
      " rank = 12300 of ranks = 99353 \n",
      " rank = 12400 of ranks = 99353 \n",
      " rank = 12500 of ranks = 99353 \n",
      " rank = 12600 of ranks = 99353 \n",
      " rank = 12700 of ranks = 99353 \n",
      " rank = 12800 of ranks = 99353 \n",
      " rank = 12900 of ranks = 99353 \n",
      " rank = 13000 of ranks = 99353 \n",
      " rank = 13100 of ranks = 99353 \n",
      " rank = 13200 of ranks = 99353 \n",
      " rank = 13300 of ranks = 99353 \n",
      " rank = 13400 of ranks = 99353 \n",
      " rank = 13500 of ranks = 99353 \n",
      " rank = 13600 of ranks = 99353 \n",
      " rank = 13700 of ranks = 99353 \n",
      " rank = 13800 of ranks = 99353 \n",
      " rank = 13900 of ranks = 99353 \n",
      " rank = 14000 of ranks = 99353 \n",
      " rank = 14100 of ranks = 99353 \n",
      " rank = 14200 of ranks = 99353 \n",
      " rank = 14300 of ranks = 99353 \n",
      " rank = 14400 of ranks = 99353 \n",
      " rank = 14500 of ranks = 99353 \n",
      " rank = 14600 of ranks = 99353 \n",
      " rank = 14700 of ranks = 99353 \n",
      " rank = 14800 of ranks = 99353 \n",
      " rank = 14900 of ranks = 99353 \n",
      " rank = 15000 of ranks = 99353 \n",
      " rank = 15100 of ranks = 99353 \n",
      " rank = 15200 of ranks = 99353 \n",
      " rank = 15300 of ranks = 99353 \n",
      " rank = 15400 of ranks = 99353 \n",
      " rank = 15500 of ranks = 99353 \n",
      " rank = 15600 of ranks = 99353 \n",
      " rank = 15700 of ranks = 99353 \n",
      " rank = 15800 of ranks = 99353 \n",
      " rank = 15900 of ranks = 99353 \n",
      " rank = 16000 of ranks = 99353 \n",
      " rank = 16100 of ranks = 99353 \n",
      " rank = 16200 of ranks = 99353 \n",
      " rank = 16300 of ranks = 99353 \n",
      " rank = 16400 of ranks = 99353 \n",
      " rank = 16500 of ranks = 99353 \n",
      " rank = 16600 of ranks = 99353 \n",
      " rank = 16700 of ranks = 99353 \n",
      " rank = 16800 of ranks = 99353 \n",
      " rank = 16900 of ranks = 99353 \n",
      " rank = 17000 of ranks = 99353 \n",
      " rank = 17100 of ranks = 99353 \n",
      " rank = 17200 of ranks = 99353 \n",
      " rank = 17300 of ranks = 99353 \n",
      " rank = 17400 of ranks = 99353 \n",
      " rank = 17500 of ranks = 99353 \n",
      " rank = 17600 of ranks = 99353 \n",
      " rank = 17700 of ranks = 99353 \n",
      " rank = 17800 of ranks = 99353 \n",
      " rank = 17900 of ranks = 99353 \n",
      " rank = 18000 of ranks = 99353 \n",
      " rank = 18100 of ranks = 99353 \n",
      " rank = 18200 of ranks = 99353 \n",
      " rank = 18300 of ranks = 99353 \n",
      " rank = 18400 of ranks = 99353 \n",
      " rank = 18500 of ranks = 99353 \n",
      " rank = 18600 of ranks = 99353 \n",
      " rank = 18700 of ranks = 99353 \n",
      " rank = 18800 of ranks = 99353 \n",
      " rank = 18900 of ranks = 99353 \n",
      " rank = 19000 of ranks = 99353 \n",
      " rank = 19100 of ranks = 99353 \n",
      " rank = 19200 of ranks = 99353 \n",
      " rank = 19300 of ranks = 99353 \n",
      " rank = 19400 of ranks = 99353 \n",
      " rank = 19500 of ranks = 99353 \n",
      " rank = 19600 of ranks = 99353 \n",
      " rank = 19700 of ranks = 99353 \n",
      " rank = 19800 of ranks = 99353 \n",
      " rank = 19900 of ranks = 99353 \n",
      " rank = 20000 of ranks = 99353 \n",
      " rank = 20100 of ranks = 99353 \n",
      " rank = 20200 of ranks = 99353 \n",
      " rank = 20300 of ranks = 99353 \n",
      " rank = 20400 of ranks = 99353 \n",
      " rank = 20500 of ranks = 99353 \n",
      " rank = 20600 of ranks = 99353 \n",
      " rank = 20700 of ranks = 99353 \n",
      " rank = 20800 of ranks = 99353 \n",
      " rank = 20900 of ranks = 99353 \n",
      " rank = 21000 of ranks = 99353 \n",
      " rank = 21100 of ranks = 99353 \n",
      " rank = 21200 of ranks = 99353 \n",
      " rank = 21300 of ranks = 99353 \n",
      " rank = 21400 of ranks = 99353 \n",
      " rank = 21500 of ranks = 99353 \n",
      " rank = 21600 of ranks = 99353 \n",
      " rank = 21700 of ranks = 99353 \n",
      " rank = 21800 of ranks = 99353 \n",
      " rank = 21900 of ranks = 99353 \n",
      " rank = 22000 of ranks = 99353 \n",
      " rank = 22100 of ranks = 99353 \n",
      " rank = 22200 of ranks = 99353 \n",
      " rank = 22300 of ranks = 99353 \n",
      " rank = 22400 of ranks = 99353 \n",
      " rank = 22500 of ranks = 99353 \n",
      " rank = 22600 of ranks = 99353 \n",
      " rank = 22700 of ranks = 99353 \n",
      " rank = 22800 of ranks = 99353 \n",
      " rank = 22900 of ranks = 99353 \n",
      " rank = 23000 of ranks = 99353 \n",
      " rank = 23100 of ranks = 99353 \n",
      " rank = 23200 of ranks = 99353 \n",
      " rank = 23300 of ranks = 99353 \n",
      " rank = 23400 of ranks = 99353 \n",
      " rank = 23500 of ranks = 99353 \n",
      " rank = 23600 of ranks = 99353 \n",
      " rank = 23700 of ranks = 99353 \n",
      " rank = 23800 of ranks = 99353 \n",
      " rank = 23900 of ranks = 99353 \n",
      " rank = 24000 of ranks = 99353 \n",
      " rank = 24100 of ranks = 99353 \n",
      " rank = 24200 of ranks = 99353 \n",
      " rank = 24300 of ranks = 99353 \n",
      " rank = 24400 of ranks = 99353 \n",
      " rank = 24500 of ranks = 99353 \n",
      " rank = 24600 of ranks = 99353 \n",
      " rank = 24700 of ranks = 99353 \n",
      " rank = 24800 of ranks = 99353 \n",
      " rank = 24900 of ranks = 99353 \n",
      " rank = 25000 of ranks = 99353 \n",
      " rank = 25100 of ranks = 99353 \n",
      " rank = 25200 of ranks = 99353 \n",
      " rank = 25300 of ranks = 99353 \n",
      " rank = 25400 of ranks = 99353 \n",
      " rank = 25500 of ranks = 99353 \n",
      " rank = 25600 of ranks = 99353 \n",
      " rank = 25700 of ranks = 99353 \n",
      " rank = 25800 of ranks = 99353 \n",
      " rank = 25900 of ranks = 99353 \n",
      " rank = 26000 of ranks = 99353 \n",
      " rank = 26100 of ranks = 99353 \n",
      " rank = 26200 of ranks = 99353 \n",
      " rank = 26300 of ranks = 99353 \n",
      " rank = 26400 of ranks = 99353 \n",
      " rank = 26500 of ranks = 99353 \n",
      " rank = 26600 of ranks = 99353 \n",
      " rank = 26700 of ranks = 99353 \n",
      " rank = 26800 of ranks = 99353 \n",
      " rank = 26900 of ranks = 99353 \n",
      " rank = 27000 of ranks = 99353 \n",
      " rank = 27100 of ranks = 99353 \n",
      " rank = 27200 of ranks = 99353 \n",
      " rank = 27300 of ranks = 99353 \n",
      " rank = 27400 of ranks = 99353 \n",
      " rank = 27500 of ranks = 99353 \n",
      " rank = 27600 of ranks = 99353 \n",
      " rank = 27700 of ranks = 99353 \n",
      " rank = 27800 of ranks = 99353 \n",
      " rank = 27900 of ranks = 99353 \n",
      " rank = 28000 of ranks = 99353 \n",
      " rank = 28100 of ranks = 99353 \n",
      " rank = 28200 of ranks = 99353 \n",
      " rank = 28300 of ranks = 99353 \n",
      " rank = 28400 of ranks = 99353 \n",
      " rank = 28500 of ranks = 99353 \n",
      " rank = 28600 of ranks = 99353 \n",
      " rank = 28700 of ranks = 99353 \n",
      " rank = 28800 of ranks = 99353 \n",
      " rank = 28900 of ranks = 99353 \n",
      " rank = 29000 of ranks = 99353 \n",
      " rank = 29100 of ranks = 99353 \n",
      " rank = 29200 of ranks = 99353 \n",
      " rank = 29300 of ranks = 99353 \n",
      " rank = 29400 of ranks = 99353 \n",
      " rank = 29500 of ranks = 99353 \n",
      " rank = 29600 of ranks = 99353 \n",
      " rank = 29700 of ranks = 99353 \n",
      " rank = 29800 of ranks = 99353 \n",
      " rank = 29900 of ranks = 99353 \n",
      " rank = 30000 of ranks = 99353 \n",
      " rank = 30100 of ranks = 99353 \n",
      " rank = 30200 of ranks = 99353 \n",
      " rank = 30300 of ranks = 99353 \n",
      " rank = 30400 of ranks = 99353 \n",
      " rank = 30500 of ranks = 99353 \n",
      " rank = 30600 of ranks = 99353 \n",
      " rank = 30700 of ranks = 99353 \n",
      " rank = 30800 of ranks = 99353 \n",
      " rank = 30900 of ranks = 99353 \n",
      " rank = 31000 of ranks = 99353 \n",
      " rank = 31100 of ranks = 99353 \n",
      " rank = 31200 of ranks = 99353 \n",
      " rank = 31300 of ranks = 99353 \n",
      " rank = 31400 of ranks = 99353 \n",
      " rank = 31500 of ranks = 99353 \n",
      " rank = 31600 of ranks = 99353 \n",
      " rank = 31700 of ranks = 99353 \n",
      " rank = 31800 of ranks = 99353 \n",
      " rank = 31900 of ranks = 99353 \n",
      " rank = 32000 of ranks = 99353 \n",
      " rank = 32100 of ranks = 99353 \n",
      " rank = 32200 of ranks = 99353 \n",
      " rank = 32300 of ranks = 99353 \n",
      " rank = 32400 of ranks = 99353 \n",
      " rank = 32500 of ranks = 99353 \n",
      " rank = 32600 of ranks = 99353 \n",
      " rank = 32700 of ranks = 99353 \n",
      " rank = 32800 of ranks = 99353 \n",
      " rank = 32900 of ranks = 99353 \n",
      " rank = 33000 of ranks = 99353 \n",
      " rank = 33100 of ranks = 99353 \n",
      " rank = 33200 of ranks = 99353 \n",
      " rank = 33300 of ranks = 99353 \n",
      " rank = 33400 of ranks = 99353 \n",
      " rank = 33500 of ranks = 99353 \n",
      " rank = 33600 of ranks = 99353 \n",
      " rank = 33700 of ranks = 99353 \n",
      " rank = 33800 of ranks = 99353 \n",
      " rank = 33900 of ranks = 99353 \n",
      " rank = 34000 of ranks = 99353 \n",
      " rank = 34100 of ranks = 99353 \n",
      " rank = 34200 of ranks = 99353 \n",
      " rank = 34300 of ranks = 99353 \n",
      " rank = 34400 of ranks = 99353 \n",
      " rank = 34500 of ranks = 99353 \n",
      " rank = 34600 of ranks = 99353 \n",
      " rank = 34700 of ranks = 99353 \n",
      " rank = 34800 of ranks = 99353 \n",
      " rank = 34900 of ranks = 99353 \n",
      " rank = 35000 of ranks = 99353 \n",
      " rank = 35100 of ranks = 99353 \n",
      " rank = 35200 of ranks = 99353 \n",
      " rank = 35300 of ranks = 99353 \n",
      " rank = 35400 of ranks = 99353 \n",
      " rank = 35500 of ranks = 99353 \n",
      " rank = 35600 of ranks = 99353 \n",
      " rank = 35700 of ranks = 99353 \n",
      " rank = 35800 of ranks = 99353 \n",
      " rank = 35900 of ranks = 99353 \n",
      " rank = 36000 of ranks = 99353 \n",
      " rank = 36100 of ranks = 99353 \n",
      " rank = 36200 of ranks = 99353 \n",
      " rank = 36300 of ranks = 99353 \n",
      " rank = 36400 of ranks = 99353 \n",
      " rank = 36500 of ranks = 99353 \n",
      " rank = 36600 of ranks = 99353 \n",
      " rank = 36700 of ranks = 99353 \n",
      " rank = 36800 of ranks = 99353 \n",
      " rank = 36900 of ranks = 99353 \n",
      " rank = 37000 of ranks = 99353 \n",
      " rank = 37100 of ranks = 99353 \n",
      " rank = 37200 of ranks = 99353 \n",
      " rank = 37300 of ranks = 99353 \n",
      " rank = 37400 of ranks = 99353 \n",
      " rank = 37500 of ranks = 99353 \n",
      " rank = 37600 of ranks = 99353 \n",
      " rank = 37700 of ranks = 99353 \n",
      " rank = 37800 of ranks = 99353 \n",
      " rank = 37900 of ranks = 99353 \n",
      " rank = 38000 of ranks = 99353 \n",
      " rank = 38100 of ranks = 99353 \n",
      " rank = 38200 of ranks = 99353 \n",
      " rank = 38300 of ranks = 99353 \n",
      " rank = 38400 of ranks = 99353 \n",
      " rank = 38500 of ranks = 99353 \n",
      " rank = 38600 of ranks = 99353 \n",
      " rank = 38700 of ranks = 99353 \n",
      " rank = 38800 of ranks = 99353 \n",
      " rank = 38900 of ranks = 99353 \n",
      " rank = 39000 of ranks = 99353 \n",
      " rank = 39100 of ranks = 99353 \n",
      " rank = 39200 of ranks = 99353 \n",
      " rank = 39300 of ranks = 99353 \n",
      " rank = 39400 of ranks = 99353 \n",
      " rank = 39500 of ranks = 99353 \n",
      " rank = 39600 of ranks = 99353 \n",
      " rank = 39700 of ranks = 99353 \n",
      " rank = 39800 of ranks = 99353 \n",
      " rank = 39900 of ranks = 99353 \n",
      " rank = 40000 of ranks = 99353 \n",
      " rank = 40100 of ranks = 99353 \n",
      " rank = 40200 of ranks = 99353 \n",
      " rank = 40300 of ranks = 99353 \n",
      " rank = 40400 of ranks = 99353 \n",
      " rank = 40500 of ranks = 99353 \n",
      " rank = 40600 of ranks = 99353 \n",
      " rank = 40700 of ranks = 99353 \n",
      " rank = 40800 of ranks = 99353 \n",
      " rank = 40900 of ranks = 99353 \n",
      " rank = 41000 of ranks = 99353 \n",
      " rank = 41100 of ranks = 99353 \n",
      " rank = 41200 of ranks = 99353 \n",
      " rank = 41300 of ranks = 99353 \n",
      " rank = 41400 of ranks = 99353 \n",
      " rank = 41500 of ranks = 99353 \n",
      " rank = 41600 of ranks = 99353 \n",
      " rank = 41700 of ranks = 99353 \n",
      " rank = 41800 of ranks = 99353 \n",
      " rank = 41900 of ranks = 99353 \n",
      " rank = 42000 of ranks = 99353 \n",
      " rank = 42100 of ranks = 99353 \n",
      " rank = 42200 of ranks = 99353 \n",
      " rank = 42300 of ranks = 99353 \n",
      " rank = 42400 of ranks = 99353 \n",
      " rank = 42500 of ranks = 99353 \n",
      " rank = 42600 of ranks = 99353 \n",
      " rank = 42700 of ranks = 99353 \n",
      " rank = 42800 of ranks = 99353 \n",
      " rank = 42900 of ranks = 99353 \n",
      " rank = 43000 of ranks = 99353 \n",
      " rank = 43100 of ranks = 99353 \n",
      " rank = 43200 of ranks = 99353 \n",
      " rank = 43300 of ranks = 99353 \n",
      " rank = 43400 of ranks = 99353 \n",
      " rank = 43500 of ranks = 99353 \n",
      " rank = 43600 of ranks = 99353 \n",
      " rank = 43700 of ranks = 99353 \n",
      " rank = 43800 of ranks = 99353 \n",
      " rank = 43900 of ranks = 99353 \n",
      " rank = 44000 of ranks = 99353 \n",
      " rank = 44100 of ranks = 99353 \n",
      " rank = 44200 of ranks = 99353 \n",
      " rank = 44300 of ranks = 99353 \n",
      " rank = 44400 of ranks = 99353 \n",
      " rank = 44500 of ranks = 99353 \n",
      " rank = 44600 of ranks = 99353 \n",
      " rank = 44700 of ranks = 99353 \n",
      " rank = 44800 of ranks = 99353 \n",
      " rank = 44900 of ranks = 99353 \n",
      " rank = 45000 of ranks = 99353 \n",
      " rank = 45100 of ranks = 99353 \n",
      " rank = 45200 of ranks = 99353 \n",
      " rank = 45300 of ranks = 99353 \n",
      " rank = 45400 of ranks = 99353 \n",
      " rank = 45500 of ranks = 99353 \n",
      " rank = 45600 of ranks = 99353 \n",
      " rank = 45700 of ranks = 99353 \n",
      " rank = 45800 of ranks = 99353 \n",
      " rank = 45900 of ranks = 99353 \n",
      " rank = 46000 of ranks = 99353 \n",
      " rank = 46100 of ranks = 99353 \n",
      " rank = 46200 of ranks = 99353 \n",
      " rank = 46300 of ranks = 99353 \n",
      " rank = 46400 of ranks = 99353 \n",
      " rank = 46500 of ranks = 99353 \n",
      " rank = 46600 of ranks = 99353 \n",
      " rank = 46700 of ranks = 99353 \n",
      " rank = 46800 of ranks = 99353 \n",
      " rank = 46900 of ranks = 99353 \n",
      " rank = 47000 of ranks = 99353 \n",
      " rank = 47100 of ranks = 99353 \n",
      " rank = 47200 of ranks = 99353 \n",
      " rank = 47300 of ranks = 99353 \n",
      " rank = 47400 of ranks = 99353 \n",
      " rank = 47500 of ranks = 99353 \n",
      " rank = 47600 of ranks = 99353 \n",
      " rank = 47700 of ranks = 99353 \n",
      " rank = 47800 of ranks = 99353 \n",
      " rank = 47900 of ranks = 99353 \n",
      " rank = 48000 of ranks = 99353 \n",
      " rank = 48100 of ranks = 99353 \n",
      " rank = 48200 of ranks = 99353 \n",
      " rank = 48300 of ranks = 99353 \n",
      " rank = 48400 of ranks = 99353 \n",
      " rank = 48500 of ranks = 99353 \n",
      " rank = 48600 of ranks = 99353 \n",
      " rank = 48700 of ranks = 99353 \n",
      " rank = 48800 of ranks = 99353 \n",
      " rank = 48900 of ranks = 99353 \n",
      " rank = 49000 of ranks = 99353 \n",
      " rank = 49100 of ranks = 99353 \n",
      " rank = 49200 of ranks = 99353 \n",
      " rank = 49300 of ranks = 99353 \n",
      " rank = 49400 of ranks = 99353 \n",
      " rank = 49500 of ranks = 99353 \n",
      " rank = 49600 of ranks = 99353 \n",
      " rank = 49700 of ranks = 99353 \n",
      " rank = 49800 of ranks = 99353 \n",
      " rank = 49900 of ranks = 99353 \n",
      " rank = 50000 of ranks = 99353 \n",
      " rank = 50100 of ranks = 99353 \n",
      " rank = 50200 of ranks = 99353 \n",
      " rank = 50300 of ranks = 99353 \n",
      " rank = 50400 of ranks = 99353 \n",
      " rank = 50500 of ranks = 99353 \n",
      " rank = 50600 of ranks = 99353 \n",
      " rank = 50700 of ranks = 99353 \n",
      " rank = 50800 of ranks = 99353 \n",
      " rank = 50900 of ranks = 99353 \n",
      " rank = 51000 of ranks = 99353 \n",
      " rank = 51100 of ranks = 99353 \n",
      " rank = 51200 of ranks = 99353 \n",
      " rank = 51300 of ranks = 99353 \n",
      " rank = 51400 of ranks = 99353 \n",
      " rank = 51500 of ranks = 99353 \n",
      " rank = 51600 of ranks = 99353 \n",
      " rank = 51700 of ranks = 99353 \n",
      " rank = 51800 of ranks = 99353 \n",
      " rank = 51900 of ranks = 99353 \n",
      " rank = 52000 of ranks = 99353 \n",
      " rank = 52100 of ranks = 99353 \n",
      " rank = 52200 of ranks = 99353 \n",
      " rank = 52300 of ranks = 99353 \n",
      " rank = 52400 of ranks = 99353 \n",
      " rank = 52500 of ranks = 99353 \n",
      " rank = 52600 of ranks = 99353 \n",
      " rank = 52700 of ranks = 99353 \n",
      " rank = 52800 of ranks = 99353 \n",
      " rank = 52900 of ranks = 99353 \n",
      " rank = 53000 of ranks = 99353 \n",
      " rank = 53100 of ranks = 99353 \n",
      " rank = 53200 of ranks = 99353 \n",
      " rank = 53300 of ranks = 99353 \n",
      " rank = 53400 of ranks = 99353 \n",
      " rank = 53500 of ranks = 99353 \n",
      " rank = 53600 of ranks = 99353 \n",
      " rank = 53700 of ranks = 99353 \n",
      " rank = 53800 of ranks = 99353 \n",
      " rank = 53900 of ranks = 99353 \n",
      " rank = 54000 of ranks = 99353 \n",
      " rank = 54100 of ranks = 99353 \n",
      " rank = 54200 of ranks = 99353 \n",
      " rank = 54300 of ranks = 99353 \n",
      " rank = 54400 of ranks = 99353 \n",
      " rank = 54500 of ranks = 99353 \n",
      " rank = 54600 of ranks = 99353 \n",
      " rank = 54700 of ranks = 99353 \n",
      " rank = 54800 of ranks = 99353 \n",
      " rank = 54900 of ranks = 99353 \n",
      " rank = 55000 of ranks = 99353 \n",
      " rank = 55100 of ranks = 99353 \n",
      " rank = 55200 of ranks = 99353 \n",
      " rank = 55300 of ranks = 99353 \n",
      " rank = 55400 of ranks = 99353 \n",
      " rank = 55500 of ranks = 99353 \n",
      " rank = 55600 of ranks = 99353 \n",
      " rank = 55700 of ranks = 99353 \n",
      " rank = 55800 of ranks = 99353 \n",
      " rank = 55900 of ranks = 99353 \n",
      " rank = 56000 of ranks = 99353 \n",
      " rank = 56100 of ranks = 99353 \n",
      " rank = 56200 of ranks = 99353 \n",
      " rank = 56300 of ranks = 99353 \n",
      " rank = 56400 of ranks = 99353 \n",
      " rank = 56500 of ranks = 99353 \n",
      " rank = 56600 of ranks = 99353 \n",
      " rank = 56700 of ranks = 99353 \n",
      " rank = 56800 of ranks = 99353 \n",
      " rank = 56900 of ranks = 99353 \n",
      " rank = 57000 of ranks = 99353 \n",
      " rank = 57100 of ranks = 99353 \n",
      " rank = 57200 of ranks = 99353 \n",
      " rank = 57300 of ranks = 99353 \n",
      " rank = 57400 of ranks = 99353 \n",
      " rank = 57500 of ranks = 99353 \n",
      " rank = 57600 of ranks = 99353 \n",
      " rank = 57700 of ranks = 99353 \n",
      " rank = 57800 of ranks = 99353 \n",
      " rank = 57900 of ranks = 99353 \n",
      " rank = 58000 of ranks = 99353 \n",
      " rank = 58100 of ranks = 99353 \n",
      " rank = 58200 of ranks = 99353 \n",
      " rank = 58300 of ranks = 99353 \n",
      " rank = 58400 of ranks = 99353 \n",
      " rank = 58500 of ranks = 99353 \n",
      " rank = 58600 of ranks = 99353 \n",
      " rank = 58700 of ranks = 99353 \n",
      " rank = 58800 of ranks = 99353 \n",
      " rank = 58900 of ranks = 99353 \n",
      " rank = 59000 of ranks = 99353 \n",
      " rank = 59100 of ranks = 99353 \n",
      " rank = 59200 of ranks = 99353 \n",
      " rank = 59300 of ranks = 99353 \n",
      " rank = 59400 of ranks = 99353 \n",
      " rank = 59500 of ranks = 99353 \n",
      " rank = 59600 of ranks = 99353 \n",
      " rank = 59700 of ranks = 99353 \n",
      " rank = 59800 of ranks = 99353 \n",
      " rank = 59900 of ranks = 99353 \n",
      " rank = 60000 of ranks = 99353 \n",
      " rank = 60100 of ranks = 99353 \n",
      " rank = 60200 of ranks = 99353 \n",
      " rank = 60300 of ranks = 99353 \n",
      " rank = 60400 of ranks = 99353 \n",
      " rank = 60500 of ranks = 99353 \n",
      " rank = 60600 of ranks = 99353 \n",
      " rank = 60700 of ranks = 99353 \n",
      " rank = 60800 of ranks = 99353 \n",
      " rank = 60900 of ranks = 99353 \n",
      " rank = 61000 of ranks = 99353 \n",
      " rank = 61100 of ranks = 99353 \n",
      " rank = 61200 of ranks = 99353 \n",
      " rank = 61300 of ranks = 99353 \n",
      " rank = 61400 of ranks = 99353 \n",
      " rank = 61500 of ranks = 99353 \n",
      " rank = 61600 of ranks = 99353 \n",
      " rank = 61700 of ranks = 99353 \n",
      " rank = 61800 of ranks = 99353 \n",
      " rank = 61900 of ranks = 99353 \n",
      " rank = 62000 of ranks = 99353 \n",
      " rank = 62100 of ranks = 99353 \n",
      " rank = 62200 of ranks = 99353 \n",
      " rank = 62300 of ranks = 99353 \n",
      " rank = 62400 of ranks = 99353 \n",
      " rank = 62500 of ranks = 99353 \n",
      " rank = 62600 of ranks = 99353 \n",
      " rank = 62700 of ranks = 99353 \n",
      " rank = 62800 of ranks = 99353 \n",
      " rank = 62900 of ranks = 99353 \n",
      " rank = 63000 of ranks = 99353 \n",
      " rank = 63100 of ranks = 99353 \n",
      " rank = 63200 of ranks = 99353 \n",
      " rank = 63300 of ranks = 99353 \n",
      " rank = 63400 of ranks = 99353 \n",
      " rank = 63500 of ranks = 99353 \n",
      " rank = 63600 of ranks = 99353 \n",
      " rank = 63700 of ranks = 99353 \n",
      " rank = 63800 of ranks = 99353 \n",
      " rank = 63900 of ranks = 99353 \n",
      " rank = 64000 of ranks = 99353 \n",
      " rank = 64100 of ranks = 99353 \n",
      " rank = 64200 of ranks = 99353 \n",
      " rank = 64300 of ranks = 99353 \n",
      " rank = 64400 of ranks = 99353 \n",
      " rank = 64500 of ranks = 99353 \n",
      " rank = 64600 of ranks = 99353 \n",
      " rank = 64700 of ranks = 99353 \n",
      " rank = 64800 of ranks = 99353 \n",
      " rank = 64900 of ranks = 99353 \n",
      " rank = 65000 of ranks = 99353 \n",
      " rank = 65100 of ranks = 99353 \n",
      " rank = 65200 of ranks = 99353 \n",
      " rank = 65300 of ranks = 99353 \n",
      " rank = 65400 of ranks = 99353 \n",
      " rank = 65500 of ranks = 99353 \n",
      " rank = 65600 of ranks = 99353 \n",
      " rank = 65700 of ranks = 99353 \n",
      " rank = 65800 of ranks = 99353 \n",
      " rank = 65900 of ranks = 99353 \n",
      " rank = 66000 of ranks = 99353 \n",
      " rank = 66100 of ranks = 99353 \n",
      " rank = 66200 of ranks = 99353 \n",
      " rank = 66300 of ranks = 99353 \n",
      " rank = 66400 of ranks = 99353 \n",
      " rank = 66500 of ranks = 99353 \n",
      " rank = 66600 of ranks = 99353 \n",
      " rank = 66700 of ranks = 99353 \n",
      " rank = 66800 of ranks = 99353 \n",
      " rank = 66900 of ranks = 99353 \n",
      " rank = 67000 of ranks = 99353 \n",
      " rank = 67100 of ranks = 99353 \n",
      " rank = 67200 of ranks = 99353 \n",
      " rank = 67300 of ranks = 99353 \n",
      " rank = 67400 of ranks = 99353 \n",
      " rank = 67500 of ranks = 99353 \n",
      " rank = 67600 of ranks = 99353 \n",
      " rank = 67700 of ranks = 99353 \n",
      " rank = 67800 of ranks = 99353 \n",
      " rank = 67900 of ranks = 99353 \n",
      " rank = 68000 of ranks = 99353 \n",
      " rank = 68100 of ranks = 99353 \n",
      " rank = 68200 of ranks = 99353 \n",
      " rank = 68300 of ranks = 99353 \n",
      " rank = 68400 of ranks = 99353 \n",
      " rank = 68500 of ranks = 99353 \n",
      " rank = 68600 of ranks = 99353 \n",
      " rank = 68700 of ranks = 99353 \n",
      " rank = 68800 of ranks = 99353 \n",
      " rank = 68900 of ranks = 99353 \n",
      " rank = 69000 of ranks = 99353 \n",
      " rank = 69100 of ranks = 99353 \n",
      " rank = 69200 of ranks = 99353 \n",
      " rank = 69300 of ranks = 99353 \n",
      " rank = 69400 of ranks = 99353 \n",
      " rank = 69500 of ranks = 99353 \n",
      " rank = 69600 of ranks = 99353 \n",
      " rank = 69700 of ranks = 99353 \n",
      " rank = 69800 of ranks = 99353 \n",
      " rank = 69900 of ranks = 99353 \n",
      " rank = 70000 of ranks = 99353 \n",
      " rank = 70100 of ranks = 99353 \n",
      " rank = 70200 of ranks = 99353 \n",
      " rank = 70300 of ranks = 99353 \n",
      " rank = 70400 of ranks = 99353 \n",
      " rank = 70500 of ranks = 99353 \n",
      " rank = 70600 of ranks = 99353 \n",
      " rank = 70700 of ranks = 99353 \n",
      " rank = 70800 of ranks = 99353 \n",
      " rank = 70900 of ranks = 99353 \n",
      " rank = 71000 of ranks = 99353 \n",
      " rank = 71100 of ranks = 99353 \n",
      " rank = 71200 of ranks = 99353 \n",
      " rank = 71300 of ranks = 99353 \n",
      " rank = 71400 of ranks = 99353 \n",
      " rank = 71500 of ranks = 99353 \n",
      " rank = 71600 of ranks = 99353 \n",
      " rank = 71700 of ranks = 99353 \n",
      " rank = 71800 of ranks = 99353 \n",
      " rank = 71900 of ranks = 99353 \n",
      " rank = 72000 of ranks = 99353 \n",
      " rank = 72100 of ranks = 99353 \n",
      " rank = 72200 of ranks = 99353 \n",
      " rank = 72300 of ranks = 99353 \n",
      " rank = 72400 of ranks = 99353 \n",
      " rank = 72500 of ranks = 99353 \n",
      " rank = 72600 of ranks = 99353 \n",
      " rank = 72700 of ranks = 99353 \n",
      " rank = 72800 of ranks = 99353 \n",
      " rank = 72900 of ranks = 99353 \n",
      " rank = 73000 of ranks = 99353 \n",
      " rank = 73100 of ranks = 99353 \n",
      " rank = 73200 of ranks = 99353 \n",
      " rank = 73300 of ranks = 99353 \n",
      " rank = 73400 of ranks = 99353 \n",
      " rank = 73500 of ranks = 99353 \n",
      " rank = 73600 of ranks = 99353 \n",
      " rank = 73700 of ranks = 99353 \n",
      " rank = 73800 of ranks = 99353 \n",
      " rank = 73900 of ranks = 99353 \n",
      " rank = 74000 of ranks = 99353 \n",
      " rank = 74100 of ranks = 99353 \n",
      " rank = 74200 of ranks = 99353 \n",
      " rank = 74300 of ranks = 99353 \n",
      " rank = 74400 of ranks = 99353 \n",
      " rank = 74500 of ranks = 99353 \n",
      " rank = 74600 of ranks = 99353 \n",
      " rank = 74700 of ranks = 99353 \n",
      " rank = 74800 of ranks = 99353 \n",
      " rank = 74900 of ranks = 99353 \n",
      " rank = 75000 of ranks = 99353 \n",
      " rank = 75100 of ranks = 99353 \n",
      " rank = 75200 of ranks = 99353 \n",
      " rank = 75300 of ranks = 99353 \n",
      " rank = 75400 of ranks = 99353 \n",
      " rank = 75500 of ranks = 99353 \n",
      " rank = 75600 of ranks = 99353 \n",
      " rank = 75700 of ranks = 99353 \n",
      " rank = 75800 of ranks = 99353 \n",
      " rank = 75900 of ranks = 99353 \n",
      " rank = 76000 of ranks = 99353 \n",
      " rank = 76100 of ranks = 99353 \n",
      " rank = 76200 of ranks = 99353 \n",
      " rank = 76300 of ranks = 99353 \n",
      " rank = 76400 of ranks = 99353 \n",
      " rank = 76500 of ranks = 99353 \n",
      " rank = 76600 of ranks = 99353 \n",
      " rank = 76700 of ranks = 99353 \n",
      " rank = 76800 of ranks = 99353 \n",
      " rank = 76900 of ranks = 99353 \n",
      " rank = 77000 of ranks = 99353 \n",
      " rank = 77100 of ranks = 99353 \n",
      " rank = 77200 of ranks = 99353 \n",
      " rank = 77300 of ranks = 99353 \n",
      " rank = 77400 of ranks = 99353 \n",
      " rank = 77500 of ranks = 99353 \n",
      " rank = 77600 of ranks = 99353 \n",
      " rank = 77700 of ranks = 99353 \n",
      " rank = 77800 of ranks = 99353 \n",
      " rank = 77900 of ranks = 99353 \n",
      " rank = 78000 of ranks = 99353 \n",
      " rank = 78100 of ranks = 99353 \n",
      " rank = 78200 of ranks = 99353 \n",
      " rank = 78300 of ranks = 99353 \n",
      " rank = 78400 of ranks = 99353 \n",
      " rank = 78500 of ranks = 99353 \n",
      " rank = 78600 of ranks = 99353 \n",
      " rank = 78700 of ranks = 99353 \n",
      " rank = 78800 of ranks = 99353 \n",
      " rank = 78900 of ranks = 99353 \n",
      " rank = 79000 of ranks = 99353 \n",
      " rank = 79100 of ranks = 99353 \n",
      " rank = 79200 of ranks = 99353 \n",
      " rank = 79300 of ranks = 99353 \n",
      " rank = 79400 of ranks = 99353 \n",
      " rank = 79500 of ranks = 99353 \n",
      " rank = 79600 of ranks = 99353 \n",
      " rank = 79700 of ranks = 99353 \n",
      " rank = 79800 of ranks = 99353 \n",
      " rank = 79900 of ranks = 99353 \n",
      " rank = 80000 of ranks = 99353 \n",
      " rank = 80100 of ranks = 99353 \n",
      " rank = 80200 of ranks = 99353 \n",
      " rank = 80300 of ranks = 99353 \n",
      " rank = 80400 of ranks = 99353 \n",
      " rank = 80500 of ranks = 99353 \n",
      " rank = 80600 of ranks = 99353 \n",
      " rank = 80700 of ranks = 99353 \n",
      " rank = 80800 of ranks = 99353 \n",
      " rank = 80900 of ranks = 99353 \n",
      " rank = 81000 of ranks = 99353 \n",
      " rank = 81100 of ranks = 99353 \n",
      " rank = 81200 of ranks = 99353 \n",
      " rank = 81300 of ranks = 99353 \n",
      " rank = 81400 of ranks = 99353 \n",
      " rank = 81500 of ranks = 99353 \n",
      " rank = 81600 of ranks = 99353 \n",
      " rank = 81700 of ranks = 99353 \n",
      " rank = 81800 of ranks = 99353 \n",
      " rank = 81900 of ranks = 99353 \n",
      " rank = 82000 of ranks = 99353 \n",
      " rank = 82100 of ranks = 99353 \n",
      " rank = 82200 of ranks = 99353 \n",
      " rank = 82300 of ranks = 99353 \n",
      " rank = 82400 of ranks = 99353 \n",
      " rank = 82500 of ranks = 99353 \n",
      " rank = 82600 of ranks = 99353 \n",
      " rank = 82700 of ranks = 99353 \n",
      " rank = 82800 of ranks = 99353 \n",
      " rank = 82900 of ranks = 99353 \n",
      " rank = 83000 of ranks = 99353 \n",
      " rank = 83100 of ranks = 99353 \n",
      " rank = 83200 of ranks = 99353 \n",
      " rank = 83300 of ranks = 99353 \n",
      " rank = 83400 of ranks = 99353 \n",
      " rank = 83500 of ranks = 99353 \n",
      " rank = 83600 of ranks = 99353 \n",
      " rank = 83700 of ranks = 99353 \n",
      " rank = 83800 of ranks = 99353 \n",
      " rank = 83900 of ranks = 99353 \n",
      " rank = 84000 of ranks = 99353 \n",
      " rank = 84100 of ranks = 99353 \n",
      " rank = 84200 of ranks = 99353 \n",
      " rank = 84300 of ranks = 99353 \n",
      " rank = 84400 of ranks = 99353 \n",
      " rank = 84500 of ranks = 99353 \n",
      " rank = 84600 of ranks = 99353 \n",
      " rank = 84700 of ranks = 99353 \n",
      " rank = 84800 of ranks = 99353 \n",
      " rank = 84900 of ranks = 99353 \n",
      " rank = 85000 of ranks = 99353 \n",
      " rank = 85100 of ranks = 99353 \n",
      " rank = 85200 of ranks = 99353 \n",
      " rank = 85300 of ranks = 99353 \n",
      " rank = 85400 of ranks = 99353 \n",
      " rank = 85500 of ranks = 99353 \n",
      " rank = 85600 of ranks = 99353 \n",
      " rank = 85700 of ranks = 99353 \n",
      " rank = 85800 of ranks = 99353 \n",
      " rank = 85900 of ranks = 99353 \n",
      " rank = 86000 of ranks = 99353 \n",
      " rank = 86100 of ranks = 99353 \n",
      " rank = 86200 of ranks = 99353 \n",
      " rank = 86300 of ranks = 99353 \n",
      " rank = 86400 of ranks = 99353 \n",
      " rank = 86500 of ranks = 99353 \n",
      " rank = 86600 of ranks = 99353 \n",
      " rank = 86700 of ranks = 99353 \n",
      " rank = 86800 of ranks = 99353 \n",
      " rank = 86900 of ranks = 99353 \n",
      " rank = 87000 of ranks = 99353 \n",
      " rank = 87100 of ranks = 99353 \n",
      " rank = 87200 of ranks = 99353 \n",
      " rank = 87300 of ranks = 99353 \n",
      " rank = 87400 of ranks = 99353 \n",
      " rank = 87500 of ranks = 99353 \n",
      " rank = 87600 of ranks = 99353 \n",
      " rank = 87700 of ranks = 99353 \n",
      " rank = 87800 of ranks = 99353 \n",
      " rank = 87900 of ranks = 99353 \n",
      " rank = 88000 of ranks = 99353 \n",
      " rank = 88100 of ranks = 99353 \n",
      " rank = 88200 of ranks = 99353 \n",
      " rank = 88300 of ranks = 99353 \n",
      " rank = 88400 of ranks = 99353 \n",
      " rank = 88500 of ranks = 99353 \n",
      " rank = 88600 of ranks = 99353 \n",
      " rank = 88700 of ranks = 99353 \n",
      " rank = 88800 of ranks = 99353 \n",
      " rank = 88900 of ranks = 99353 \n",
      " rank = 89000 of ranks = 99353 \n",
      " rank = 89100 of ranks = 99353 \n",
      " rank = 89200 of ranks = 99353 \n",
      " rank = 89300 of ranks = 99353 \n",
      " rank = 89400 of ranks = 99353 \n",
      " rank = 89500 of ranks = 99353 \n",
      " rank = 89600 of ranks = 99353 \n",
      " rank = 89700 of ranks = 99353 \n",
      " rank = 89800 of ranks = 99353 \n",
      " rank = 89900 of ranks = 99353 \n",
      " rank = 90000 of ranks = 99353 \n",
      " rank = 90100 of ranks = 99353 \n",
      " rank = 90200 of ranks = 99353 \n",
      " rank = 90300 of ranks = 99353 \n",
      " rank = 90400 of ranks = 99353 \n",
      " rank = 90500 of ranks = 99353 \n",
      " rank = 90600 of ranks = 99353 \n",
      " rank = 90700 of ranks = 99353 \n",
      " rank = 90800 of ranks = 99353 \n",
      " rank = 90900 of ranks = 99353 \n",
      " rank = 91000 of ranks = 99353 \n",
      " rank = 91100 of ranks = 99353 \n",
      " rank = 91200 of ranks = 99353 \n",
      " rank = 91300 of ranks = 99353 \n",
      " rank = 91400 of ranks = 99353 \n",
      " rank = 91500 of ranks = 99353 \n",
      " rank = 91600 of ranks = 99353 \n",
      " rank = 91700 of ranks = 99353 \n",
      " rank = 91800 of ranks = 99353 \n",
      " rank = 91900 of ranks = 99353 \n",
      " rank = 92000 of ranks = 99353 \n",
      " rank = 92100 of ranks = 99353 \n",
      " rank = 92200 of ranks = 99353 \n",
      " rank = 92300 of ranks = 99353 \n",
      " rank = 92400 of ranks = 99353 \n",
      " rank = 92500 of ranks = 99353 \n",
      " rank = 92600 of ranks = 99353 \n",
      " rank = 92700 of ranks = 99353 \n",
      " rank = 92800 of ranks = 99353 \n",
      " rank = 92900 of ranks = 99353 \n",
      " rank = 93000 of ranks = 99353 \n",
      " rank = 93100 of ranks = 99353 \n",
      " rank = 93200 of ranks = 99353 \n",
      " rank = 93300 of ranks = 99353 \n",
      " rank = 93400 of ranks = 99353 \n",
      " rank = 93500 of ranks = 99353 \n",
      " rank = 93600 of ranks = 99353 \n",
      " rank = 93700 of ranks = 99353 \n",
      " rank = 93800 of ranks = 99353 \n",
      " rank = 93900 of ranks = 99353 \n",
      " rank = 94000 of ranks = 99353 \n",
      " rank = 94100 of ranks = 99353 \n",
      " rank = 94200 of ranks = 99353 \n",
      " rank = 94300 of ranks = 99353 \n",
      " rank = 94400 of ranks = 99353 \n",
      " rank = 94500 of ranks = 99353 \n",
      " rank = 94600 of ranks = 99353 \n",
      " rank = 94700 of ranks = 99353 \n",
      " rank = 94800 of ranks = 99353 \n",
      " rank = 94900 of ranks = 99353 \n",
      " rank = 95000 of ranks = 99353 \n",
      " rank = 95100 of ranks = 99353 \n",
      " rank = 95200 of ranks = 99353 \n",
      " rank = 95300 of ranks = 99353 \n",
      " rank = 95400 of ranks = 99353 \n",
      " rank = 95500 of ranks = 99353 \n",
      " rank = 95600 of ranks = 99353 \n",
      " rank = 95700 of ranks = 99353 \n",
      " rank = 95800 of ranks = 99353 \n",
      " rank = 95900 of ranks = 99353 \n",
      " rank = 96000 of ranks = 99353 \n",
      " rank = 96100 of ranks = 99353 \n",
      " rank = 96200 of ranks = 99353 \n",
      " rank = 96300 of ranks = 99353 \n",
      " rank = 96400 of ranks = 99353 \n",
      " rank = 96500 of ranks = 99353 \n",
      " rank = 96600 of ranks = 99353 \n",
      " rank = 96700 of ranks = 99353 \n",
      " rank = 96800 of ranks = 99353 \n",
      " rank = 96900 of ranks = 99353 \n",
      " rank = 97000 of ranks = 99353 \n",
      " rank = 97100 of ranks = 99353 \n",
      " rank = 97200 of ranks = 99353 \n",
      " rank = 97300 of ranks = 99353 \n",
      " rank = 97400 of ranks = 99353 \n",
      " rank = 97500 of ranks = 99353 \n",
      " rank = 97600 of ranks = 99353 \n",
      " rank = 97700 of ranks = 99353 \n",
      " rank = 97800 of ranks = 99353 \n",
      " rank = 97900 of ranks = 99353 \n",
      " rank = 98000 of ranks = 99353 \n",
      " rank = 98100 of ranks = 99353 \n",
      " rank = 98200 of ranks = 99353 \n",
      " rank = 98300 of ranks = 99353 \n",
      " rank = 98400 of ranks = 99353 \n",
      " rank = 98500 of ranks = 99353 \n",
      " rank = 98600 of ranks = 99353 \n",
      " rank = 98700 of ranks = 99353 \n",
      " rank = 98800 of ranks = 99353 \n",
      " rank = 98900 of ranks = 99353 \n",
      " rank = 99000 of ranks = 99353 \n",
      " rank = 99100 of ranks = 99353 \n",
      " rank = 99200 of ranks = 99353 \n",
      " rank = 99300 of ranks = 99353 \u001b[0m\n",
      "\u001b[34mclass_id = 0, name = tiger, ap = 0.00%   #011 (TP = 0, FP = 99353) \n",
      "\n",
      " for conf_thresh = 0.25, precision = 0.00, recall = 0.00, F1-score = -nan \n",
      " for conf_thresh = 0.25, TP = 0, FP = 99353, FN = 544, average IoU = 0.00 % \n",
      "\n",
      " IoU threshold = 50 %, used Area-Under-Curve for each unique Recall \n",
      " mean average precision (mAP@0.50) = 0.000000, or 0.00 % \n",
      "\u001b[0m\n",
      "\u001b[34mSet -points flag:\n",
      " `-points 101` for MS COCO \n",
      " `-points 11` for PascalVOC 2007 (uncomment `difficult` in voc.data) \n",
      " `-points 0` (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset\u001b[0m\n",
      "\u001b[34mnet.optimized_memory = 0 \u001b[0m\n",
      "\u001b[34mmini_batch = 1, batch = 8, time_steps = 1, train = 0 \u001b[0m\n",
      "\u001b[34mCreate CUDA-stream - 0 \u001b[0m\n",
      "\u001b[34m seen 64, trained: 0 K-images (0 Kilo-batches_64) \n",
      " Detection layer: 133 - type = 28 \n",
      " Detection layer: 141 - type = 28 \n",
      " Detection layer: 149 - type = 28 \u001b[0m\n",
      "\u001b[34mCompleted 256.0 KiB/4.6 MiB (3.3 MiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mCompleted 512.0 KiB/4.6 MiB (6.4 MiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mCompleted 768.0 KiB/4.6 MiB (9.3 MiB/s) with 1 file(s) remaining\u001b[0m\n",
      "\u001b[34mCompleted 1.0 MiB/4.6 MiB (12.0 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 1.2 MiB/4.6 MiB (14.8 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 1.5 MiB/4.6 MiB (17.3 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 1.8 MiB/4.6 MiB (19.9 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.0 MiB/4.6 MiB (22.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.2 MiB/4.6 MiB (25.0 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.5 MiB/4.6 MiB (27.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 2.8 MiB/4.6 MiB (29.6 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.0 MiB/4.6 MiB (31.8 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.2 MiB/4.6 MiB (34.0 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.5 MiB/4.6 MiB (36.1 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 3.8 MiB/4.6 MiB (38.3 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.0 MiB/4.6 MiB (40.5 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.2 MiB/4.6 MiB (42.5 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.5 MiB/4.6 MiB (44.4 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mCompleted 4.6 MiB/4.6 MiB (31.3 MiB/s) with 1 file(s) remaining \u001b[0m\n",
      "\u001b[34mupload: backup/yolo-mini-tiger_final.weights to s3://calvinandpogs-ee148/atrw/detection/out/yolo-mini-tiger.weights\u001b[0m\n",
      "\u001b[34m+ S3_BASE=s3://calvinandpogs-ee148/atrw/detection\u001b[0m\n",
      "\u001b[34m+ DN_BASE=darknet/data/tiger/VOCdevkit/VOC2007\u001b[0m\n",
      "\u001b[34m+ git clone https://github.com/AlexeyAB/darknet\u001b[0m\n",
      "\u001b[34mCloning into 'darknet'...\u001b[0m\n",
      "\u001b[34m+ rm darknet/Makefile\u001b[0m\n",
      "\u001b[34m+ cp -r darknet_files/Makefile darknet_files/cfg darknet_files/data darknet/\u001b[0m\n",
      "\u001b[34m+ mkdir -p darknet/data/tiger/VOCdevkit/VOC2007\u001b[0m\n",
      "\u001b[34m+ ln -s /opt/ml/input/data/annot/Annotations darknet/data/tiger/VOCdevkit/VOC2007/Annotations\u001b[0m\n",
      "\u001b[34m+ ln -s /opt/ml/input/data/annot/ImageSets darknet/data/tiger/VOCdevkit/VOC2007/ImageSets\u001b[0m\n",
      "\u001b[34m+ ln -s /opt/ml/input/data/train darknet/data/tiger/VOCdevkit/VOC2007/JPEGImages\u001b[0m\n",
      "\u001b[34m+ cd darknet\u001b[0m\n",
      "\u001b[34m+ make -j4\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34mnvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\u001b[0m\n",
      "\u001b[34m+ cd data/tiger\u001b[0m\n",
      "\u001b[34m+ python voc_label.py\u001b[0m\n",
      "\u001b[34m+ cd ../..\u001b[0m\n",
      "\u001b[34m+ echo ----------------------------LISTING----------------------------\u001b[0m\n",
      "\u001b[34m+ ls data/tiger/VOCdevkit/VOC2007/\u001b[0m\n",
      "\u001b[34m+ head -n 10\u001b[0m\n",
      "\u001b[34m+ ls data/tiger/VOCdevkit/VOC2007/Annotations/\u001b[0m\n",
      "\u001b[34m+ echo '---------------------------DONE LISTING---------------------------'\u001b[0m\n",
      "\u001b[34m+ ./darknet detector train cfg/tiger.data cfg/yolo-mini-tiger.cfg -dont_show\n",
      " CUDA-version: 11010 (11010), GPU count: 1  \n",
      " OpenCV isn't used - data augmentation will be slow \n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 2    224 x 224 x   3 ->  112 x 112 x  16 0.011 BF\n",
      "   1 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   2 conv     16/  16  3 x 3/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.004 BF\n",
      "   3 avg                           112 x 112 x  16 ->     16\n",
      "   4 conv      4       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x   4 0.000 BF\n",
      "   5 conv     16       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  16 0.000 BF\n",
      "   6 scale Layer: 2\n",
      "   7 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   8 conv     48       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  48 0.019 BF\n",
      "   9 conv     48/  48  3 x 3/ 2    112 x 112 x  48 ->   56 x  56 x  48 0.003 BF\n",
      "  10 avg                            56 x  56 x  48 ->     48\n",
      "  11 conv     16       1 x 1/ 1      1 x   1 x  48 ->    1 x   1 x  16 0.000 BF\n",
      "  12 conv     48       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  48 0.000 BF\n",
      "  13 scale Layer: 9\n",
      "  14 conv     24       1 x 1/ 1     56 x  56 x  48 ->   56 x  56 x  24 0.007 BF\n",
      "  15 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\u001b[0m\n",
      "\u001b[34m2021-05-15 05:01:33,183 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  16 conv     72/  72  3 x 3/ 1     56 x  56 x  72 ->   56 x  56 x  72 0.004 BF\n",
      "  17 avg                            56 x  56 x  72 ->     72\n",
      "  18 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  19 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  20 scale Layer: 16\n",
      "  21 conv     24       1 x 1/ 1     56 x  56 x  72 ->   56 x  56 x  24 0.011 BF\n",
      "  22 dropout    p = 0.200        75264  ->   75264\n",
      "  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs:  56 x  56 x  24 0.000 BF\n",
      "  24 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  25 conv     72/  72  5 x 5/ 2     56 x  56 x  72 ->   28 x  28 x  72 0.003 BF\n",
      "  26 avg                            28 x  28 x  72 ->     72\n",
      "  27 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  28 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  29 scale Layer: 25\n",
      "  30 conv     20       1 x 1/ 1     28 x  28 x  72 ->   28 x  28 x  20 0.002 BF\n",
      "  31 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  32 conv     96/  96  5 x 5/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.004 BF\n",
      "  33 avg                            28 x  28 x  96 ->     96\n",
      "  34 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  35 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  36 scale Layer: 32\n",
      "  37 conv     20       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  20 0.003 BF\n",
      "  38 dropout    p = 0.200        15680  ->   15680\n",
      "  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  28 x  28 x  20 0.000 BF\n",
      "  40 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  41 conv     96/  96  3 x 3/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.001 BF\n",
      "  42 avg                            28 x  28 x  96 ->     96\n",
      "  43 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  44 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  45 scale Layer: 41\n",
      "  46 conv     40       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  40 0.006 BF\n",
      "  47 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  48 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  49 avg                            28 x  28 x 192 ->    192\n",
      "  50 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  51 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  52 scale Layer: 48\n",
      "  53 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  54 dropout    p = 0.200        31360  ->   31360\n",
      "  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  56 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  57 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  58 avg                            28 x  28 x 192 ->    192\n",
      "  59 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  60 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  61 scale Layer: 57\n",
      "  62 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  63 dropout    p = 0.200        31360  ->   31360\n",
      "  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  65 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  66 conv    192/ 192  5 x 5/ 2     28 x  28 x 192 ->   14 x  14 x 192 0.002 BF\n",
      "  67 avg                            14 x  14 x 192 ->    192\n",
      "  68 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  69 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  70 scale Layer: 66\n",
      "  71 conv     56       1 x 1/ 1     14 x  14 x 192 ->   14 x  14 x  56 0.004 BF\n",
      "  72 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  73 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  74 avg                            14 x  14 x 288 ->    288\n",
      "  75 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  76 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  77 scale Layer: 73\n",
      "  78 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  79 dropout    p = 0.200        10976  ->   10976\n",
      "  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  81 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  82 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  83 avg                            14 x  14 x 288 ->    288\n",
      "  84 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  85 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  86 scale Layer: 82\n",
      "  87 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  88 dropout    p = 0.200        10976  ->   10976\n",
      "  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  90 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  91 conv    288/ 288  5 x 5/ 2     14 x  14 x 288 ->    7 x   7 x 288 0.001 BF\n",
      "  92 avg                             7 x   7 x 288 ->    288\n",
      "  93 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  94 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  95 scale Layer: 91\n",
      "  96 conv     96       1 x 1/ 1      7 x   7 x 288 ->    7 x   7 x  96 0.003 BF\n",
      "  97 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      "  98 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      "  99 avg                             7 x   7 x 480 ->    480\n",
      " 100 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 101 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 102 scale Layer: 98\n",
      " 103 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 104 dropout    p = 0.200        4704  ->   4704\n",
      " 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 106 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 107 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 108 avg                             7 x   7 x 480 ->    480\n",
      " 109 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 110 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 111 scale Layer: 107\n",
      " 112 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 113 dropout    p = 0.200        4704  ->   4704\n",
      " 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 115 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 116 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 117 avg                             7 x   7 x 480 ->    480\n",
      " 118 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 119 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 120 scale Layer: 116\n",
      " 121 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 122 dropout    p = 0.200        4704  ->   4704\n",
      " 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 124 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 125 conv    480/ 480  3 x 3/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.000 BF\n",
      " 126 avg                             7 x   7 x 480 ->    480\n",
      " 127 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 128 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 129 scale Layer: 125\n",
      " 130 conv    160       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x 160 0.008 BF\n",
      " 131 conv    640       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 640 0.010 BF\n",
      " 132 conv     18       1 x 1/ 1      7 x   7 x 640 ->    7 x   7 x  18 0.001 BF\n",
      " 133 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 134 route  130 #011#011                           ->    7 x   7 x 160 \n",
      " 135 conv    256       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 256 0.004 BF\n",
      " 136 upsample                 2x     7 x   7 x 256 ->   14 x  14 x 256\n",
      " 137 route  136 89 #011                           ->   14 x  14 x 312 \n",
      " 138 conv     64       1 x 1/ 1     14 x  14 x 312 ->   14 x  14 x  64 0.008 BF\n",
      " 139 conv    128       3 x 3/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.029 BF\n",
      " 140 conv     18       1 x 1/ 1     14 x  14 x 128 ->   14 x  14 x  18 0.001 BF\n",
      " 141 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 142 route  138 #011#011                           ->   14 x  14 x  64 \n",
      " 143 conv    128       1 x 1/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.003 BF\n",
      " 144 upsample                 2x    14 x  14 x 128 ->   28 x  28 x 128\n",
      " 145 route  144 64 #011                           ->   28 x  28 x 168 \n",
      " 146 conv     64       1 x 1/ 1     28 x  28 x 168 ->   28 x  28 x  64 0.017 BF\n",
      " 147 conv    128       3 x 3/ 1     28 x  28 x  64 ->   28 x  28 x 128 0.116 BF\n",
      " 148 conv     18       1 x 1/ 1     28 x  28 x 128 ->   28 x  28 x  18 0.004 BF\n",
      " 149 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\u001b[0m\n",
      "\u001b[34mTotal BFLOPS 0.466 \u001b[0m\n",
      "\u001b[34mavg_outputs = 54324 \n",
      " Allocate additional workspace_size = 1.81 MB \n",
      " Create 64 permanent cpu-threads \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.585736), count: 4, class_loss = 35.330635, iou_loss = 2.181286, total_loss = 37.511921 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.644636), count: 2, class_loss = 151.843353, iou_loss = 3.813477, total_loss = 155.656830 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.419581), count: 5, class_loss = 530.343933, iou_loss = 46.338135, total_loss = 576.682068 \n",
      " total_bbox = 11, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.495229), count: 3, class_loss = 35.664543, iou_loss = 1.640972, total_loss = 37.305515 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.523735), count: 6, class_loss = 151.885803, iou_loss = 10.915680, total_loss = 162.801483 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.475257), count: 1, class_loss = 526.414856, iou_loss = 7.220886, total_loss = 533.635742 \n",
      " total_bbox = 21, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.513340), count: 4, class_loss = 35.445744, iou_loss = 1.141075, total_loss = 36.586819 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.553230), count: 6, class_loss = 152.169235, iou_loss = 8.926483, total_loss = 161.095718 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.392674), count: 5, class_loss = 524.883911, iou_loss = 73.594299, total_loss = 598.478210 \n",
      " total_bbox = 36, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.348026), count: 6, class_loss = 35.071285, iou_loss = 1.503960, total_loss = 36.575245 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.755478), count: 3, class_loss = 152.431747, iou_loss = 8.887680, total_loss = 161.319427 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.517911), count: 13, class_loss = 520.573181, iou_loss = 169.724121, total_loss = 690.297302 \n",
      " total_bbox = 58, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.684345), count: 1, class_loss = 34.937000, iou_loss = 0.760727, total_loss = 35.697727 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.489515), count: 9, class_loss = 152.088348, iou_loss = 15.018829, total_loss = 167.107178 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.447787), count: 10, class_loss = 533.960144, iou_loss = 105.611511, total_loss = 639.571655 \n",
      " total_bbox = 78, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.547033), count: 6, class_loss = 35.607727, iou_loss = 4.369614, total_loss = 39.977341 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.246344), count: 4, class_loss = 151.984818, iou_loss = 1.893692, total_loss = 153.878510 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.381798), count: 2, class_loss = 526.086060, iou_loss = 9.890625, total_loss = 535.976685 \n",
      " total_bbox = 90, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.568146), count: 4, class_loss = 35.505116, iou_loss = 2.427509, total_loss = 37.932625 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.487822), count: 8, class_loss = 151.844620, iou_loss = 12.998062, total_loss = 164.842682 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.325997), count: 6, class_loss = 535.079468, iou_loss = 30.168091, total_loss = 565.247559 \n",
      " total_bbox = 108, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.571127), count: 4, class_loss = 35.165573, iou_loss = 3.253288, total_loss = 38.418861 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.426562), count: 5, class_loss = 152.036789, iou_loss = 7.086517, total_loss = 159.123306 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.462290), count: 4, class_loss = 523.327209, iou_loss = 76.915894, total_loss = 600.243103 \n",
      " total_bbox = 121, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.598499), count: 4, class_loss = 35.059731, iou_loss = 2.399509, total_loss = 37.459240 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.519438), count: 7, class_loss = 152.094833, iou_loss = 12.844666, total_loss = 164.939499 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.530595), count: 4, class_loss = 520.737305, iou_loss = 42.643677, total_loss = 563.380981 \n",
      " total_bbox = 136, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.510293), count: 6, class_loss = 35.376144, iou_loss = 3.209522, total_loss = 38.585667 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.420248), count: 2, class_loss = 151.502747, iou_loss = 3.455963, total_loss = 154.958710 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.438875), count: 9, class_loss = 534.586914, iou_loss = 152.306580, total_loss = 686.893494 \n",
      " total_bbox = 153, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.461813), count: 4, class_loss = 34.773491, iou_loss = 1.973824, total_loss = 36.747314 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.377821), count: 6, class_loss = 152.018234, iou_loss = 6.207855, total_loss = 158.226089 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.389082), count: 7, class_loss = 518.862671, iou_loss = 56.576660, total_loss = 575.439331 \n",
      " total_bbox = 170, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.482420), count: 3, class_loss = 35.131065, iou_loss = 1.247120, total_loss = 36.378185 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.425204), count: 8, class_loss = 152.463547, iou_loss = 11.956512, total_loss = 164.420059 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.316959), count: 7, class_loss = 519.558228, iou_loss = 23.324097, total_loss = 542.882324 \n",
      " total_bbox = 188, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.510154), count: 8, class_loss = 35.035259, iou_loss = 3.716808, total_loss = 38.752068 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.413458), count: 9, class_loss = 152.459183, iou_loss = 9.992706, total_loss = 162.451889 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.430331), count: 3, class_loss = 522.877014, iou_loss = 19.462280, total_loss = 542.339294 \n",
      " total_bbox = 208, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.506934), count: 3, class_loss = 35.070370, iou_loss = 1.435448, total_loss = 36.505817 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.409041), count: 6, class_loss = 151.304901, iou_loss = 6.918808, total_loss = 158.223709 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.610418), count: 2, class_loss = 524.506836, iou_loss = 14.789612, total_loss = 539.296448 \n",
      " total_bbox = 219, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.525927), count: 4, class_loss = 35.138493, iou_loss = 2.263649, total_loss = 37.402142 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.517989), count: 9, class_loss = 152.605301, iou_loss = 18.197052, total_loss = 170.802353 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.431010), count: 13, class_loss = 525.264160, iou_loss = 124.049622, total_loss = 649.313782 \n",
      " total_bbox = 245, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.607351), count: 6, class_loss = 35.210945, iou_loss = 4.999844, total_loss = 40.210789 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.510143), count: 6, class_loss = 152.568054, iou_loss = 11.815002, total_loss = 164.383057 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.558595), count: 3, class_loss = 520.944641, iou_loss = 17.242004, total_loss = 538.186646 \n",
      " total_bbox = 260, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.491533), count: 7, class_loss = 35.229370, iou_loss = 3.168995, total_loss = 38.398365 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.465793), count: 8, class_loss = 151.448502, iou_loss = 10.911499, total_loss = 162.360001 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.396951), count: 7, class_loss = 525.876099, iou_loss = 45.354553, total_loss = 571.230652 \n",
      " total_bbox = 282, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.458504), count: 7, class_loss = 35.494331, iou_loss = 4.039120, total_loss = 39.533451 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.546817), count: 6, class_loss = 151.707306, iou_loss = 10.602112, total_loss = 162.309418 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.385759), count: 3, class_loss = 529.599731, iou_loss = 15.600525, total_loss = 545.200256 \n",
      " total_bbox = 298, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.595805), count: 6, class_loss = 35.621601, iou_loss = 4.079136, total_loss = 39.700737 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.478176), count: 8, class_loss = 150.770645, iou_loss = 14.312027, total_loss = 165.082672 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.577505), count: 7, class_loss = 534.691040, iou_loss = 90.688538, total_loss = 625.379578 \n",
      " total_bbox = 319, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.437495), count: 5, class_loss = 35.089596, iou_loss = 2.869629, total_loss = 37.959225 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.411559), count: 3, class_loss = 151.846069, iou_loss = 4.319550, total_loss = 156.165619 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.476347), count: 3, class_loss = 520.999023, iou_loss = 22.161255, total_loss = 543.160278 \n",
      " total_bbox = 329, rewritten_bbox = 0.000000 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.465969), count: 7, class_loss = 35.300972, iou_loss = 2.650635, total_loss = 37.951607 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.467532), count: 4, class_loss = 150.720978, iou_loss = 4.931351, total_loss = 155.652328 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.465636), count: 5, class_loss = 526.194458, iou_loss = 54.331726, total_loss = 580.526184 \n",
      " total_bbox = 345, rewritten_bbox = 0.289855 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.492902), count: 2, class_loss = 35.586918, iou_loss = 1.293480, total_loss = 36.880398 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.563213), count: 10, class_loss = 152.257187, iou_loss = 17.120911, total_loss = 169.378098 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.000000), count: 1, class_loss = 528.056824, iou_loss = 0.000000, total_loss = 528.056824 \n",
      " total_bbox = 357, rewritten_bbox = 0.280112 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.465080), count: 6, class_loss = 35.426193, iou_loss = 2.237743, total_loss = 37.663937 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.591607), count: 3, class_loss = 151.087814, iou_loss = 5.668060, total_loss = 156.755875 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.278506), count: 8, class_loss = 534.602844, iou_loss = 52.077881, total_loss = 586.680725 \n",
      " total_bbox = 374, rewritten_bbox = 0.267380 % \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 133 Avg (IOU: 0.518638), count: 7, class_loss = 35.302952, iou_loss = 7.063366, total_loss = 42.366318 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 141 Avg (IOU: 0.615219), count: 7, class_loss = 151.758911, iou_loss = 13.518036, total_loss = 165.276947 \u001b[0m\n",
      "\u001b[34mv3 (giou loss, Normalizer: (iou: 0.50, obj: 1.00, cls: 1.00) Region 149 Avg (IOU: 0.298709), count: 13, class_loss = 531.658569, iou_loss = 116.587708, total_loss = 648.246277 \n",
      " total_bbox = 401, rewritten_bbox = 0.249377 % \u001b[0m\n",
      "\u001b[34mSaving weights to backup/yolo-mini-tiger_final.weights\u001b[0m\n",
      "\u001b[34m+ ./darknet detector map cfg/tiger.data cfg/yolo-mini-tiger.cfg backup/yolo-mini-tiger_final.weights\n",
      " CUDA-version: 11010 (11010), GPU count: 1  \n",
      " OpenCV isn't used - data augmentation will be slow \n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 2    224 x 224 x   3 ->  112 x 112 x  16 0.011 BF\n",
      "   1 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   2 conv     16/  16  3 x 3/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.004 BF\n",
      "   3 avg                           112 x 112 x  16 ->     16\n",
      "   4 conv      4       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x   4 0.000 BF\n",
      "   5 conv     16       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  16 0.000 BF\n",
      "   6 scale Layer: 2\n",
      "   7 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   8 conv     48       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  48 0.019 BF\n",
      "   9 conv     48/  48  3 x 3/ 2    112 x 112 x  48 ->   56 x  56 x  48 0.003 BF\n",
      "  10 avg                            56 x  56 x  48 ->     48\n",
      "  11 conv     16       1 x 1/ 1      1 x   1 x  48 ->    1 x   1 x  16 0.000 BF\n",
      "  12 conv     48       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  48 0.000 BF\n",
      "  13 scale Layer: 9\n",
      "  14 conv     24       1 x 1/ 1     56 x  56 x  48 ->   56 x  56 x  24 0.007 BF\n",
      "  15 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  16 conv     72/  72  3 x 3/ 1     56 x  56 x  72 ->   56 x  56 x  72 0.004 BF\n",
      "  17 avg                            56 x  56 x  72 ->     72\n",
      "  18 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  19 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  20 scale Layer: 16\n",
      "  21 conv     24       1 x 1/ 1     56 x  56 x  72 ->   56 x  56 x  24 0.011 BF\n",
      "  22 dropout    p = 0.200        75264  ->   75264\n",
      "  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs:  56 x  56 x  24 0.000 BF\n",
      "  24 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  25 conv     72/  72  5 x 5/ 2     56 x  56 x  72 ->   28 x  28 x  72 0.003 BF\n",
      "  26 avg                            28 x  28 x  72 ->     72\n",
      "  27 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  28 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  29 scale Layer: 25\n",
      "  30 conv     20       1 x 1/ 1     28 x  28 x  72 ->   28 x  28 x  20 0.002 BF\n",
      "  31 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  32 conv     96/  96  5 x 5/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.004 BF\n",
      "  33 avg                            28 x  28 x  96 ->     96\n",
      "  34 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  35 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  36 scale Layer: 32\n",
      "  37 conv     20       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  20 0.003 BF\n",
      "  38 dropout    p = 0.200        15680  ->   15680\n",
      "  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  28 x  28 x  20 0.000 BF\n",
      "  40 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  41 conv     96/  96  3 x 3/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.001 BF\n",
      "  42 avg                            28 x  28 x  96 ->     96\n",
      "  43 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  44 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  45 scale Layer: 41\n",
      "  46 conv     40       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  40 0.006 BF\n",
      "  47 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  48 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  49 avg                            28 x  28 x 192 ->    192\n",
      "  50 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  51 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  52 scale Layer: 48\n",
      "  53 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  54 dropout    p = 0.200        31360  ->   31360\n",
      "  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  56 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  57 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  58 avg                            28 x  28 x 192 ->    192\n",
      "  59 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  60 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  61 scale Layer: 57\n",
      "  62 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  63 dropout    p = 0.200        31360  ->   31360\n",
      "  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  65 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  66 conv    192/ 192  5 x 5/ 2     28 x  28 x 192 ->   14 x  14 x 192 0.002 BF\n",
      "  67 avg                            14 x  14 x 192 ->    192\n",
      "  68 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  69 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  70 scale Layer: 66\n",
      "  71 conv     56       1 x 1/ 1     14 x  14 x 192 ->   14 x  14 x  56 0.004 BF\n",
      "  72 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  73 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  74 avg                            14 x  14 x 288 ->    288\n",
      "  75 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  76 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  77 scale Layer: 73\n",
      "  78 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  79 dropout    p = 0.200        10976  ->   10976\n",
      "  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  81 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  82 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  83 avg                            14 x  14 x 288 ->    288\n",
      "  84 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  85 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  86 scale Layer: 82\n",
      "  87 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  88 dropout    p = 0.200        10976  ->   10976\n",
      "  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  90 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  91 conv    288/ 288  5 x 5/ 2     14 x  14 x 288 ->    7 x   7 x 288 0.001 BF\n",
      "  92 avg                             7 x   7 x 288 ->    288\n",
      "  93 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  94 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  95 scale Layer: 91\n",
      "  96 conv     96       1 x 1/ 1      7 x   7 x 288 ->    7 x   7 x  96 0.003 BF\n",
      "  97 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      "  98 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      "  99 avg                             7 x   7 x 480 ->    480\n",
      " 100 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 101 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 102 scale Layer: 98\n",
      " 103 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 104 dropout    p = 0.200        4704  ->   4704\n",
      " 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 106 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 107 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 108 avg                             7 x   7 x 480 ->    480\n",
      " 109 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 110 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 111 scale Layer: 107\n",
      " 112 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 113 dropout    p = 0.200        4704  ->   4704\n",
      " 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 115 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 116 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 117 avg                             7 x   7 x 480 ->    480\n",
      " 118 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 119 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 120 scale Layer: 116\n",
      " 121 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 122 dropout    p = 0.200        4704  ->   4704\n",
      " 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 124 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 125 conv    480/ 480  3 x 3/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.000 BF\n",
      " 126 avg                             7 x   7 x 480 ->    480\n",
      " 127 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 128 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 129 scale Layer: 125\n",
      " 130 conv    160       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x 160 0.008 BF\n",
      " 131 conv    640       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 640 0.010 BF\n",
      " 132 conv     18       1 x 1/ 1      7 x   7 x 640 ->    7 x   7 x  18 0.001 BF\n",
      " 133 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 134 route  130 #011#011                           ->    7 x   7 x 160 \n",
      " 135 conv    256       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 256 0.004 BF\n",
      " 136 upsample                 2x     7 x   7 x 256 ->   14 x  14 x 256\n",
      " 137 route  136 89 #011                           ->   14 x  14 x 312 \n",
      " 138 conv     64       1 x 1/ 1     14 x  14 x 312 ->   14 x  14 x  64 0.008 BF\n",
      " 139 conv    128       3 x 3/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.029 BF\n",
      " 140 conv     18       1 x 1/ 1     14 x  14 x 128 ->   14 x  14 x  18 0.001 BF\n",
      " 141 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 142 route  138 #011#011                           ->   14 x  14 x  64 \n",
      " 143 conv    128       1 x 1/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.003 BF\n",
      " 144 upsample                 2x    14 x  14 x 128 ->   28 x  28 x 128\n",
      " 145 route  144 64 #011                           ->   28 x  28 x 168 \n",
      " 146 conv     64       1 x 1/ 1     28 x  28 x 168 ->   28 x  28 x  64 0.017 BF\n",
      " 147 conv    128       3 x 3/ 1     28 x  28 x  64 ->   28 x  28 x 128 0.116 BF\n",
      " 148 conv     18       1 x 1/ 1     28 x  28 x 128 ->   28 x  28 x  18 0.004 BF\n",
      " 149 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\u001b[0m\n",
      "\u001b[34mTotal BFLOPS 0.466 \u001b[0m\n",
      "\u001b[34mavg_outputs = 54324 \n",
      " Allocate additional workspace_size = 1.81 MB \u001b[0m\n",
      "\u001b[34mLoading weights from backup/yolo-mini-tiger_final.weights...Done! Loaded 150 layers from weights-file \u001b[0m\n",
      "\u001b[34m#0154#0158#01512#01516#01520#01524#01528#01532#01536#01540#01544#01548#01552#01556#01560#01564#01568#01572#01576#01580#01584#01588#01592#01596#015100#015104#015108#015112#015116#015120#015124#015128#015132#015136#015140#015144#015148#015152#015156#015160#015164#015168#015172#015176#015180#015184#015188#015192#015196#015200#015204#015208#015212#015216#015220#015224#015228#015232#015236#015240#015244#015248#015252#015256#015260#015264#015268#015272#015276#015280Total Detection Time: 16 Seconds\u001b[0m\n",
      "\u001b[34m+ ./darknet detector valid cfg/tiger.data cfg/yolo-mini-tiger.cfg backup/yolo-mini-tiger_final.weights -out ''\n",
      " CUDA-version: 11010 (11010), GPU count: 1  \n",
      " OpenCV isn't used - data augmentation will be slow \u001b[0m\n",
      "\u001b[34mresults: Using default 'results'\n",
      " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
      "   layer   filters  size/strd(dil)      input                output\n",
      "   0 conv     16       3 x 3/ 2    224 x 224 x   3 ->  112 x 112 x  16 0.011 BF\n",
      "   1 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   2 conv     16/  16  3 x 3/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.004 BF\n",
      "   3 avg                           112 x 112 x  16 ->     16\n",
      "   4 conv      4       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x   4 0.000 BF\n",
      "   5 conv     16       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  16 0.000 BF\n",
      "   6 scale Layer: 2\n",
      "   7 conv     16       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  16 0.006 BF\n",
      "   8 conv     48       1 x 1/ 1    112 x 112 x  16 ->  112 x 112 x  48 0.019 BF\n",
      "   9 conv     48/  48  3 x 3/ 2    112 x 112 x  48 ->   56 x  56 x  48 0.003 BF\n",
      "  10 avg                            56 x  56 x  48 ->     48\n",
      "  11 conv     16       1 x 1/ 1      1 x   1 x  48 ->    1 x   1 x  16 0.000 BF\n",
      "  12 conv     48       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  48 0.000 BF\n",
      "  13 scale Layer: 9\n",
      "  14 conv     24       1 x 1/ 1     56 x  56 x  48 ->   56 x  56 x  24 0.007 BF\n",
      "  15 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  16 conv     72/  72  3 x 3/ 1     56 x  56 x  72 ->   56 x  56 x  72 0.004 BF\n",
      "  17 avg                            56 x  56 x  72 ->     72\n",
      "  18 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  19 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  20 scale Layer: 16\n",
      "  21 conv     24       1 x 1/ 1     56 x  56 x  72 ->   56 x  56 x  24 0.011 BF\n",
      "  22 dropout    p = 0.200        75264  ->   75264\n",
      "  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs:  56 x  56 x  24 0.000 BF\n",
      "  24 conv     72       1 x 1/ 1     56 x  56 x  24 ->   56 x  56 x  72 0.011 BF\n",
      "  25 conv     72/  72  5 x 5/ 2     56 x  56 x  72 ->   28 x  28 x  72 0.003 BF\n",
      "  26 avg                            28 x  28 x  72 ->     72\n",
      "  27 conv      4       1 x 1/ 1      1 x   1 x  72 ->    1 x   1 x   4 0.000 BF\n",
      "  28 conv     72       1 x 1/ 1      1 x   1 x   4 ->    1 x   1 x  72 0.000 BF\n",
      "  29 scale Layer: 25\n",
      "  30 conv     20       1 x 1/ 1     28 x  28 x  72 ->   28 x  28 x  20 0.002 BF\n",
      "  31 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  32 conv     96/  96  5 x 5/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.004 BF\n",
      "  33 avg                            28 x  28 x  96 ->     96\n",
      "  34 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  35 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  36 scale Layer: 32\n",
      "  37 conv     20       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  20 0.003 BF\n",
      "  38 dropout    p = 0.200        15680  ->   15680\n",
      "  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  28 x  28 x  20 0.000 BF\n",
      "  40 conv     96       1 x 1/ 1     28 x  28 x  20 ->   28 x  28 x  96 0.003 BF\n",
      "  41 conv     96/  96  3 x 3/ 1     28 x  28 x  96 ->   28 x  28 x  96 0.001 BF\n",
      "  42 avg                            28 x  28 x  96 ->     96\n",
      "  43 conv      8       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x   8 0.000 BF\n",
      "  44 conv     96       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  96 0.000 BF\n",
      "  45 scale Layer: 41\n",
      "  46 conv     40       1 x 1/ 1     28 x  28 x  96 ->   28 x  28 x  40 0.006 BF\n",
      "  47 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  48 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  49 avg                            28 x  28 x 192 ->    192\n",
      "  50 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  51 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  52 scale Layer: 48\n",
      "  53 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  54 dropout    p = 0.200        31360  ->   31360\n",
      "  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  56 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  57 conv    192/ 192  3 x 3/ 1     28 x  28 x 192 ->   28 x  28 x 192 0.003 BF\n",
      "  58 avg                            28 x  28 x 192 ->    192\n",
      "  59 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  60 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  61 scale Layer: 57\n",
      "  62 conv     40       1 x 1/ 1     28 x  28 x 192 ->   28 x  28 x  40 0.012 BF\n",
      "  63 dropout    p = 0.200        31360  ->   31360\n",
      "  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  28 x  28 x  40 0.000 BF\n",
      "  65 conv    192       1 x 1/ 1     28 x  28 x  40 ->   28 x  28 x 192 0.012 BF\n",
      "  66 conv    192/ 192  5 x 5/ 2     28 x  28 x 192 ->   14 x  14 x 192 0.002 BF\n",
      "  67 avg                            14 x  14 x 192 ->    192\n",
      "  68 conv     12       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  12 0.000 BF\n",
      "  69 conv    192       1 x 1/ 1      1 x   1 x  12 ->    1 x   1 x 192 0.000 BF\n",
      "  70 scale Layer: 66\n",
      "  71 conv     56       1 x 1/ 1     14 x  14 x 192 ->   14 x  14 x  56 0.004 BF\n",
      "  72 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  73 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  74 avg                            14 x  14 x 288 ->    288\n",
      "  75 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  76 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  77 scale Layer: 73\n",
      "  78 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  79 dropout    p = 0.200        10976  ->   10976\n",
      "  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  81 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  82 conv    288/ 288  5 x 5/ 1     14 x  14 x 288 ->   14 x  14 x 288 0.003 BF\n",
      "  83 avg                            14 x  14 x 288 ->    288\n",
      "  84 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  85 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  86 scale Layer: 82\n",
      "  87 conv     56       1 x 1/ 1     14 x  14 x 288 ->   14 x  14 x  56 0.006 BF\n",
      "  88 dropout    p = 0.200        10976  ->   10976\n",
      "  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  14 x  14 x  56 0.000 BF\n",
      "  90 conv    288       1 x 1/ 1     14 x  14 x  56 ->   14 x  14 x 288 0.006 BF\n",
      "  91 conv    288/ 288  5 x 5/ 2     14 x  14 x 288 ->    7 x   7 x 288 0.001 BF\n",
      "  92 avg                             7 x   7 x 288 ->    288\n",
      "  93 conv     16       1 x 1/ 1      1 x   1 x 288 ->    1 x   1 x  16 0.000 BF\n",
      "  94 conv    288       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 288 0.000 BF\n",
      "  95 scale Layer: 91\n",
      "  96 conv     96       1 x 1/ 1      7 x   7 x 288 ->    7 x   7 x  96 0.003 BF\n",
      "  97 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      "  98 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      "  99 avg                             7 x   7 x 480 ->    480\n",
      " 100 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 101 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 102 scale Layer: 98\n",
      " 103 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 104 dropout    p = 0.200        4704  ->   4704\n",
      " 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 106 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 107 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 108 avg                             7 x   7 x 480 ->    480\n",
      " 109 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 110 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 111 scale Layer: 107\n",
      " 112 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 113 dropout    p = 0.200        4704  ->   4704\n",
      " 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 115 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 116 conv    480/ 480  5 x 5/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.001 BF\n",
      " 117 avg                             7 x   7 x 480 ->    480\n",
      " 118 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 119 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 120 scale Layer: 116\n",
      " 121 conv     96       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x  96 0.005 BF\n",
      " 122 dropout    p = 0.200        4704  ->   4704\n",
      " 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:   7 x   7 x  96 0.000 BF\n",
      " 124 conv    480       1 x 1/ 1      7 x   7 x  96 ->    7 x   7 x 480 0.005 BF\n",
      " 125 conv    480/ 480  3 x 3/ 1      7 x   7 x 480 ->    7 x   7 x 480 0.000 BF\n",
      " 126 avg                             7 x   7 x 480 ->    480\n",
      " 127 conv     32       1 x 1/ 1      1 x   1 x 480 ->    1 x   1 x  32 0.000 BF\n",
      " 128 conv    480       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 480 0.000 BF\n",
      " 129 scale Layer: 125\n",
      " 130 conv    160       1 x 1/ 1      7 x   7 x 480 ->    7 x   7 x 160 0.008 BF\n",
      " 131 conv    640       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 640 0.010 BF\n",
      " 132 conv     18       1 x 1/ 1      7 x   7 x 640 ->    7 x   7 x  18 0.001 BF\n",
      " 133 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 134 route  130 #011#011                           ->    7 x   7 x 160 \n",
      " 135 conv    256       1 x 1/ 1      7 x   7 x 160 ->    7 x   7 x 256 0.004 BF\n",
      " 136 upsample                 2x     7 x   7 x 256 ->   14 x  14 x 256\n",
      " 137 route  136 89 #011                           ->   14 x  14 x 312 \n",
      " 138 conv     64       1 x 1/ 1     14 x  14 x 312 ->   14 x  14 x  64 0.008 BF\n",
      " 139 conv    128       3 x 3/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.029 BF\n",
      " 140 conv     18       1 x 1/ 1     14 x  14 x 128 ->   14 x  14 x  18 0.001 BF\n",
      " 141 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
      " 142 route  138 #011#011                           ->   14 x  14 x  64 \n",
      " 143 conv    128       1 x 1/ 1     14 x  14 x  64 ->   14 x  14 x 128 0.003 BF\n",
      " 144 upsample                 2x    14 x  14 x 128 ->   28 x  28 x 128\n",
      " 145 route  144 64 #011                           ->   28 x  28 x 168 \n",
      " 146 conv     64       1 x 1/ 1     28 x  28 x 168 ->   28 x  28 x  64 0.017 BF\n",
      " 147 conv    128       3 x 3/ 1     28 x  28 x  64 ->   28 x  28 x 128 0.116 BF\n",
      " 148 conv     18       1 x 1/ 1     28 x  28 x 128 ->   28 x  28 x  18 0.004 BF\n",
      " 149 yolo\u001b[0m\n",
      "\u001b[34m[yolo] params: iou loss: giou (1), iou_norm: 0.50, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\u001b[0m\n",
      "\u001b[34mTotal BFLOPS 0.466 \u001b[0m\n",
      "\u001b[34mavg_outputs = 54324 \n",
      " Allocate additional workspace_size = 1.81 MB \u001b[0m\n",
      "\u001b[34mLoading weights from backup/yolo-mini-tiger_final.weights...Done! Loaded 150 layers from weights-file \u001b[0m\n",
      "\u001b[34mLearning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\u001b[0m\n",
      "\u001b[34meval: Using default 'voc'\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34m8\u001b[0m\n",
      "\u001b[34m12\u001b[0m\n",
      "\u001b[34m16\u001b[0m\n",
      "\u001b[34m20\u001b[0m\n",
      "\u001b[34m24\u001b[0m\n",
      "\u001b[34m28\u001b[0m\n",
      "\u001b[34m32\u001b[0m\n",
      "\u001b[34m36\u001b[0m\n",
      "\u001b[34m40\u001b[0m\n",
      "\u001b[34m44\u001b[0m\n",
      "\u001b[34m48\u001b[0m\n",
      "\u001b[34m52\u001b[0m\n",
      "\u001b[34m56\u001b[0m\n",
      "\u001b[34m60\u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34m68\u001b[0m\n",
      "\u001b[34m72\u001b[0m\n",
      "\u001b[34m76\u001b[0m\n",
      "\u001b[34m80\u001b[0m\n",
      "\u001b[34m84\u001b[0m\n",
      "\u001b[34m88\u001b[0m\n",
      "\u001b[34m92\u001b[0m\n",
      "\u001b[34m96\u001b[0m\n",
      "\u001b[34m100\u001b[0m\n",
      "\u001b[34m104\u001b[0m\n",
      "\u001b[34m108\u001b[0m\n",
      "\u001b[34m112\u001b[0m\n",
      "\u001b[34m116\u001b[0m\n",
      "\u001b[34m120\u001b[0m\n",
      "\u001b[34m124\u001b[0m\n",
      "\u001b[34m128\u001b[0m\n",
      "\u001b[34m132\u001b[0m\n",
      "\u001b[34m136\u001b[0m\n",
      "\u001b[34m140\u001b[0m\n",
      "\u001b[34m144\u001b[0m\n",
      "\u001b[34m148\u001b[0m\n",
      "\u001b[34m152\u001b[0m\n",
      "\u001b[34m156\u001b[0m\n",
      "\u001b[34m160\u001b[0m\n",
      "\u001b[34m164\u001b[0m\n",
      "\u001b[34m168\u001b[0m\n",
      "\u001b[34m172\u001b[0m\n",
      "\u001b[34m176\u001b[0m\n",
      "\u001b[34m180\u001b[0m\n",
      "\u001b[34m184\u001b[0m\n",
      "\u001b[34m188\u001b[0m\n",
      "\u001b[34m192\u001b[0m\n",
      "\u001b[34m196\u001b[0m\n",
      "\u001b[34m200\u001b[0m\n",
      "\u001b[34m204\u001b[0m\n",
      "\u001b[34m208\u001b[0m\n",
      "\u001b[34m212\u001b[0m\n",
      "\u001b[34m216\u001b[0m\n",
      "\u001b[34m220\u001b[0m\n",
      "\u001b[34m224\u001b[0m\n",
      "\u001b[34m228\u001b[0m\n",
      "\u001b[34m232\u001b[0m\n",
      "\u001b[34m236\u001b[0m\n",
      "\u001b[34m240\u001b[0m\n",
      "\u001b[34m244\u001b[0m\n",
      "\u001b[34m248\u001b[0m\n",
      "\u001b[34m252\u001b[0m\n",
      "\u001b[34m256\u001b[0m\n",
      "\u001b[34m260\u001b[0m\n",
      "\u001b[34m264\u001b[0m\n",
      "\u001b[34m268\u001b[0m\n",
      "\u001b[34m272\u001b[0m\n",
      "\u001b[34m276\u001b[0m\n",
      "\u001b[34m280\u001b[0m\n",
      "\u001b[34mTotal Detection Time: 17.000000 Seconds\u001b[0m\n",
      "\u001b[34m+ aws s3 cp backup/yolo-mini-tiger_final.weights s3://calvinandpogs-ee148/atrw/detection/out/yolo-mini-tiger.weights\n",
      "\u001b[0m\n",
      "\n",
      "2021-05-15 05:02:05 Uploading - Uploading generated training model\n",
      "2021-05-15 05:02:05 Completed - Training job completed\n",
      "ProfilerReport-1621054439: NoIssuesFound\n",
      "Training seconds: 308\n",
      "Billable seconds: 308\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'annot': f's3://{bucket}/atrw/detection/annotations/',\n",
    "               'train': f's3://{bucket}/atrw/detection/train/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d891584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The function delete_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e580ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
